{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.agents import initialize_agent, AgentType, Tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Union\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import csv\n",
    "import io \n",
    "from datetime import time\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm(model: str = \"gpt-oss:20b\", temperature: float = 0.0) -> ChatOllama:\n",
    "    return ChatOllama(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        base_url=\"http://localhost:11434\",   \n",
    "        timeout=500                       \n",
    "    )\n",
    "\n",
    "#embeddings = OllamaEmbeddings(model=\"all-minilm\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "llm1 = build_llm('gpt-oss:20b')       \n",
    "llm2 = build_llm('gpt-oss:20b')         \n",
    "llm_code = build_llm('gpt-oss:20b')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(output,selected_problem):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in mathematical optimization and Python programming. Your task is to write Python code to solve the provided mathematical optimization model using the Gurobi library. The code should include the definition of the objective function, constraints, and decision variables. Please don't add additional explanations. Please don't include ```python and ```.Below is the provided mathematical optimization model:\n",
    "\n",
    "    Mathematical Optimization Model:\n",
    "    {output}\n",
    "    \"\"\"\n",
    "\n",
    "    if selected_problem == \"Network Revenue Management\" or selected_problem == \"NRM\" or selected_problem == \"Network Revenue Management Problem\":\n",
    "\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\max \\quad \\sum_i A_i \\cdot x_i$\n",
    "Constraints\n",
    "1. Inventory Constraints:\n",
    "$\\quad \\quad x_i \\leq I_i, \\quad \\forall i$\n",
    "2. Demand Constraints:\n",
    "$x_i \\leq d_i, \\quad \\forall i$\n",
    "3. Startup Constraint:\n",
    "$\\sum_i x_i \\geq s$\n",
    "Retrieved Information\n",
    "$\\small I = [7550, 6244]$\n",
    "$\\small A = [149, 389]$\n",
    "$\\small d = [15057, 12474]$\n",
    "$\\small s = 100$\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Product_Optimization\")\n",
    "\n",
    "# Decision variables for the number of units of each product\n",
    "x_1 = m.addVar(vtype=GRB.INTEGER, name=\"x_1\") # Number of units of product 1\n",
    "x_2 = m.addVar(vtype=GRB.INTEGER, name=\"x_2\") # Number of units of product 2\n",
    "\n",
    "# Objective function: Maximize 149 x_1 + 389 x_2\n",
    "m.setObjective(149 * x_1 + 389 * x_2, GRB.MAXIMIZE)\n",
    "\n",
    "# Constraints\n",
    "m.addConstr(x_1 <= 7550, name=\"inventory_constraint_1\")\n",
    "m.addConstr(x_2 <= 6244, name=\"inventory_constraint_2\")\n",
    "m.addConstr(x_1 <= 15057, name=\"demand_constraint_1\")\n",
    "m.addConstr(x_2 <= 12474, name=\"demand_constraint_2\")\n",
    "\n",
    "# Non-negativity constraints are implicitly handled by the integer constraints (x_1, x_2 >= 0)\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    elif selected_problem == \"Facility Location Problem\" or selected_problem == \"FLP\" or selected_problem == \"Facility Location\":\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\min \\quad \\sum_{i} \\sum_{j} A_{ij} \\cdot x_{ij} + \\sum_{i} c_i \\cdot y_i$\n",
    "\n",
    "Constraints\n",
    "1. Demand Constraint:\n",
    "$\\quad \\quad \\sum_i x_{ij} = d_j, \\quad \\forall j$\n",
    "2. Capacity Constraint:\n",
    "$\\quad \\quad \\sum_j x_{ij} \\leq M \\cdot y_i, \\quad \\forall i$\n",
    "3. Non-negativity:\n",
    "$\\quad \\quad x_{ij} \\geq 0, \\quad \\forall i,j$\n",
    "4. Binary Requirement:\n",
    "$\\quad \\quad y_i \\in \\{0,1\\}, \\quad \\forall i$\n",
    "\n",
    "Retrieved Information\n",
    "$\\small d = [1083, 776, 16214, 553, 17106, 594, 732]$\n",
    "$\\small c = [102.33, 94.92, 91.83, 98.71, 95.73, 99.96, 98.16]$\n",
    "$\\small A = \\begin{bmatrix}\n",
    "1506.22 & 70.90 & 8.44 & 260.27 & 197.47 & 71.71 & 61.19 \\\\  \n",
    "1732.65 & 1780.72 & 567.44 & 448.68 & 29.00 & 1484.91 & 963.92 \\\\  \n",
    "115.66 & 100.76 & 64.68 & 1324.53 & 64.99 & 134.88 & 2102.83 \\\\  \n",
    "1254.78 & 1115.63 & 52.31 & 1036.16 & 892.63 & 1464.04 & 1383.41 \\\\  \n",
    "42.90 & 891.01 & 1013.94 & 1128.72 & 58.91 & 42.89 & 1570.31 \\\\  \n",
    "0.70 & 139.46 & 70.03 & 79.15 & 1482.00 & 0.91 & 110.46 \\\\  \n",
    "1732.30 & 1780.44 & 486.50 & 523.74 & 522.08 & 82.48 & 826.41\n",
    "\\end{bmatrix}$\n",
    "$\\small M = \\sum_j d_j = 1083 + 776 + 16214 + 553 + 17106 + 594 + 732 = 38058 $\n",
    "\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "d = np.array([1083, 776, 16214, 553, 17106, 594, 732])\n",
    "c = np.array([102.33, 94.92, 91.83, 98.71, 95.73, 99.96, 98.16])\n",
    "A = np.array([[1506.22, 70.90, 8.44, 260.27, 197.47, 71.71, 61.19],  \n",
    "[1732.65, 1780.72, 567.44, 448.68, 29.00, 1484.91, 963.92],  \n",
    "[115.66, 100.76, 64.68, 1324.53, 64.99, 134.88, 2102.83],  \n",
    "[1254.78, 1115.63, 52.31, 1036.16, 892.63, 1464.04, 1383.41],  \n",
    "[42.90, 891.01, 1013.94, 1128.72, 58.91, 42.89, 1570.31],  \n",
    "[0.70, 139.46, 70.03, 79.15, 1482.00, 0.91, 110.46],  \n",
    "[1732.30, 1780.44, 486.50, 523.74, 522.08, 82.48, 826.41]])\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization_Model\")\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(A.shape[0], A.shape[1], lb=0, name=\"x\")\n",
    "y = m.addVars(A.shape[0], vtype=GRB.BINARY, name=\"y\")\n",
    "\n",
    "# Objective function\n",
    "m.setObjective(gp.quicksum(A[i, j]*x[i, j] for i in range(A.shape[0]) for j in range(A.shape[1])) + gp.quicksum(c[i]*y[i] for i in range(A.shape[0])), GRB.MINIMIZE)\n",
    "\n",
    "# Constraints\n",
    "for j in range(A.shape[1]):\n",
    "    m.addConstr(gp.quicksum(x[i, j] for i in range(A.shape[0])) == d[j], name=f\"demand_constraint_{j}\")\n",
    "\n",
    "M = 1000000  # large number\n",
    "for i in range(A.shape[0]):\n",
    "    m.addConstr(-M*y[i] + gp.quicksum(x[i, j] for j in range(A.shape[1])) <= 0, name=f\"M_constraint_{i}\")\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "        \"\"\"\n",
    "\n",
    "    elif selected_problem == \"Assignment Problem\" or selected_problem == \"AP\" or selected_problem == \"Assignment\":\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\min \\quad \\sum_{i=1}^3 \\sum_{j=1}^3 c_{ij} \\cdot x_{ij}$\n",
    "\n",
    "Constraints\n",
    "1. Row Assignment Constraint:\n",
    "$\\quad \\quad \\sum_{j=1}^3 x_{ij} = 1, \\quad \\forall i \\in \\{1,2,3\\}$\n",
    "2. Column Assignment Constraint:\n",
    "$\\quad \\quad \\sum_{i=1}^3 x_{ij} = 1, \\quad \\forall j \\in \\{1,2,3\\}$\n",
    "3. Binary Constraint:\n",
    "$\\quad \\quad x_{ij} \\in \\{0,1\\}, \\quad \\forall i,j$\n",
    "\n",
    "Retrieved Information\n",
    "$\\small c = \\begin{bmatrix}\n",
    "3000 & 3200 & 3100 \\\\\n",
    "2800 & 3300 & 2900 \\\\\n",
    "2900 & 3100 & 3000 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "c = np.array([\n",
    "    [3000, 3200, 3100],\n",
    "    [2800, 3300, 2900],\n",
    "    [2900, 3100, 3000]\n",
    "])\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization_Model\")\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(c.shape[0], c.shape[1], vtype=GRB.BINARY, name=\"x\")\n",
    "\n",
    "# Objective function\n",
    "m.setObjective(gp.quicksum(c[i, j]*x[i, j] for i in range(c.shape[0]) for j in range(c.shape[1])), GRB.MINIMIZE)\n",
    "\n",
    "# Constraints\n",
    "for i in range(c.shape[0]):\n",
    "    m.addConstr(gp.quicksum(x[i, j] for j in range(c.shape[1])) == 1, name=f\"row_constraint_{i}\")\n",
    "\n",
    "for j in range(c.shape[1]):\n",
    "    m.addConstr(gp.quicksum(x[i, j] for i in range(c.shape[0])) == 1, name=f\"col_constraint_{j}\")\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    elif selected_problem == \"Transportation Problem\" or selected_problem == \"TP\" or selected_problem == \"Transportation\":\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\min \\quad \\sum_i \\sum_j c_{ij} \\cdot x_{ij}$\n",
    "\n",
    "Constraints\n",
    "1. Demand Constraint:\n",
    "$\\quad \\quad \\sum_i x_{ij} \\geq d_j, \\quad \\forall j$\n",
    "2. Capacity Constraint:\n",
    "$\\quad \\quad \\sum_j x_{ij} \\leq s_i, \\quad \\forall i$\n",
    "\n",
    "Retrieved Information\n",
    "$\\small d = [94, 39, 65, 435]$\n",
    "$\\small s = [2531, 20, 210, 241]$\n",
    "$\\small c = \\begin{bmatrix}\n",
    "883.91 & 0.04 & 0.03 & 44.45 \\\\\n",
    "543.75 & 23.68 & 23.67 & 447.75 \\\\\n",
    "537.34 & 23.76 & 498.95 & 440.60 \\\\\n",
    "1791.49 & 68.21 & 1432.48 & 1527.76\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization\")\n",
    "\n",
    "# Decision variables\n",
    "x_S1_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C1\")\n",
    "x_S1_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C2\")\n",
    "x_S1_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C3\")\n",
    "x_S1_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C4\")\n",
    "x_S2_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C1\")\n",
    "x_S2_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C2\")\n",
    "x_S2_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C3\")\n",
    "x_S2_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C4\")\n",
    "x_S3_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C1\")\n",
    "x_S3_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C2\")\n",
    "x_S3_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C3\")\n",
    "x_S3_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C4\")\n",
    "x_S4_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C1\")\n",
    "x_S4_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C2\")\n",
    "x_S4_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C3\")\n",
    "x_S4_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C4\")\n",
    "\n",
    "# Objective function\n",
    "m.setObjective(883.91 * x_S2_C1 + 0.04 * x_S2_C2 + 0.03 * x_S2_C3 + 44.45 * x_S2_C4 + 543.75 * x_S1_C1 + 23.68 * x_S1_C2 + 23.67 * x_S1_C3 + 447.75 * x_S1_C4 + 537.34 * x_S3_C1 + 23.76 * x_S3_C2 + 498.95 * x_S3_C3 + 440.60 * x_S3_C4 + 1791.49 * x_S4_C1 + 68.21 * x_S4_C2 + 1432.48 * x_S4_C3 + 1527.76 * x_S4_C4, GRB.MINIMIZE)\n",
    "\n",
    "# Constraints\n",
    "m.addConstr(x_S1_C1 + x_S2_C1 + x_S3_C1 + x_S4_C1 >= 94, name=\"demand_constraint1\")\n",
    "m.addConstr(x_S1_C2 + x_S2_C2 + x_S3_C2 + x_S4_C2 >= 39, name=\"demand_constraint2\")\n",
    "m.addConstr(x_S1_C3 + x_S2_C3 + x_S3_C3 + x_S4_C3 >= 65, name=\"demand_constraint3\")\n",
    "m.addConstr(x_S1_C4 + x_S2_C4 + x_S3_C4 + x_S4_C4 >= 435, name=\"demand_constraint4\")\n",
    "m.addConstr(x_S1_C1 + x_S1_C2 + x_S1_C3 + x_S1_C4 <= 2531, name=\"capacity_constraint1\")\n",
    "m.addConstr(x_S2_C1 + x_S2_C2 + x_S2_C3 + x_S2_C4 <= 20, name=\"capacity_constraint2\")\n",
    "m.addConstr(x_S3_C1 + x_S3_C2 + x_S3_C3 + x_S3_C4 <= 210, name=\"capacity_constraint3\")\n",
    "m.addConstr(x_S4_C1 + x_S4_C2 + x_S4_C3 + x_S4_C4 <= 241, name=\"capacity_constraint4\")\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "        \"\"\"\n",
    "    \n",
    "    elif selected_problem == \"Resource Allocation\" or selected_problem == \"RA\" or selected_problem == \"Resource Allocation Problem\":\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Always remember: If not specified. All the variables are non-negative interger.\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\max \\quad \\sum_i \\sum_j p_i \\cdot x_{ij}$\n",
    "\n",
    "Constraints\n",
    "1. Capacity Constraint:\n",
    "$\\quad \\quad \\sum_i a_i \\cdot x_{ij} \\leq c_j, \\quad \\forall j$\n",
    "2. Non-negativity Constraint:\n",
    "$\\quad \\quad x_{ij} \\geq 0, \\quad \\forall i,j$\n",
    "\n",
    "Retrieved Information\n",
    "$\\small p = [321, 309, 767, 300, 763, 318, 871, 522, 300, 275, 858, 593, 126, 460, 685, 443, 700, 522, 940, 598]$\n",
    "$\\small a = [495, 123, 165, 483, 472, 258, 425, 368, 105, 305, 482, 387, 469, 341, 318, 104, 377, 213, 56, 131]$\n",
    "$\\small c = [4466]$\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization_Model\")\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(20, vtype=GRB.INTEGER, name=\"x\")\n",
    "\n",
    "# Objective function\n",
    "m.setObjective(sum(x[i]*c[i] for i in range(20)), GRB.MAXIMIZE)\n",
    "\n",
    "# Constraints\n",
    "m.addConstr(sum(x[i]*w[i] for i in range(20)) <= 4466, name=\"capacity_constraint\")\n",
    "\n",
    "# Coefficients for the objective function\n",
    "c = [321, 309, 767, 300, 763, 318, 871, 522, 300, 275, 858, 593, 126, 460, 685, 443, 700, 522, 940, 598]\n",
    "\n",
    "# Coefficients for the capacity constraint\n",
    "w = [495, 123, 165, 483, 472, 258, 425, 368, 105, 305, 482, 387, 469, 341, 318, 104, 377, 213, 56, 131]\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "```\n",
    "\n",
    "-----\n",
    "Here is another simple instance for reference:\n",
    "\n",
    "Objective Function:\n",
    "$\\quad \\quad \\max \\quad \\sum_i p_i \\cdot x_i$\n",
    "\n",
    "Constraints\n",
    "1. Capacity Constraint:\n",
    "$\\quad \\quad \\sum_i a_i \\cdot x_i \\leq 180$\n",
    "2. Dependency Constraint:\n",
    "$\\quad \\quad x_1 \\leq x_3$\n",
    "3. Non-negativity Constraint:\n",
    "$\\quad \\quad x_i \\geq 0, \\quad \\forall i$\n",
    "\n",
    "Retrieved Information\n",
    "$\\small p = [888, 134, 129, 370, 921, 765, 154, 837, 584, 365]$\n",
    "$\\small a = [4, 2, 4, 3, 2, 1, 2, 1, 3, 3]$\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization_Model\")\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(10, vtype=GRB.INTEGER, name=\"x\")\n",
    "\n",
    "# Objective function\n",
    "p = [888, 134, 129, 370, 921, 765, 154, 837, 584, 365]\n",
    "m.setObjective(sum(x[i]*p[i] for i in range(10)), GRB.MAXIMIZE)\n",
    "\n",
    "# Constraints\n",
    "a = [4, 2, 4, 3, 2, 1, 2, 1, 3, 3]\n",
    "m.addConstr(sum(x[i]*a[i] for i in range(10)) <= 180, name=\"capacity_constraint\")\n",
    "m.addConstr(x[0] <= x[2], name=\"dependency_constraint\")\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "        \n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt += \"\"\"\n",
    "For example, here is a simple instance for reference:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "Maximize 5x_S + 8x_F\n",
    "Subject to\n",
    "    2x_S + 5x_F <= 200\n",
    "    x_S <= 0.3(x_S + x_F)\n",
    "    x_F >= 10\n",
    "    x_S, x_F _ Z+\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Worker_Optimization\")\n",
    "\n",
    "# Decision variables for the number of seasonal (x_S) and full-time (x_F) workers\n",
    "x_S = m.addVar(vtype=GRB.INTEGER, lb=0, name=\"x_S\")  # Number of seasonal workers\n",
    "x_F = m.addVar(vtype=GRB.INTEGER, lb=0, name=\"x_F\")  # Number of full-time workers\n",
    "\n",
    "# Objective function: Maximize Z = 5x_S + 8x_F\n",
    "m.setObjective(5 * x_S + 8 * x_F, GRB.MAXIMIZE)\n",
    "\n",
    "# Constraints\n",
    "m.addConstr(2 * x_S + 5 * x_F <= 200, name=\"resource_constraint\")\n",
    "m.addConstr(x_S <= 0.3 * (x_S + x_F), name=\"seasonal_ratio_constraint\")\n",
    "m.addConstr(x_F >= 10, name=\"full_time_minimum_constraint\")\n",
    "\n",
    "# Non-negativity constraints are implicitly handled by the integer constraints (x_S, x_F >= 0)\n",
    "\n",
    "# Solve the model\n",
    "m.optimize()\n",
    "```\n",
    "The another example is:\n",
    "\n",
    "Mathematical Optimization Model:\n",
    "Minimize 919x_11 + 556x_12 + 951x_13 + 21x_21 + 640x_22 + 409x_23 + 59x_31 + 786x_32 + 304x_33\n",
    "Subject to\n",
    "    x_11 + x_12 + x_13 = 1\n",
    "    x_21 + x_22 + x_23 = 1\n",
    "    x_31 + x_32 + x_33 = 1\n",
    "    x_11 + x_21 + x_31 = 1\n",
    "    x_12 + x_22 + x_32 = 1\n",
    "    x_13 + x_23 + x_33 = 1\n",
    "    x_11, x_12, x_13, x_21, x_22, x_23, x_31, x_32, x_33 ∈ {{0,1}}\n",
    "\n",
    "\n",
    "The corresponding Python code for this instance is as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "c = np.array([\n",
    "    [919, 556, 951],\n",
    "    [21, 640, 409],\n",
    "    [59, 786, 304]\n",
    "])\n",
    "\n",
    "# Create the model\n",
    "m = gp.Model(\"Optimization_Model\")\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(c.shape[0], c.shape[1], vtype=GRB.BINARY, name=\"x\")\n",
    "\n",
    "# Objective function\n",
    "m.setObjective(gp.quicksum(c[i, j]*x[i, j] for i in range(c.shape[0]) for j in range(c.shape[1])), GRB.MINIMIZE)\n",
    "\n",
    "# Constraints\n",
    "for i in range(c.shape[0]):\n",
    "    m.addConstr(gp.quicksum(x[i, j] for j in range(c.shape[1])) == 1, name=f\"row_constraint_{i}\")\n",
    "\n",
    "for j in range(c.shape[1]):\n",
    "    m.addConstr(gp.quicksum(x[i, j] for i in range(c.shape[0])) == 1, name=f\"col_constraint_{j}\")\n",
    "\n",
    "# Solve the model\n",
    "m.optimize() \n",
    "```\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=prompt) \n",
    "    ]\n",
    "\n",
    "    response = llm_code.invoke(messages)\n",
    "\n",
    "\n",
    "    print(response.content)\n",
    "\n",
    "    return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code(text):\n",
    "    pattern = r'```python(.*?)```'\n",
    "    match = re.search(pattern, text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def run_gurobi_code(code):\n",
    "    try:\n",
    "        \n",
    "        local_vars = {'gp': gp, 'GRB': GRB, '__builtins__': __builtins__}\n",
    "        \n",
    "        exec(code, local_vars)\n",
    "        \n",
    "        models = []\n",
    "        for var_name, var_value in local_vars.items():\n",
    "            if isinstance(var_value, gp.Model):\n",
    "                models.append(var_value)\n",
    "        \n",
    "        if models:\n",
    "            model = models[-1] \n",
    "            if model.status == GRB.OPTIMAL:\n",
    "                return model.objVal\n",
    "            else:\n",
    "                print(f\"Model status is not optimal: {model.status}\")\n",
    "                return None\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Gurobi code: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_braces(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace every { with {{  and every } with }}\n",
    "    so that ChatPromptTemplate won't treat them as variables.\n",
    "    \"\"\"\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "REF_CSV_PATH = \"Large_Scale_Or_Files/RefData.csv\"\n",
    "ref_docs = CSVLoader(file_path=REF_CSV_PATH, encoding=\"utf-8\").load()\n",
    "ref_store: FAISS = FAISS.from_documents(ref_docs, embeddings)\n",
    "#②\n",
    "# def split_by_newline(docs, min_len=1):\n",
    "#     out = []\n",
    "#     for d in docs:\n",
    "#         lines = d.page_content.splitlines()\n",
    "#         for line in lines:\n",
    "#             line = line.strip()\n",
    "#             if len(line) >= min_len:\n",
    "#                 out.append(Document(page_content=line, metadata=d.metadata))\n",
    "#     return out\n",
    "\n",
    "# ref_docs_split = split_by_newline(ref_docs)\n",
    "# ref_store = FAISS.from_documents(ref_docs_split, embeddings)\n",
    "#③\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,      # 先用 800~1500 试\n",
    "#     chunk_overlap=100\n",
    "# )\n",
    "\n",
    "# ref_docs_split = splitter.split_documents(ref_docs)\n",
    "# ref_store = FAISS.from_documents(ref_docs_split, embeddings)\n",
    "\n",
    "def retrieve_ref_examples(query: str, k: int = 5) -> List[Dict]:\n",
    "    retriever = ref_store.as_retriever(search_kwargs={\"k\": k})\n",
    "    docs = retriever.invoke(query)   # ✅ LangChain v1\n",
    "    return docs\n",
    "\n",
    "def build_dynamic_few_shot(query: str, k: int = 5) -> str:\n",
    "    examples = []\n",
    "    for doc in retrieve_ref_examples(query, k=k):\n",
    "        text = doc.page_content\n",
    "        prompt_part = text.split(\"prompt:\", 1)[1].split(\"Data_address:\", 1)[0].strip()\n",
    "        data_addr   = text.split(\"Data_address:\", 1)[1].split(\"Label:\", 1)[0].strip()\n",
    "        label_part = text.split(\"Label:\", 1)[1].split(\"Related:\", 1)[0].strip()\n",
    "\n",
    "        data_blocks = []\n",
    "        for fp in map(str.strip, data_addr.splitlines()):\n",
    "            if not fp:\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                header = f\"[Data from {os.path.basename(fp)}]\"\n",
    "                rows = \"\\n\".join(\n",
    "                    \", \".join(f\"{col}={row[col]}\" for col in df.columns)\n",
    "                    for _, row in df.iterrows()\n",
    "                )\n",
    "                data_blocks.append(header + \"\\n\" + rows)\n",
    "            except Exception as e:\n",
    "                data_blocks.append(f\"[Could not read {fp}: {e}]\")\n",
    "\n",
    "        data_section = \"\\n\".join(data_blocks) if data_blocks else \"[No data found]\"\n",
    "\n",
    "        example_str=(\n",
    "            f\"<EXAMPLE>\\n\"\n",
    "            f\"Query: {prompt_part}\\n\"\n",
    "            f\"Data:\\n{data_section}\\n\"\n",
    "            f\"Answer: {label_part}\\n\"\n",
    "            f\"</EXAMPLE>\"\n",
    "        )\n",
    "        example_str = escape_braces(example_str)\n",
    "        examples.append(example_str)\n",
    "\n",
    "\n",
    "    return \"\\n\\n\".join(examples)\n",
    "\n",
    "ALLOWED_CATS = [\n",
    "    \"Network Revenue Management\",\n",
    "    \"Resource Allocation\",\n",
    "    \"Transportation\",\n",
    "    \"Facility Location Problem\",\n",
    "    \"Assignment Problem\",\n",
    "    \"Others with CSV\",\n",
    "]\n",
    "\n",
    "FEW_SHOT_FALLBACK_NO_CSV = \"\"\"\n",
    "<EXAMPLE>\n",
    "Query: A book distributor moves books between two warehouses and two libraries (no csv mentioned).\n",
    "Answer: Others without CSV\n",
    "</EXAMPLE>\n",
    "\"\"\".strip()\n",
    "\n",
    "CLASSIFY_SYS_MSG = (\n",
    "    \"You are an assistant that classifies operations-research problems. \"\n",
    "    \"Return **exactly one** category from the following list:\\n\"\n",
    "    f\"{', '.join(ALLOWED_CATS)}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. Follow the provided few-shot examples strictly.\\n\"\n",
    "    \"2. Output only the category name, without explanation.\"\n",
    "    \"3. For mixed-type problems such as including some additional requirements in resource allocation problem, return the label 'Others with CSV' instead of 'resource allocation'.\"\n",
    "\n",
    ").strip()\n",
    "\n",
    "\n",
    "def classify_problem(user_query: str) -> str:\n",
    "    few_shot_dynamic = build_dynamic_few_shot(user_query, k=5)\n",
    "    few_shot_section = few_shot_dynamic + \"\\n\\n\" + FEW_SHOT_FALLBACK_NO_CSV\n",
    "    few_shot_section = escape_braces(few_shot_section)\n",
    "\n",
    "    prompt_tmpl = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", CLASSIFY_SYS_MSG),\n",
    "            (\"assistant\", few_shot_section),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    msgs = prompt_tmpl.format_messages(input=user_query)\n",
    "    resp = llm1.invoke(msgs)           \n",
    "    category = resp.content.strip()   \n",
    "    return category if category in ALLOWED_CATS else \"Others with CSV\"\n",
    "\n",
    "\n",
    "def process_dataset_address(dataset_address: str) -> List[Document]:\n",
    "\n",
    "    documents = []\n",
    "    file_addresses = dataset_address.strip().split('\\n')  \n",
    "    for file_idx, file_address in enumerate(file_addresses, start=1):\n",
    "        try:\n",
    "            df = pd.read_csv(file_address.strip())  \n",
    "            file_name = file_address.strip().split('/')[-1]  \n",
    "            for row_idx, row in df.iterrows():\n",
    "                page_content = \", \".join([f\"{col} = {row[col]}\" for col in df.columns])\n",
    "                documents.append(Document(page_content=page_content))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_address}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "NRM_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_NRM2_MD.csv\"\n",
    "nrm_docs = CSVLoader(file_path=NRM_RAG_PATH, encoding=\"utf-8\").load()\n",
    "nrm_store: FAISS = FAISS.from_documents(nrm_docs, embeddings)\n",
    "\n",
    "RA_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_RA2_MD.csv\"\n",
    "ra_docs = CSVLoader(file_path=RA_RAG_PATH, encoding=\"utf-8\").load()\n",
    "ra_store: FAISS = FAISS.from_documents(ra_docs, embeddings)\n",
    "\n",
    "TP_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_TP2_MD.csv\"\n",
    "tp_docs = CSVLoader(file_path=TP_RAG_PATH, encoding=\"utf-8\").load()\n",
    "tp_store: FAISS = FAISS.from_documents(tp_docs, embeddings)\n",
    "\n",
    "FLP_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_FLP2_MD.csv\"\n",
    "flp_docs = CSVLoader(file_path=FLP_RAG_PATH, encoding=\"utf-8\").load()\n",
    "flp_store: FAISS = FAISS.from_documents(flp_docs, embeddings)\n",
    "\n",
    "AP_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_AP2_MD.csv\"\n",
    "ap_docs = CSVLoader(file_path=AP_RAG_PATH, encoding=\"utf-8\").load()\n",
    "ap_store: FAISS = FAISS.from_documents(ap_docs, embeddings)\n",
    "\n",
    "OTHERS_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_Others.csv\"\n",
    "Others_docs = CSVLoader(file_path=OTHERS_RAG_PATH, encoding=\"utf-8\").load()\n",
    "Others_store: FAISS = FAISS.from_documents(Others_docs, embeddings)\n",
    "\n",
    "OW_RAG_PATH = \"Large_Scale_Or_Files/RAG_Example_Others_Without_CSV.csv\"\n",
    "OW_docs = CSVLoader(file_path=OW_RAG_PATH, encoding=\"utf-8\").load()\n",
    "OW_store: FAISS = FAISS.from_documents(OW_docs, embeddings)\n",
    "\n",
    "\n",
    "def retrieve_examples(store, query: str, k: int = 1):\n",
    "    retriever = store.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever.invoke(query)   # ✅ LangChain v1\n",
    "\n",
    "\n",
    "def build_few_shot(store, user_query: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    Build few-shot block.  \n",
    "    Each example includes:\n",
    "      • Question (prompt)\n",
    "      • A 'Thought' line reminding to look for <Related> rows\n",
    "      • Final Answer (Label)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    label_part = \"\"\n",
    "\n",
    "    # retrieve k most-similar reference examples\n",
    "    for doc in retrieve_examples(store, user_query, k=k):\n",
    "        txt = doc.page_content\n",
    "        \n",
    "        split_at_formulation = txt.split(\"Data_address:\", 1)\n",
    "        prompt_part = split_at_formulation[0].replace(\"prompt:\", \"\").strip()  \n",
    "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
    "        data_addr = split_at_address[0].strip()\n",
    "\n",
    "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
    "        label_part = split_at_label[0].strip()  \n",
    "        related_part = split_at_label[1].strip()\n",
    "        \n",
    "        # ---------- read CSVs listed in Data_address ----------\n",
    "        data_blocks = []\n",
    "        for fp in map(str.strip, data_addr.splitlines()):\n",
    "            if not fp:\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                df_show = df.head()\n",
    "                header = f\"[{os.path.basename(fp)} | showing {len(df_show)}/{len(df)} rows]\"\n",
    "                rows   = \"\\n\".join(\n",
    "                    \", \".join(f\"{c}={row[c]}\" for c in df_show.columns)\n",
    "                    for _, row in df_show.iterrows()\n",
    "                )\n",
    "                data_blocks.append(header + \"\\n\" + rows)\n",
    "            except Exception as e:\n",
    "                data_blocks.append(f\"[Could not read {fp}: {e}]\")\n",
    "\n",
    "        data_section = \"\\n\".join(data_blocks) if data_blocks else \"[No data found]\"\n",
    "\n",
    "        # ---------- compose few-shot example ----------\n",
    "        ex = (\n",
    "\"<EXAMPLE>\\n\"\n",
    "f\"Question: {prompt_part}\\n\\n\"\n",
    "f'Thought: Retrieve rows related to \"{related_part}\"; then use the retrieved CSV to define sets and coefficients; choose variable domains (binary/integer for selections or lots or assignments, nonnegative reals for flows or amounts); build the linear objective; add capacity or supply bounds, demand/target constraints, flow conservation or assignment equalities, and any linking, batch, fixed-charge, or ratio/share constraints; then state nonnegativity and variable domains; output only the LP.'\n",
    "\"from the user's CSV file(s) and then formulate the optimisation model.\\n\\n\"\n",
    "f\"Here is the data from retrieval in this demo example:\\n{data_section}\\n\\n\"\n",
    "\"Final Answer:\\n\"\n",
    "f\"{label_part}\\n\"\n",
    "\"</EXAMPLE>\"\n",
    "        )\n",
    "        examples.append(escape_braces(ex))\n",
    "\n",
    "\n",
    "    return \"\\n\\n\".join(examples),label_part\n",
    "\n",
    "\n",
    "def build_few_shot_Other(store, user_query: str, k: int = 1,t='Model'):\n",
    "    \"\"\"\n",
    "    Build few-shot block.  \n",
    "    Each example includes:\n",
    "      • Question (prompt)\n",
    "      • A 'Thought' line reminding to look for <Related> rows\n",
    "      • Final Answer (Label)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    label_part = \"\"\n",
    "\n",
    "    # retrieve k most-similar reference examples\n",
    "    for doc in retrieve_examples(store, user_query, k=k):\n",
    "        txt = doc.page_content\n",
    "        \n",
    "        split_at_formulation = txt.split(\"Data_address:\", 1)\n",
    "        prompt_part = split_at_formulation[0].replace(\"prompt:\", \"\").strip()  \n",
    "        if t =='Model':\n",
    "            split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
    "        else:\n",
    "            split_at_address = split_at_formulation[1].split(\"Label_Code:\", 1)\n",
    "        data_addr = split_at_address[0].strip()\n",
    "\n",
    "        data_blocks = []\n",
    "        for fp in map(str.strip, data_addr.splitlines()):\n",
    "            if not fp:\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                df_show = df.head()\n",
    "                header = f\"[{os.path.basename(fp)} | showing {len(df_show)}/{len(df)} rows]\"\n",
    "                rows   = \"\\n\".join(\n",
    "                    \", \".join(f\"{c}={row[c]}\" for c in df_show.columns)\n",
    "                    for _, row in df_show.iterrows()\n",
    "                )\n",
    "                data_blocks.append(header + \"\\n\" + rows)\n",
    "            except Exception as e:\n",
    "                data_blocks.append(f\"[Could not read {fp}: {e}]\")\n",
    "\n",
    "        data_section = \"\\n\".join(data_blocks) if data_blocks else \"[No data found]\"\n",
    "\n",
    "        if t == 'Model':\n",
    "            # ---------- compose few-shot example ----------\n",
    "            ex = (\n",
    "    \"<EXAMPLE>\\n\"\n",
    "    f\"Question: {prompt_part}\\n\"\n",
    "    f'Thought: Load the referenced CSV tables and extract needed columns for coefficients and limits; define index sets from table rows and columns; choose variable types (counts and on-off as integer or binary, flows and amounts as nonnegative reals); then build the model using only the retrieved data.'\n",
    "    \"Action: CSVQA\\n\"\n",
    "    \"Action Input: Retrieve all necesscary data\\n\"\n",
    "    f\"Observation: Here is the data from retrieval in this demo example:\\n{data_section}\\n\"\n",
    "    \"Final Answer:\\n\"\n",
    "    f\"{label_part}\\n\"\n",
    "    \"</EXAMPLE>\"\n",
    "            )\n",
    "            examples.append(escape_braces(ex))\n",
    "        else:\n",
    "            ex = (\n",
    "    \"<EXAMPLE>\\n\"\n",
    "    f\"Question: {prompt_part}\\n\"\n",
    "    \"Thought: I need to inspect the CSV file's structure (columns and data types) before writing the code.\"\n",
    "    \"Action: GetCSVSchema\\n\"\n",
    "    f\"Action Input: {data_addr}\\n\"\n",
    "    f\"Observation: \\n{data_section}\\n\"\n",
    "    \"Final Answer:\\n\"\n",
    "    f\"{label_part}\\n\"\n",
    "    \"</EXAMPLE>\"\n",
    "            )\n",
    "            examples.append(escape_braces(ex))\n",
    "\n",
    "    return \"\\n\\n\".join(examples),label_part\n",
    "\n",
    "\n",
    "def build_few_shot_TP(store, user_query: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    Build few-shot block.  \n",
    "    Each example includes:\n",
    "      • Question (prompt)\n",
    "      • A 'Thought' line reminding to look for <Related> rows\n",
    "      • Final Answer (Label)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    label_part = \"\"\n",
    "\n",
    "    # retrieve k most-similar reference examples\n",
    "    for doc in retrieve_examples(store, user_query, k=k):\n",
    "        txt = doc.page_content\n",
    "        \n",
    "        split_at_formulation = txt.split(\"Data_address:\", 1)\n",
    "        prompt_part = split_at_formulation[0].replace(\"prompt:\", \"\").strip()  \n",
    "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
    "        data_addr = split_at_address[0].strip()\n",
    "\n",
    "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
    "        label_part = split_at_label[0].strip()  \n",
    "        related_part = split_at_label[1].strip()\n",
    "        \n",
    "        # ---------- read CSVs listed in Data_address ----------\n",
    "        data_blocks = []\n",
    "        for fp in map(str.strip, data_addr.splitlines()):\n",
    "            if not fp:\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(fp)\n",
    "                df_show = df.head()\n",
    "                header = f\"[{os.path.basename(fp)} | showing {len(df_show)}/{len(df)} rows]\"\n",
    "                rows   = \"\\n\".join(\n",
    "                    \", \".join(f\"{c}={row[c]}\" for c in df_show.columns)\n",
    "                    for _, row in df_show.iterrows()\n",
    "                )\n",
    "                data_blocks.append(header + \"\\n\" + rows)\n",
    "            except Exception as e:\n",
    "                data_blocks.append(f\"[Could not read {fp}: {e}]\")\n",
    "\n",
    "        data_section = \"\\n\".join(data_blocks) if data_blocks else \"[No data found]\"\n",
    "        ex = (\n",
    "            f\"Question: {prompt_part}\\n\\n\"\n",
    "\n",
    "            f\"Thought: The user wants me to formulate an optimization model. First, I must use the `csvqa` tool to get the data from the file paths. \\n\"\n",
    "            \"Action: csvqa \\n\"\n",
    "            f\"Action Input: {data_addr}\\n\"\n",
    "            \"Observation: d = [11, 1148, 54, 833], s = [4, 575, 1504], c = \\\\begin{{bmatrix}} 0.63 & 49.71 & 33.75 & 1570.67 \\\\605.47 & 64.53 & 478.47 & 887.04 \\\\1139.04 & 4.78 & 1805.62 & 1302.89\\\\end{{bmatrix}}$\\n\"\n",
    "            \"Thought: I have successfully received the data for demand (d), supply (s), and costs (c). Now I will formulate the complete mathematical model in Markdown as my final answer.\\n\"\n",
    "            \"Final Answer: FINAL_MODEL_OUTPUT: \\n\"\n",
    "            f\"{label_part}\\n\"\n",
    "            \"</EXAMPLE>\"\n",
    "        )\n",
    "        examples.append(escape_braces(ex))\n",
    "\n",
    "\n",
    "    return \"\\n\\n\".join(examples),label_part\n",
    "\n",
    "\n",
    "\n",
    "DISALLOWED = {\n",
    "    \"Revenue\",\n",
    "    \"sales\",\n",
    "    \"demand\",\n",
    "    \"inventory\",\n",
    "    \"initial inventory\",\n",
    "    \"product\",\n",
    "    \"product name\",\n",
    "    \"column\",\n",
    "    \"row\",\n",
    "    \"none\",\n",
    "}\n",
    "\n",
    "def _post_process_kw(raw_kw: str) -> str:\n",
    "    \"\"\"Clean quotes & punctuation, lower-case check against disallowed set.\"\"\"\n",
    "    kw = re.sub(r\"[\\\"'“”‘’]\", \"\", raw_kw).strip()\n",
    "    return \"\" if kw.lower() in DISALLOWED or kw == \"\" else kw\n",
    "\n",
    "def extract_retrieval_keyword(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Return ONE product / item keyword; empty string means 'no keyword'.\n",
    "    Filters out obvious column names via DISALLOWED set.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Read the USER QUESTION and output ONE product name or product type \"\n",
    "                \"to look up in the CSV files. \"\n",
    "                \"If the question doesn't mention a specific product or type, output NONE.\\n\\n\"\n",
    "                \"❌  Do NOT output column names like 'Revenue', 'Inventory', 'Demand', etc.\\n\"\n",
    "                \"✅  Example:\\n\"\n",
    "                \"   Question: The store wants to maximise revenue for the Nike x Olivia Kim shoes …\\n\"\n",
    "                \"   Answer: Nike x Olivia Kim\\n\"\n",
    "            ),\n",
    "            (\"human\", \"{q}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Get raw LLM output\n",
    "    raw_kw = (prompt | llm2).invoke({\"q\": user_query}).content.strip()\n",
    "    return _post_process_kw(raw_kw)\n",
    "\n",
    "def extract_retrieval_keyword_other(user_query: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract a specific product/item/category keyword ONLY if the user explicitly\n",
    "    states that the modeling should focus on a subset of the data.\n",
    "    Otherwise, return NONE.\n",
    "    \n",
    "    Also returns the English-translated version of the original query.\n",
    "\n",
    "    Returns:\n",
    "        (keyword: str, query_en: str)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 2: Keyword extraction prompt\n",
    "    keyword_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"Read the USER QUESTION and check if it explicitly states that the modeling \"\n",
    "                \"should only consider a specific subset of the data, such as one product, \"\n",
    "                \"one factory, or one category.\\n\"\n",
    "                \"- If the query does NOT clearly restrict to a subset, output NONE.\\n\"\n",
    "                \"- If it does, output ONLY the concise English name of that product/item/category.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. NEVER output column names like Revenue, Inventory, Demand, Sales, etc.\\n\"\n",
    "                \"2. NEVER output vague or generic terms like 'market', 'production', 'data', 'factory', etc.\\n\"\n",
    "                \"3. Keyword must be specific.\\n\"\n",
    "                \"4. Output should ONLY be the keyword or NONE.\"\n",
    "            )\n",
    "        ),\n",
    "        (\"human\", \"{q}\")\n",
    "    ])\n",
    "    messages = keyword_prompt.format_messages(q=user_query)   \n",
    "    resp = llm2.invoke(messages)                    \n",
    "    raw_kw = resp.content.strip()\n",
    "\n",
    "    # Step 3: Post-processing filter\n",
    "    def _post_process_kw_en(s: str) -> str:\n",
    "        disallowed = {\n",
    "            'revenue', 'inventory', 'demand', 'sales', 'profit',\n",
    "            'market', 'production', 'data', 'factory', 'plant', 'type',\n",
    "            'cost', 'price', 'amount', 'value', 'total', 'quantity'\n",
    "        }\n",
    "        kw = s.strip().lower()\n",
    "        # If NONE or empty, return NONE\n",
    "        if not kw or kw == 'none':\n",
    "            return 'NONE'\n",
    "        # Filter out disallowed and overly generic terms\n",
    "        if kw in disallowed or any(word in disallowed for word in kw.split()):\n",
    "            return 'NONE'\n",
    "        # Keep keyword in original English formatting (capitalize if needed)\n",
    "        return s.strip()\n",
    "\n",
    "    keyword_final = _post_process_kw_en(raw_kw)\n",
    "\n",
    "    return keyword_final\n",
    "\n",
    "\n",
    "def llm_extract_keyword(llm: ChatOllama, user_query: str) -> str:\n",
    "    extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "         \"You are a precise text analyzer. Your task is to identify the specific **product ID, category, or classification** that the user's request is focusing on for data filtering. \"\n",
    "         \"If no specific filtering condition is clearly mentioned, return **ONLY** the phrase: 'all relevant data'.\"),\n",
    "        (\"human\", \"Analyze the following request: {query}\")\n",
    "    ])\n",
    "    \n",
    "    extraction_chain = (\n",
    "        extraction_prompt \n",
    "        | llm  \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        keyword = extraction_chain.invoke({\"query\": user_query}).strip().strip(\"'\\\"\").lower()\n",
    "        if not keyword or keyword in [\"the\", \"a\", \"of\", \"products\", \"items\"]:\n",
    "            return \"all relevant data\"\n",
    "        return keyword\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] LLM Keyword Extraction Failed: {e}\")\n",
    "        return \"all relevant data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_lists_from_docs_no_mapping(documents: List[Document]):\n",
    "    if not documents:\n",
    "        return {\"ERROR\": \"[FATAL_EXTRACTION_FAILURE: No documents retrieved.]\"}\n",
    "    first_content = documents[0].page_content\n",
    "    key_value_pairs = first_content.split(', ')\n",
    "    column_names = []\n",
    "    \n",
    "    for pair in key_value_pairs:\n",
    "        if '=' in pair:\n",
    "            key = pair.split('=', 1)[0].strip()\n",
    "            column_names.append(key)\n",
    "\n",
    "    if not column_names:\n",
    "        return {\"ERROR\": \"[FATAL_MAPPING_FAILURE: Could not identify any column names in the documents.]\"}\n",
    "    extracted_lists: Dict[str, List[str]] = {name: [] for name in column_names}\n",
    "\n",
    "    for doc in documents:\n",
    "        content = doc.page_content\n",
    "        \n",
    "        for col_name in column_names:\n",
    "            tag = f\"{col_name} =\"\n",
    "            value_str = 'N/A' \n",
    "            \n",
    "            try:\n",
    "                start_index = content.index(tag) + len(tag)\n",
    "                end_index = content.find(',', start_index)\n",
    "                \n",
    "                if end_index == -1:\n",
    "                    value_str = content[start_index:].strip()\n",
    "                else:\n",
    "                    value_str = content[start_index:end_index].strip()\n",
    "                \n",
    "                value_str = value_str.replace('{', '').replace('}', '').replace('\"', '').strip()\n",
    "                \n",
    "            except ValueError:\n",
    "                pass \n",
    "            \n",
    "            extracted_lists[col_name].append(value_str)\n",
    "            \n",
    "    return extracted_lists\n",
    "\n",
    "\n",
    "def process_dataset_address(dataset_address: str) -> List[Document]:\n",
    "    documents = []\n",
    "    file_addresses = dataset_address.strip().split('\\n')\n",
    "    for file_address in file_addresses:\n",
    "        file_address = file_address.strip()\n",
    "        if not file_address: continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_address)\n",
    "            for row_idx, row in df.iterrows():\n",
    "                page_content = \", \".join([f\"{col} = {row[col]}\" for col in df.columns])\n",
    "                documents.append(Document(page_content=page_content))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_address}: {e}\")\n",
    "            continue\n",
    "    return documents\n",
    "\n",
    "def process_dataset_address_RA(dataset_address: str) -> List[Document]:\n",
    "    \"\"\"Reads CSV files and creates a list of Documents for FAISS indexing.\n",
    "    \n",
    "    MODIFIED: This version adds the source filename to each document's metadata.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    file_addresses = dataset_address.strip().split('\\n')\n",
    "    for file_address in file_addresses:\n",
    "        file_address = file_address.strip()\n",
    "        if not file_address: continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_address)\n",
    "            \n",
    "            # --- 1. Get the filename from the full path ---\n",
    "            file_name = os.path.basename(file_address)\n",
    "            \n",
    "            for row_idx, row in df.iterrows():\n",
    "                page_content = \", \".join([f\"{col} = {row[col]}\" for col in df.columns])\n",
    "                \n",
    "                # --- 2. Create the metadata dictionary ---\n",
    "                metadata = {\"source\": file_name}\n",
    "                \n",
    "                # --- 3. Pass the metadata when creating the Document ---\n",
    "                documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_address}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "def process_dataset_address_vector_FLP(dataset_address: str) -> List[Document]:\n",
    "    documents = []\n",
    "    file_addresses = dataset_address.strip().split('\\n')\n",
    "    df_index = 0\n",
    "    data_description = \" \"\n",
    "    for file_address in file_addresses:\n",
    "        try:\n",
    "            df = pd.read_csv(file_address) \n",
    "            file_name = file_address.split('/')[-1] \n",
    "            if 'demand' in df.columns:\n",
    "                result = df['demand'].values.tolist()\n",
    "                data_description += \"d=\" + str(result) + \"\\n\"\n",
    "            elif 'fixed_costs' in df.columns:\n",
    "                result = df['fixed_costs'].values.tolist()\n",
    "                data_description +=\"c=\" + str(result) + \"\\n\"\n",
    "            elif df_index == 2:\n",
    "                matrix = df.iloc[:,1:].values\n",
    "                data_description +=\"A=\" + np.array_str(matrix)+ \".\"\n",
    "            else:\n",
    "                for row_idx, row in df.iterrows():\n",
    "                    data_description += \", \".join([f\"{col} = {row[col]}\" for col in df.columns])\n",
    "            df_index += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_address}: {e}\")\n",
    "    documents = [data_description]\n",
    "    return documents\n",
    "\n",
    "\n",
    "def process_dataset_address_vector_AP(dataset_address: str) -> List[Document]:\n",
    "    dfs=[]\n",
    "    file_addresses = dataset_address.strip().split('\\n')\n",
    "    df_index = 0\n",
    "    data_description = \" \"\n",
    "    for file_address in file_addresses:\n",
    "        try:\n",
    "            df = pd.read_csv(file_address) \n",
    "            file_name = file_address.split('/')[-1] \n",
    "            matrix = df.iloc[:,1:].values\n",
    "            data_description +=\"C=\" + np.array_str(matrix)+ \".\"\n",
    "            dfs.append((file_name, df))\n",
    "            df_index += 1\n",
    "            dfs.append((file_name, df))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_address}: {e}\")\n",
    "    documents = [data_description]\n",
    "    return documents\n",
    "\n",
    "def process_dataset_address_vector_TP(dataset_address: str) -> List[Document]:\n",
    "    data = []\n",
    "    dfs=[]\n",
    "\n",
    "    file_addresses = dataset_address.strip().split('\\n')\n",
    "    for file_address in file_addresses:\n",
    "        try:\n",
    "            df = pd.read_csv(file_address) \n",
    "            file_name = file_address.split('/')[-1] \n",
    "            dfs.append((file_name, df))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_address}: {e}\")\n",
    "\n",
    "    for df_index, (file_name, df) in enumerate(dfs):\n",
    "        data.append(f\"\\nDataFrame {df_index + 1} - {file_name}:\\n\")\n",
    "\n",
    "        for i, r in df.iterrows():\n",
    "            description = \"\"\n",
    "            description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
    "            data.append(description + \"\\n\")\n",
    "\n",
    "    documents = [content for content in data]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def identify_data_roles(dfs: List[pd.DataFrame], llm) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyzes the structure of a list of DataFrames using an LLM and returns a JSON object\n",
    "    mapping each data role to its corresponding column name and DataFrame index.\n",
    "    \"\"\"\n",
    "    if not dfs:\n",
    "        return {}\n",
    "\n",
    "    # Create a string describing the structure of all DataFrames to serve as context for the LLM.\n",
    "    context_str = \"\"\n",
    "    for i, df in enumerate(dfs):\n",
    "        context_str += f\"DataFrame {i} columns: {df.columns.tolist()}\\n\"\n",
    "        context_str += f\"First two rows of DataFrame {i}:\\n{df.head(2).to_string()}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data analyst. Your task is to analyze the structure of the DataFrames below and identify the role each one plays.\n",
    "    You need to identify three roles: 'demand' (customer needs), 'supply' (supplier capacity), and 'costs' (a cost matrix).\n",
    "\n",
    "    Here is the structural information for all DataFrames:\n",
    "    ---\n",
    "    {context_str}\n",
    "    ---\n",
    "\n",
    "    Based on this information, please return your analysis in a strict JSON object format. The JSON object must contain three keys: 'demand', 'supply', and 'costs'.\n",
    "    - For 'demand' and 'supply', provide their DataFrame index (`df_index`) and the specific column name (`column_name`).\n",
    "    - For 'costs', provide its DataFrame index (`df_index`) and the name of the column used to identify the supplier ID (`supplier_id_column`).\n",
    "\n",
    "    JSON format example:\n",
    "    {{\n",
    "      \"demand\": {{ \"df_index\": 0, \"column_name\": \"requirement\" }},\n",
    "      \"supply\": {{ \"df_index\": 1, \"column_name\": \"capacity\" }},\n",
    "      \"costs\": {{ \"df_index\": 2, \"supplier_id_column\": \"supplier_id\" }}\n",
    "    }}\n",
    "    \n",
    "    Output the JSON object directly, without any other explanations or markdown.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        # Attempt to parse the string returned by the LLM into JSON.\n",
    "        # For newer versions of LangChain, you can use .with_structured_output(MyPydanticModel) for more stable JSON.\n",
    "        roles = json.loads(response.content)\n",
    "        return roles\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to identify data roles: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_dataframes_from_files(file_paths_str: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    A general-purpose function to read one or more CSV file paths\n",
    "    and load them into a list of DataFrames. This version is enhanced\n",
    "    to handle malformed input strings where paths might be joined together.\n",
    "    \"\"\"\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # --- Robust Parsing Logic ---\n",
    "    # 1. Normalize the input by replacing common incorrect separators (like spaces) with a known marker.\n",
    "    #    We'll use the '.csv' extension as an anchor.\n",
    "    # 2. This line finds every '.csv' and inserts a newline character right after it,\n",
    "    #    effectively splitting mashed-together paths.\n",
    "    normalized_str = file_paths_str.replace('.csv ', '.csv\\n')\n",
    "\n",
    "    # 3. Now, split the normalized string by newlines and filter out any empty strings.\n",
    "    file_paths = [path.strip() for path in normalized_str.strip().split('\\n') if path.strip()]\n",
    "    \n",
    "    print(f\"Preparing to load files (after parsing): {file_paths}\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if not file_path:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            list_of_dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File '{file_path}' not found, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: An error occurred while loading '{file_path}': {e}, skipping.\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"Warning: No data was loaded, returning an empty list.\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded {len(list_of_dfs)} DataFrame(s).\")\n",
    "        \n",
    "    return list_of_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_NRM_response(user_query: str, dataset_address: str) -> str:\n",
    "    \n",
    "    keyword = llm_extract_keyword(llm1, user_query)\n",
    "    action_input_kw = keyword if keyword else \"all relevant data\"\n",
    "    few_shot_block, _ = build_few_shot(nrm_store, user_query, k=1)\n",
    "    print(\"[INFO] Loading all documents from CSVs...\")\n",
    "    user_docs = process_dataset_address(dataset_address)\n",
    "    if not user_docs:\n",
    "        return \"Final Answer: [Could not process dataset. Please check file paths.]\"\n",
    "        \n",
    "    print(\"[INFO] Creating a complete FAISS index...\")\n",
    "    user_store = FAISS.from_documents(user_docs, embeddings)\n",
    "    retriever = user_store.as_retriever(search_kwargs={\"k\": 300}) \n",
    "    print(\"[INFO] Retriever is ready.\")\n",
    "\n",
    "    def csvqa_list_extractor_tool_func(query: str) -> str:\n",
    "        print(f\"[INFO] Tool received query: '{query}'\")\n",
    "        is_keyword_query = ' ' not in query.strip() and len(query.strip()) < 10\n",
    "\n",
    "        docs_to_process = []\n",
    "\n",
    "        if \"all relevant data\" in query:\n",
    "            print(f\"[INFO] Semantic query detected. Using vector retriever.\")\n",
    "            docs_to_process = retriever.invoke(query)\n",
    "        else:\n",
    "            print(f\"[INFO] Keyword query detected. Using precise code filtering on all {len(user_docs)} documents.\")\n",
    "            query_lower = query.lower()\n",
    "            safe_query_re = re.escape(query_lower)\n",
    "            \n",
    "            exact_pattern = re.compile(r'\\b' + safe_query_re + r'\\b', re.IGNORECASE)\n",
    "            starts_with_pattern = re.compile(r'\\b' + safe_query_re, re.IGNORECASE)\n",
    "\n",
    "            exact_matches = []\n",
    "            starts_with_matches = []\n",
    "            \n",
    "            for doc in user_docs:\n",
    "                if exact_pattern.search(doc.page_content):\n",
    "                    exact_matches.append(doc)\n",
    "                elif starts_with_pattern.search(doc.page_content):\n",
    "                    starts_with_matches.append(doc)\n",
    "            \n",
    "            docs_to_process = exact_matches if exact_matches else starts_with_matches\n",
    "        \n",
    "        if not docs_to_process:\n",
    "            return \"No relevant data found for the query.\"\n",
    "\n",
    "        print(f\"[INFO] Found {len(docs_to_process)} documents to process.\")\n",
    "        extracted_data_dict = extract_all_lists_from_docs_no_mapping(docs_to_process)\n",
    "        \n",
    "        if \"ERROR\" in extracted_data_dict:\n",
    "            return extracted_data_dict[\"ERROR\"]\n",
    "            \n",
    "        output_lines = []\n",
    "        for col_name, data_list in extracted_data_dict.items():\n",
    "            safe_col_name = col_name.replace(' ', '_').upper()\n",
    "            output_lines.append(f\"{safe_col_name}_LIST={data_list}\")\n",
    "        \n",
    "        return \"\\n\".join(output_lines)\n",
    "    \n",
    "    qa_tool = Tool(\n",
    "        name=\"CSVQA\",\n",
    "        func=csvqa_list_extractor_tool_func, \n",
    "        description=(\n",
    "            \"Use this tool to search and retrieve data from a pre-loaded dataset. \"\n",
    "            \"The input to this tool should be a query describing the data, e.g., 'all relevant revenue data' or a specific product keyword like '4U'.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        \"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "        \"Output **ONLY** the three lines for the Action, nothing before or after it:\\n\\n\"\n",
    "        f\"Thought: I must first use the CSVQA tool to retrieve necessary data based on the extracted keyword: '{action_input_kw}'.\\n\"\n",
    "        \"Action: CSVQA\\n\"\n",
    "        f\"Action Input: {action_input_kw}\\n\\n\"\n",
    "        \"Begin! Now process the human input:\\n\"\n",
    "        \"{input}\"\n",
    "    )\n",
    "    suffix = (\n",
    "        \"### INSTRUCTIONS – FINAL STEP\\n\"\n",
    "        \"The Observation contains the necessary structured lists (R_LIST, D_LIST, I_LIST) OR a failure signal.\\n\"\n",
    "        \"You MUST now output **exactly two lines** to finalize the process. Do **not** add any other text.\\n\"\n",
    "        \"Thought: I have successfully received the R_LIST, D_LIST, and I_LIST. I must now integrate these lists with the NRM optimization structure and output the final model.\\n\" \n",
    "        \"Final Answer:FINAL_MODEL_OUTPUT:<LaTeX optimization model followed by R_LIST/D_LIST/I_LIST data>.\"\n",
    "    )\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=[qa_tool], llm=llm2, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\"prefix\": prefix, \"suffix\": suffix, \"input_variables\": [\"input\"]},\n",
    "        verbose=True, handle_parsing_errors=True, max_iterations=5,\n",
    "    )\n",
    "\n",
    "    escaped_query = user_query.replace('{', '{{').replace('}', '}}')\n",
    "    final_answer = agent.run({\"input\": escaped_query})\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_data_dynamically(docs: List[Document]) -> Dict[str, List[Any]]:\n",
    "    \"\"\"\n",
    "    A robust two-pass parser that handles documents with different columns, followed by a\n",
    "    cleaning step to ensure data integrity.\n",
    "\n",
    "    - Pass 1: Discovers all unique column names from all documents.\n",
    "    - Pass 2: Extracts values for all discovered columns, using None for missing values.\n",
    "    - Pass 3 (Cleaning): Removes any records that are missing essential data (e.g., Value or Weight).\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return {\"ERROR\": \"No documents to process.\"}\n",
    "\n",
    "    all_column_names = set()\n",
    "    parsed_rows = []\n",
    "\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.strip()\n",
    "        if not content or '=' not in content:\n",
    "            continue\n",
    "        \n",
    "        row_data = {}\n",
    "        pairs = [p.strip() for p in content.split(',')]\n",
    "        for pair in pairs:\n",
    "            try:\n",
    "                key, value = pair.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                \n",
    "                all_column_names.add(key)\n",
    "                row_data[key] = value\n",
    "            except ValueError:\n",
    "                continue \n",
    "        \n",
    "        parsed_rows.append(row_data)\n",
    "\n",
    "    if not all_column_names:\n",
    "        return {\"ERROR\": \"Could not find any valid key-value pairs in the documents.\"}\n",
    "\n",
    "    initial_lists = {col: [] for col in all_column_names}\n",
    "\n",
    "    for row_data in parsed_rows:\n",
    "        for col_name in all_column_names:\n",
    "            value = row_data.get(col_name, None)\n",
    "            \n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Attempt to convert to float/int\n",
    "                    value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    pass # Keep as string if it's not a number\n",
    "            \n",
    "            initial_lists[col_name].append(value)\n",
    "            \n",
    "    essential_keys = ['Value', 'Weight']\n",
    "    if not all(key in initial_lists for key in essential_keys):\n",
    "        return initial_lists\n",
    "\n",
    "    indices_to_remove = {\n",
    "        i for i, (v, w) in enumerate(zip(initial_lists['Value'], initial_lists['Weight'])) \n",
    "        if v is None or w is None\n",
    "    }\n",
    "\n",
    "    if not indices_to_remove:\n",
    "        return initial_lists # No cleaning needed, return the complete lists\n",
    "\n",
    "    cleaned_lists = {}\n",
    "    for col_name, data_list in initial_lists.items():\n",
    "        cleaned_list = [\n",
    "            item for i, item in enumerate(data_list) \n",
    "            if i not in indices_to_remove\n",
    "        ]\n",
    "        cleaned_lists[col_name] = cleaned_list\n",
    "\n",
    "    return cleaned_lists\n",
    "\n",
    "def get_RA_response(user_query: str, dataset_address: str) -> str:\n",
    "    \"\"\"\n",
    "    Optimization: Max iterations increased for robustness.\n",
    "    \"\"\"\n",
    "    few_shot_block,label= build_few_shot(ra_store, user_query, k=2)\n",
    "\n",
    "    user_docs = process_dataset_address_RA(dataset_address)\n",
    "    if not user_docs:\n",
    "        return \"Final Answer: [Could not process dataset. Please check file paths.]\"\n",
    "        \n",
    "    user_store = FAISS.from_documents(user_docs, embeddings)\n",
    "    retriever = user_store.as_retriever(search_kwargs={\"k\": 400}) # OPTIMIZED: k=20\n",
    "\n",
    "    keyword = extract_retrieval_keyword(user_query)\n",
    "    print(f\"[DEBUG]: keyword is {keyword}\\n\")\n",
    "\n",
    "    action_input_kw = keyword if keyword else \"all data\"\n",
    "    def csvqa(q: str) -> str:\n",
    "        system_prompt = (\n",
    "            \"You are a silent, precise data extraction robot. Your only function is to convert the Context into a string containing multiple Python-style lists. Follow these rules with absolute precision:\\n\"\n",
    "            \"1.  **Discover Fields:** Scan the entire Context to identify all unique data fields (e.g., 'ProductName', 'Value', 'Weight', 'Capacity').\\n\"\n",
    "            \"2.  **Create Clean Lists:** For each field, create a list containing ONLY the actual values found for that field in the context.\\n\"\n",
    "            \"3.  **CRITICAL RULE: Do NOT pad lists with `None` to make them the same length. Each list should have its own natural length. This means lists for different data sources WILL have different lengths.**\\n\"\n",
    "            \"4.  **Format Output String:** Construct the final output string. Each list must be on a new line in the format: `fieldname_list = [...]`.\\n\"\n",
    "            \"\\n\"\n",
    "            \"---CRITICAL OUTPUT RULES---\\n\"\n",
    "            \"-   Your response MUST start immediately with the first list. DO NOT include any other text, explanations, or markdown.\\n\"\n",
    "            \"-   Strings in lists must use single quotes (').\\n\"\n",
    "            \"-   Numbers must be written as numbers (e.g., 8372.0).\\n\"\n",
    "            \"\\n\"\n",
    "            \"---EXAMPLE---\\n\"\n",
    "            \"Context: ProductName = Truck, Value = 8372, Weight = 36\\nCapacity = 1576\\nProductName = Sedan, Value = 1752, Weight = 15\\n\"\n",
    "            \"Correct Output:\\n\"\n",
    "            \"productname_list = ['Truck', 'Sedan']\\n\"\n",
    "            \"value_list = [8372.0, 1752.0]\\n\"\n",
    "            \"weight_list = [36.0, 15.0]\\n\"\n",
    "            \"capacity_list = [1576.0]\\n\"\n",
    "            \"---END EXAMPLE---\\n\"\n",
    "            \"\\n\"\n",
    "            \"Now, process the following context.\\n\\n\"\n",
    "            \"Context: {context}\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
    "        qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        formatted_string_from_llm = qa_chain.invoke({\"input\": 'all data'})['answer']\n",
    "        return formatted_string_from_llm\n",
    "\n",
    "\n",
    "    CSVQA_TOOL = Tool(\n",
    "        name=\"CSVQA\",\n",
    "        func=csvqa,\n",
    "        description=\"Return rows from the user's CSV files that best match the given keyword/phrase.\",\n",
    "    )\n",
    "\n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        \"You are an autonomous operations research analyst. Your sole purpose is to formulate a mathematical optimization model based on the user's problem description.\\n\"\n",
    "        \"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "        \"Your first and only action is to use the `CSVQA` tool to retrieve the structured data.\\n\"\n",
    "        \"Output **ONLY** the three lines for the Action, nothing before or after it:\\n\\n\"\n",
    "        f\"Thought: I must first use the CSVQA tool to retrieve necessary data based on the extracted keyword: '{action_input_kw}'.\\n\"\n",
    "        \"Action: CSVQA\\n\"\n",
    "        f\"Action Input: {action_input_kw}\\n\\n\"\n",
    "        \"Begin! Now process the human input:\\n\"\n",
    "        \"{input}\"\n",
    "    )\n",
    "\n",
    "    suffix = (\n",
    "        \"### FINAL INSTRUCTIONS ###\\n\"\n",
    "        \"The Observation contains the clean, structured data lists needed for the model.\\n\"\n",
    "        \"You MUST now formulate the final mathematical model in MarkDown.\\n\"\n",
    "        \"You MUST output **exactly two lines**: a 'Thought' and a 'Final Answer'.\\n\\n\"\n",
    "        \"Thought: I have received the clean data lists. My plan is as follows:\\n\"\n",
    "        \"1.  **Parameters:** I will identify all numerical parameters from the Observation.\\n\"\n",
    "        \"2.  **Assemble and Expand Markdown:** I will write out every single term of the objective function and all constraints in the Markdown model, using the identified parameters.\\n\"\n",
    "        \"Now I will execute this plan to create the final Markdown model.\\n\\n\"\n",
    "        \"Final Answer: FINAL_MODEL_OUTPUT: <The complete and fully expanded Markdown optimization model, based on the data from the Observation. Do not use placeholders like '...'.>\"\n",
    "    )\n",
    "    agent = initialize_agent(\n",
    "        tools=[CSVQA_TOOL],\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "            \"input_variables\": [\"input\"]\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=5, \n",
    "    )\n",
    "    escaped_query = user_query.replace('{', '{{').replace('}', '}}')\n",
    "    final_answer = agent.run({\"input\": escaped_query})\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_AP_response(user_query: str, dataset_address: str) -> str:\n",
    "    \"\"\"\n",
    "    Optimization: Max iterations increased for robustness.\n",
    "    \"\"\"\n",
    "    few_shot_block,label= build_few_shot(ap_store, user_query, k=1)\n",
    "\n",
    "    user_docs = process_dataset_address_vector_AP(dataset_address)\n",
    "    if not user_docs:\n",
    "        return \"Final Answer: [Could not process dataset. Please check file paths.]\"\n",
    "        \n",
    "    user_store = FAISS.from_texts(user_docs, embeddings)\n",
    "    retriever = user_store.as_retriever(search_kwargs={\"k\": 400}) # OPTIMIZED: k=20\n",
    "\n",
    "    keyword = extract_retrieval_keyword(user_query)\n",
    "    print(f\"[DEBUG]: keyword is {keyword}\\n\")\n",
    "\n",
    "    action_input_kw = keyword if keyword else \"all relevant data\"\n",
    "\n",
    "    def csvqa(q: str) -> str:\n",
    "        system_prompt = (\n",
    "        \"Retrieve the documents in order from top to bottom. Use the retrieved context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead.\"\n",
    "        \"Context: {context}\"\n",
    "    )\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
    "        qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        return qa_chain.invoke({\"input\": q})['answer']\n",
    "\n",
    "\n",
    "    CSVQA_TOOL = Tool(\n",
    "        name=\"CSVQA\",\n",
    "        func=csvqa,\n",
    "        description=\"Return rows from the user's CSV files that best match the given keyword/phrase.\",\n",
    "    )\n",
    "\n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        \"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "        \"Output **ONLY** the three lines for the Action, nothing before or after it:\\n\\n\"\n",
    "        f\"Thought: I must first use the CSVQA tool to retrieve necessary data based on the extracted keyword: '{action_input_kw}'.\\n\"\n",
    "        \"Action: CSVQA\\n\"\n",
    "        f\"Action Input: {action_input_kw}\\n\\n\"\n",
    "        \"Begin! Now process the human input:\\n\"\n",
    "        \"{input}\"\n",
    "    )\n",
    "\n",
    "    suffix = (\n",
    "    \"### FINAL INSTRUCTIONS ###\\n\"\n",
    "    \"The Observation contains the structured data in JSON format.\\n\"\n",
    "    \"Your task is to create the final, fully expanded optimization model.\\n\"\n",
    "    \"You MUST output **exactly two lines**: a 'Thought' and a 'Final Answer'.\\n\\n\"\n",
    "\n",
    "    \"Thought: I have the structured data. My plan is as follows:\\n\"\n",
    "    \"1.  **Parameters:** I will identify all numerical parameters (objective coefficients, constraint coefficients, and limits) from the Observation JSON.\\n\"\n",
    "    \"2.  **Objective & Constraints:** I will define the objective function and all constraints using these specific numbers.\\n\"\n",
    "    \"3.  **Assemble and Fully Expand:** I will now combine all parts into a single, complete LaTeX model. I will write out every single term in the summations explicitly, without using any summary notation.\\n\"\n",
    "\n",
    "    \"Final Answer: FINAL_MODEL_OUTPUT: <\\n\"\n",
    "    \"---CRITICAL INSTRUCTION---\\n\"\n",
    "    \"The model MUST be fully expanded. Do NOT use summary notation like Σ or placeholders like '...'. Write out every term using the actual numbers from the data.\\n\\n\"\n",
    "    \"**CORRECT FORMAT EXAMPLE:** `$$max \\\\sum_{{i=1}}^{{3}} p_i x_i, p=[24,66,57[$$`\\n\"\n",
    "    \">\"\n",
    "    )\n",
    "    agent = initialize_agent(\n",
    "        tools=[CSVQA_TOOL],\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "            \"input_variables\": [\"input\"]\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=5,\n",
    "    )\n",
    "\n",
    "    escaped_query = user_query.replace('{', '{{').replace('}', '}}')\n",
    "    final_answer = agent.run({\"input\": escaped_query})\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_FLP_response(user_query: str, dataset_address: str) -> str:\n",
    "    \"\"\"\n",
    "    Optimization: Max iterations increased for robustness.\n",
    "    \"\"\"\n",
    "    few_shot_block,label= build_few_shot(flp_store, user_query, k=1) # Corrected store to flp_store\n",
    "\n",
    "    user_docs = process_dataset_address_vector_FLP(dataset_address)\n",
    "    if not user_docs:\n",
    "        return \"Final Answer: [Could not process dataset. Please check file paths.]\"\n",
    "        \n",
    "    user_store = FAISS.from_texts(user_docs, embeddings)\n",
    "    retriever = user_store.as_retriever(search_kwargs={\"k\": 400}) # OPTIMIZED: k=20\n",
    "\n",
    "    keyword = extract_retrieval_keyword(user_query)\n",
    "    print(f\"[DEBUG]: keyword is {keyword}\\n\")\n",
    "\n",
    "    action_input_kw = keyword if keyword else \"all relevant data\"\n",
    "\n",
    "    def csvqa(q: str) -> str:\n",
    "        system_prompt = (\n",
    "        \"Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved.\"\n",
    "        \"Context: {context}\"\n",
    "    )\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
    "        qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        return qa_chain.invoke({\"input\": q})['answer']\n",
    "\n",
    "\n",
    "    CSVQA_TOOL = Tool(\n",
    "        name=\"CSVQA\",\n",
    "        func=csvqa,\n",
    "        description=\"Return rows from the user's CSV files that best match the given keyword/phrase.\",\n",
    "    )\n",
    "\n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        \"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "        \"Output **ONLY** the three lines for the Action, nothing before or after it:\\n\\n\"\n",
    "        f\"Thought: If the observation is null, I must first use the CSVQA tool to retrieve necessary data based on the extracted keyword: '{action_input_kw}'. If I have the observation data, I should output the final model instead of using any tool.\\n\"\n",
    "        \"Action: CSVQA\\n\"\n",
    "        f\"Action Input: {dataset_address}\\n\\n\"\n",
    "        \"Begin! Now process the human input:\\n\"\n",
    "        \"{input}\"\n",
    "    )\n",
    "    suffix = (\n",
    "        \"### INSTRUCTIONS – FINAL STEP\\n\"\n",
    "        \"The Observation contains the necessary structured lists OR a failure signal.\\n\"\n",
    "        \"You MUST now output **exactly two lines** to finalize the process. Do **not** add any other text.\\n\"\n",
    "        \"Thought: I have successfully received the necessary data. I must now integrate these lists with the transportation optimization structure and output the final model.\\n\" \n",
    "        \"Final Answer:FINAL_MODEL_OUTPUT:<LaTeX optimization model followed by the data from the Observation.\"\n",
    "    )\n",
    "\n",
    "    # AgentType.ZERO_SHOT_REACT_DESCRIPTION automatically handles Thought/Action/Observation\n",
    "    agent = initialize_agent(\n",
    "        tools=[CSVQA_TOOL],\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "            \"input_variables\": [\"input\"]\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=5,\n",
    "    )\n",
    "\n",
    "    escaped_query = user_query.replace('{', '{{').replace('}', '}}')\n",
    "    final_answer = agent.run({\"input\": escaped_query})\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "\n",
    "def get_TP_response(user_query: str, dataset_address: str) -> str:\n",
    "    \"\"\"\n",
    "    Optimization: Max iterations increased for robustness.\n",
    "    \"\"\"\n",
    "    few_shot_block,label= build_few_shot_TP(tp_store, user_query, k=1)\n",
    "    keyword = extract_retrieval_keyword(user_query)\n",
    "\n",
    "    action_input_kw = keyword if keyword else \"all relevant data\"\n",
    "\n",
    "    def extract_data_with_roles(dataset_address) -> Dict:\n",
    "        \"\"\"\n",
    "        Extracts d, s, and c from a list of DataFrames based on the identified role information.\n",
    "        \"\"\"\n",
    "        dfs = load_dataframes_from_files(dataset_address)\n",
    "        roles = identify_data_roles(dfs,llm1)\n",
    "        \n",
    "        if not roles:\n",
    "            print(\"❌ Cannot extract data due to missing role information.\")\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            # --- Extract demand vector d ---\n",
    "            demand_info = roles['demand']\n",
    "            demand_df = dfs[demand_info['df_index']]\n",
    "            d = demand_df[demand_info['column_name']].tolist()\n",
    "\n",
    "            # --- Extract supply vector s ---\n",
    "            supply_info = roles['supply']\n",
    "            supply_df = dfs[supply_info['df_index']]\n",
    "            s = supply_df[supply_info['column_name']].tolist()\n",
    "\n",
    "            # --- Extract cost matrix c ---\n",
    "            costs_info = roles['costs']\n",
    "            costs_df = dfs[costs_info['df_index']]\n",
    "            # The cost matrix consists of all columns except for the ID column.\n",
    "            supplier_id_col = costs_info['supplier_id_column']\n",
    "            matrix_df = costs_df.drop(columns=[supplier_id_col])\n",
    "            c = matrix_df.values.tolist()\n",
    "            obs = ''\n",
    "            obs += f'd = {d} \\n'\n",
    "            obs += f's = {s} \\n'\n",
    "            obs += f'c = {c} \\n'\n",
    "\n",
    "            return obs\n",
    "\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"❌ Error while extracting data based on roles: {e}. Please check if the role information returned by the LLM is accurate.\")\n",
    "            return {}\n",
    "        \n",
    "\n",
    "    CSVQA_TOOL = Tool(\n",
    "        name=\"csvqa\",\n",
    "        func=extract_data_with_roles,\n",
    "        description=\"Use this tool as the first and only step to read data from CSV file paths. It returns 'd', 's', and 'c' lists.\"\n",
    "    )\n",
    "\n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        \"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "        \"Output **ONLY** the three lines for the Action, nothing before or after it:\\n\\n\"\n",
    "        f\"Thought: If the observation is null, I must first use the csvqa tool to retrieve necessary data based on the dataset address: '{dataset_address}'. If I have the observation data, I should output the final model instead of using any tool.\\n\"\n",
    "        \"Action: csvqa\\n\"\n",
    "        f\"Action Input: {dataset_address}\\n\\n\"\n",
    "        \"Begin! Now process the human input:\\n\"\n",
    "        \"{input}\"\n",
    "    )\n",
    "    suffix = (\n",
    "        \"### INSTRUCTIONS – FINAL STEP\\n\"\n",
    "        \"The Observation contains the necessary structured lists OR a failure signal.\\n\"\n",
    "        \"You MUST now output **exactly two lines** to finalize the process. Do **not** add any other text.\\n\"\n",
    "        \"Thought: I have successfully received the necessary data. I must now integrate these lists with the transportation optimization structure and output the final model.\\n\" \n",
    "        \"Final Answer:FINAL_MODEL_OUTPUT:<LaTeX optimization model followed by the data from the Observation.\"\n",
    "    )\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=[CSVQA_TOOL],\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "            \"input_variables\": [\"input\", \"agent_scratchpad\"] \n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=5,\n",
    "    )\n",
    "    escaped_query = user_query.replace('{', '{{').replace('}', '}}')\n",
    "    final_answer = agent.run({\"input\": escaped_query})\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others with csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_few_shot_Other(store, user_query: str, k: int = 1, t='Code'):\n",
    "    examples = []\n",
    "    \n",
    "    for doc in retrieve_examples(store, user_query, k=k):\n",
    "        txt = doc.page_content.lstrip('\\ufeff') \n",
    "        \n",
    "        try:\n",
    "            f = io.StringIO(txt)\n",
    "            reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "            \n",
    "            parts = next(reader)\n",
    "            if len(parts) < 4:\n",
    "                print(f\"[build_few_shot_Other Warning] Skipping malformed CSV row: {txt[:50]}\")\n",
    "                continue\n",
    "            \n",
    "            prompt_part = parts[0].strip()\n",
    "            data_addr = parts[1].strip()\n",
    "            label_model_part = parts[2].strip()\n",
    "            label_code_part = parts[3].strip() \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[build_few_shot_Other Error] Failed to parse CSV row: {e}. Content: {txt[:50]}\")\n",
    "            continue\n",
    "\n",
    "        data_blocks = []\n",
    "        for fp in map(str.strip, data_addr.splitlines()):\n",
    "            if not fp: continue\n",
    "            try:\n",
    "                header = f\"[{os.path.basename(fp)} | (Example Schema)]\"\n",
    "                rows = \"col1, col2, col3\\n1, 2, 3\\n4, 5, 6\" \n",
    "                data_blocks.append(header + \"\\n\" + rows)\n",
    "            except Exception as e:\n",
    "                data_blocks.append(f\"[Could not read example data {fp}: {e}]\")\n",
    "        data_section = \"\\n\".join(data_blocks) if data_blocks else \"[No data found]\"\n",
    "        \n",
    "        if t == 'Model':\n",
    "            ex = (\n",
    "                \"<EXAMPLE>\\n\"\n",
    "                f\"Question: {prompt_part}\\n\"\n",
    "                f\"Observation (Schema): \\n{data_section}\\n\" \n",
    "                \"Final Answer:\\n\"\n",
    "                f\"{label_model_part}\\n\" \n",
    "                \"</EXAMPLE>\"\n",
    "            )\n",
    "            examples.append(escape_braces(ex))\n",
    "        \n",
    "        else: \n",
    "            ex = (\n",
    "                \"<EXAMPLE>\\n\"\n",
    "                f\"Question: {prompt_part}\\n\"\n",
    "                f\"CSV Schema: \\n{data_section}\\n\" \n",
    "                f\"Abstract Model Plan: \\n{label_model_part}\\n\" # \n",
    "                \"Final Answer (Code):\\n\"\n",
    "                f\"{label_code_part}\\n\" \n",
    "                \"</EXAMPLE>\"\n",
    "            )\n",
    "            examples.append(escape_braces(ex))\n",
    "\n",
    "    return \"\\n\\n\".join(examples)\n",
    "\n",
    "def initialize_abstract_modeler_chain(few_shot_examples: str):\n",
    " \n",
    "    abstract_model_template = \"\"\"\n",
    "You are an expert optimization modeler.\n",
    "Your task is to create an \"Abstract Model Plan\" based on the user's query and the CSV Schema.\n",
    "This plan is *not* Gurobi code or mathematical formulas, but a clear, step-by-step reasoning process in English.\n",
    "\n",
    "[Examples]\n",
    "{few_shot_examples}\n",
    "\n",
    "[Current Task]\n",
    "User Query: {query}\n",
    "\n",
    "CSV Schema:\n",
    "{schema}\n",
    "\n",
    "[Your Output]\n",
    "You must strictly follow this format for your \"Abstract Model Plan\" output:\n",
    "\n",
    "[Abstract Model Plan START]\n",
    "1.  **Analyze Query:** The user wants to {{{{Your analysis}}}}.\n",
    "2.  **Identify Model Type:** Based on the query, this is a {{{{e.g., LP, MIP, Fixed-Charge, Blending}}}} problem.\n",
    "3.  **Define Index Sets:** The primary indices are {{{{e.g., Products, Workers, Sources, Destinations}}}}.\n",
    "4.  **Define Decision Variables:**\n",
    "    -   `x[i]` = {{{{Describe first variable, e.g., 'quantity of product i'}}}}. Type: {{{{GRB.CONTINUOUS / GRB.INTEGER}}}}.\n",
    "    -   `y[i]` = {{{{Describe second variable, e.g., 'if product i is produced'}}}}. Type: {{{{GRB.BINARY}}}}.\n",
    "5.  **Identify Parameters (from Schema):**\n",
    "    -   Objective coefficients (e.g., profit) will come from column(s): {{{{e.g., 'Price', 'Production Cost'}}}}.\n",
    "    -   Constraint coefficients (e.g., resource use) will come from: {{{{e.g., 'Resource 1', 'Available Time'}}}}.\n",
    "    -   Constraint RHS (limits) will come from: {{{{e.g., 'Max Demand'}}}}.\n",
    "6.  **Formulate Objective:** {{{{Describe objective, e.g., 'Maximize sum((schema['Price'] - schema['Cost']) * x[i] - schema['FixedCost'] * y[i])'}}}}.\n",
    "7.  **Formulate Constraints:**\n",
    "    -   Constraint 1 (e.g., Resource Limit): {{{{Describe constraint 1, e.g., 'sum(schema['Resource 1'][i] * x[i]) <= schema['Available']'}}}}.\n",
    "    -   Constraint 2 (e.g., Linking): {{{{Describe constraint 2, e.g., 'x[i] <= M * y[i])'}}}}.\n",
    "    -   ... (Other constraints) ...\n",
    "[Abstract Model Plan END]\n",
    "\"\"\"\n",
    "    \n",
    "    abstract_prompt = PromptTemplate(\n",
    "        template=abstract_model_template,\n",
    "        input_variables=[\"query\", \"schema\", \"few_shot_examples\"]\n",
    "    )\n",
    "    \n",
    "    return LLMChain(llm=llm2, prompt=abstract_prompt)\n",
    "\n",
    "def initialize_code_gen_chain(few_shot_examples: str):\n",
    "    code_gen_template = \"\"\"\n",
    "You are an expert Gurobi programmer.\n",
    "Your task is to strictly follow the User Query, CSV Schema, and \"Abstract Model Plan\" to translate them into a single, complete, executable Gurobi Python code block.\n",
    "The code must start with ```python and end with ```.\n",
    "The code must include all necessary imports (`gurobipy`, `pandas`, `numpy`, `sys`, `re`).\n",
    "The code must include a `try...except` block to handle errors.\n",
    "The code must robustly read '{dataset_address}' (if it's multiple paths, read the first).\n",
    "The code must *fully* implement all variables, objectives, and constraints from the \"Abstract Model Plan\".\n",
    "\n",
    "[Examples of Query -> Plan -> Code]\n",
    "{few_shot_examples}\n",
    "\n",
    "[Current Task]\n",
    "User Query: {query}\n",
    "\n",
    "[CSV Schema]\n",
    "{schema}\n",
    "\n",
    "[Abstract Model Plan]\n",
    "{abstract_plan}\n",
    "\n",
    "[Your Gurobi Code]\n",
    "(Your code *must* start with ```python)\n",
    "```python\n",
    "import gurobipy as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def find_col(df, pattern):\n",
    "    # A helper function to robustly find column names\n",
    "    for col in df.columns:\n",
    "        if pattern.lower() in col.lower():\n",
    "            return col\n",
    "    print(f\"[Warning] Could not find column containing '{{{{pattern}}}}'. Trying exact match.\", file=sys.stderr)\n",
    "    if pattern in df.columns:\n",
    "        return pattern\n",
    "    raise KeyError(f\"Could not find a column containing '{{{{pattern}}}}' in {{{{df.columns.tolist()}}}}\")\n",
    "\n",
    "try:\n",
    "    # 1. Load data\n",
    "    data_path = '{dataset_address}'\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(data_path, encoding='gbk')\n",
    "    except Exception as e:\n",
    "        # Try to split path if multiple files\n",
    "        try:\n",
    "            first_path = re.split(r'[\\s\\n]+', data_path)[0]\n",
    "            df = pd.read_csv(first_path)\n",
    "            print(f\"Warning: Could not read full path list. Loaded first file: {{{{first_path}}}}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error loading data: {{{{e2}}}}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "            \n",
    "    print(\"Successfully loaded data. Head:\")\n",
    "    print(df.head())\n",
    "\n",
    "    #\n",
    "    # --- LLM MUST GENERATE THE REST OF THE CODE HERE ---\n",
    "    # (Based on the Abstract Model Plan)\n",
    "    #\n",
    "    \n",
    "    print(\"\\n[INFO] Building Gurobi model based on the abstract plan...\")\n",
    "    \n",
    "    # [LLM: Fill in the Gurobi model logic here, following your plan]\n",
    "    \n",
    "    # --- Placeholder - LLM must replace this ---\n",
    "    m = gp.Model(\"PlaceholderModel\")\n",
    "    # Example:\n",
    "    # products = df['Product'].tolist()\n",
    "    # profit = dict(zip(df['Product'], df['Price'] - df['Cost']))\n",
    "    # x = m.addVars(products, vtype=GRB.INTEGER, name=\"x\")\n",
    "    # m.setObjective(gp.quicksum(profit[i] * x[i] for i in products), gp.GRB.MAXIMIZE)\n",
    "    \n",
    "    m.optimize()\n",
    "    if m.status == gp.GRB.OPTIMAL:\n",
    "        print(\"Placeholder model solved.\")\n",
    "    else:\n",
    "        print(\"Placeholder model did not solve.\")\n",
    "    # --- End Placeholder ---\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {{{{e}}}}\", file=sys.stderr)\n",
    "\"\"\"\n",
    "    code_gen_prompt = PromptTemplate(\n",
    "        template=code_gen_template,\n",
    "        input_variables=[\"query\", \"schema\", \"abstract_plan\", \"dataset_address\", \"few_shot_examples\"]\n",
    "    )\n",
    "\n",
    "    return LLMChain(llm=llm2, prompt=code_gen_prompt)\n",
    "\n",
    "def get_Others_response(user_query: str, dataset_address: str) -> str:\n",
    "    def get_csv_schema(file_paths_string: str) -> str:\n",
    "        \"\"\"\n",
    "        [步骤 1: Python 函数]\n",
    "        接收一个文件路径字符串，解析路径，并返回所有文件的 schema。\n",
    "        \"\"\"\n",
    "        print(f\"\\n[Pipeline Step 1/3]: Getting CSV Schema (Python Call)...\")\n",
    "        paths = re.split(r'[\\s\\n]+', file_paths_string)\n",
    "        all_schema_info = []\n",
    "        for path in paths:\n",
    "            if not path.strip(): continue\n",
    "            try:\n",
    "                try:\n",
    "                    df = pd.read_csv(path, nrows=10, encoding='utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    df = pd.read_csv(path, nrows=10, encoding='gbk')\n",
    "                \n",
    "                schema_info = (\n",
    "                    f\"Successfully read first 10 rows from {path}.\\n\"\n",
    "                    f\"Columns: {df.columns.to_list()}\\n\\n\"\n",
    "                    f\"Data Head:\\n{df.to_string()}\"\n",
    "                )\n",
    "                all_schema_info.append(schema_info)\n",
    "            except Exception as e:\n",
    "                error_info = (\n",
    "                    f\"Error reading CSV at {path}: {e}. \"\n",
    "                    \"You must formulate the model/code based on the file path and assumed column names.\"\n",
    "                )\n",
    "                all_schema_info.append(error_info)\n",
    "        if not all_schema_info:\n",
    "            return \"Error: No valid file paths were provided.\"\n",
    "        \n",
    "        result_schema = \"\\n\\n---\\n\\n\".join(all_schema_info)\n",
    "        print(f\"[Observation]:\\n{result_schema}\")\n",
    "        return result_schema\n",
    "\n",
    "    print(\"[INFO]: Starting 3-Step Hybrid Pipeline...\")\n",
    "\n",
    "    try:\n",
    "        schema = get_csv_schema(dataset_address)\n",
    "        if \"Error:\" in schema:\n",
    "            return f\"Pipeline failed at Step 1 (Get Schema): {schema}\"\n",
    "\n",
    "        print(\"\\n[Pipeline Step 2/3]: Formulating Abstract Model (LLMChain Call)...\")\n",
    "\n",
    "        few_shot_examples_for_model = build_few_shot_Other(Others_store, user_query, k=3, t='Model')\n",
    "        \n",
    "        abstract_modeler_chain = initialize_abstract_modeler_chain(few_shot_examples_for_model)\n",
    "        \n",
    "        abstract_model_plan = abstract_modeler_chain.run(\n",
    "            query=user_query,\n",
    "            schema=schema,\n",
    "            few_shot_examples=few_shot_examples_for_model\n",
    "        )\n",
    "        print(f\"[Observation]:\\n{abstract_model_plan}\")\n",
    "\n",
    "        if \"Error:\" in abstract_model_plan or not \"[Abstract Model Plan START]\" in abstract_model_plan:\n",
    "            return f\"Pipeline failed at Step 2 (Abstract Modeler Chain): {abstract_model_plan}\"\n",
    "\n",
    "        print(\"\\n[Pipeline Step 3/3]: Generating Gurobi Code (LLMChain Call)...\")\n",
    "        \n",
    "        few_shot_examples_for_code = build_few_shot_Other(Others_store, user_query, k=3, t='Code')\n",
    "        \n",
    "        code_gen_chain = initialize_code_gen_chain(few_shot_examples_for_code)\n",
    "\n",
    "        final_code = code_gen_chain.run(\n",
    "            query=user_query,\n",
    "            schema=schema,\n",
    "            abstract_plan=abstract_model_plan,\n",
    "            dataset_address=dataset_address,\n",
    "            few_shot_examples=few_shot_examples_for_code\n",
    "        )\n",
    "        \n",
    "        if \"```python\" in final_code:\n",
    "            code_block = final_code.split(\"```python\", 1)[1]\n",
    "            if \"```\" in code_block:\n",
    "                code_block = code_block.split(\"```\", 1)[0]\n",
    "            final_answer = \"```python\\n\" + code_block.strip() + \"\\n```\"\n",
    "        else:\n",
    "            if not final_code.strip().startswith(\"import\"):\n",
    "                print(f\"[Warning] Step 3 output was not a valid code block. Output: {final_code[:200]}...\")\n",
    "                final_answer = f\"Error: Code generation failed. LLM returned non-code output:\\n{final_code}\"\n",
    "            else:\n",
    "                print(\"[Warning] Step 3 output missed ```python tag, adding it.\")\n",
    "                final_answer = \"```python\\n\" + final_code.strip() + \"\\n```\"\n",
    "            \n",
    "        print(f\"[Final Answer]:\\n{final_answer}\")\n",
    "        return final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Gurobi Pipeline Error]: {e}\")\n",
    "        return f\"Error: The Gurobi Code Generation pipeline failed with error: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others without CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_others_without_CSV_response(query):\n",
    "    \"\"\"\n",
    "    Optimization: Max iterations increased for robustness.\n",
    "    \"\"\"\n",
    "    few_shot_block,label= build_few_shot(OW_store, query, k=5)\n",
    "\n",
    "\n",
    "    retriever = OW_store.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm2,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    qa_tool = Tool(\n",
    "        name=\"ORLM_QA\",\n",
    "        func=qa_chain.invoke,\n",
    "        description=(\n",
    "            \"Use this tool to answer Querys.\"\n",
    "            \"Provide the Query as input, and the tool will retrieve the relevant information from the file and use it to answer the Query.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    prefix = (\n",
    "        f\"{few_shot_block}\\n\\n\"\n",
    "        # \"You are now solving the following USER QUESTION:\\n\"\n",
    "        # \"{input}\\n\\n\"\n",
    "        # \"### FIRST RESPONSE FORMAT (exactly 3 lines) ###\\n\"\n",
    "        # \"Thought: <your short reasoning to decide whether to call the tool>\\n\"\n",
    "        # \"Action: ORLM_QA\\n\"\n",
    "        # \"Action Input: <{input}>\\n\\n\"\n",
    "        # \"If you do NOT need the tool, leave the Action Input empty.\\n\"\n",
    "        # \"Begin.\"\n",
    "        \"\"\" \n",
    "    Use the following triggers to identify problem structures and apply the corresponding mathematical formulations.\n",
    "    Thought: Read the question and 1) identify the goal (minimize time/cost/crew or maximize throughput/value) and collect per-unit coefficients; 2) define decision variables and pick domains: counts of trips/units/vehicles are nonnegative integers, yes or no choices are binary, divisible flows or weights are nonnegative reals; 3) write the linear objective from the coefficients; 4) add constraints in this order: demand or target (exactly, at least, at most), capacity or supply upper bounds, flow conservation or stage linking across nodes or arcs, and any share or ratio limits rewritten as linear inequalities using the given per-unit rates, plus any minimum or maximum usage; 5) add nonnegativity and the chosen integrality or binary domains; 6) output only the LP: objective first, then constraints line by line with brief labels if needed, then a final line stating the variable domains.\n",
    "    (1) INTEGER trigger:\n",
    "    Question: A factory can run two machine types. How many of each machine should be installed given budget and space limits? Maximize output.\n",
    "    Final Answer: \n",
    "    $\\\\max\\\\; p_1 x_1 + p_2 x_2$\n",
    "    $\\\\text{{s.t. }} a_1 x_1 + a_2 x_2 \\\\le B,\\\\; s_1 x_1 + s_2 x_2 \\\\le S$\n",
    "    $x_1, x_2 \\\\in \\\\mathbb{{Z}}_+.$\n",
    "    (2) MULTI-PERIOD FLOW trigger:\n",
    "    Question: Multi-period production with inventory and backorders. Costs for production/holding/backorder. Initial and terminal conditions given.\n",
    "    Final Answer:\n",
    "    \\textbf{{Indices: }} t\\in T=\\{{1,\\dots,n\\}}. \\\\\n",
    "    \\textbf{{Given: }} d_t,\\ I_0,\\ B_0,\\ \\dots \\\\\n",
    "    \\textbf{{Vars: }} x_t\\ge0,\\ I_t\\ge0,\\ B_t\\ge0. \\\\\n",
    "    \\min \\sum_t (c x_t + h I_t + p B_t) \\\\\n",
    "    \\text{{s.t. }} I_t - B_t = I_{{t-1}} - B_{{t-1}} + x_t - d_t,\\ \\forall t \\\\\n",
    "    I_n \\ge I^{{\\min}},\\ B_n=0.\n",
    "    (3) LOGIC+BINARY trigger:\n",
    "    Question: Choose exactly one option from set P and at least K items from set V, with quantities and budget.\n",
    "    Final Answer:\n",
    "    \\textbf{{Sets: }} i\\in P,\\ j\\in V. \\ \\textbf{{Vars: }} q_i,q_j\\ge0;\\ y_i\\in\\{{0,1\\}}, z_j\\in\\{{0,1\\}}.\\\\\n",
    "    \\max \\sum_j f_j q_j \\\\\n",
    "    \\text{{s.t. }} \\sum_i y_i = 1,\\ \\sum_j z_j \\ge K \\\\\n",
    "    0\\le q_i \\le M_i y_i,\\ \\forall i;\\ \\ 0\\le q_j \\le M_j z_j,\\ \\forall j \\\\\n",
    "    \\text{{Budget: }} \\sum_i c_i q_i + \\sum_j c_j q_j \\le B.\n",
    "    \"\"\"\n",
    "    \"USER QUESTION:\\n{input}\\n\\n\"\n",
    "    \"TASK:\\n\"\n",
    "    \"- Produce a complete LaTeX optimization model using ONLY information in the question.\\n\"\n",
    "    \"- Use a fixed structure INSIDE LaTeX: Indices/Sets; Given Parameters (convert all tables to arrays); Decision Variables (with domains); Objective; Constraints; Domain lines.\\n\"\n",
    "    \"- For multi-period problems, you MUST include state-balance recurrences and initial/terminal conditions explicitly.\\n\"\n",
    "    \"- Avoid nonlinear forms when possible: rewrite ratios/logic using linear constraints + binaries (big-M) with clearly defined M.\\n\\n\"\n",
    "    \"You should decide VARIABLE–TYPE first!\\n (If not mentioned or ambiguous, integer by default!)\"\n",
    "    \"### FIRST RESPONSE FORMAT (exactly 3 lines) ###\\n\"\n",
    "    \"Thought: <brief>\\n\"\n",
    "    \"Action: ORLM_QA\\n\"\n",
    "    \"Action Input: {input}\\n\\n\"\n",
    "    \"Begin.\"\n",
    "    )\n",
    "\n",
    "    suffix = (\n",
    "        \"\\n### AFTER OBSERVATION ###\\n\"\n",
    "        \"Respond with exactly two lines:\\n\"\n",
    "        \"Thought: <variable types: integer/binary/continuous>\\n\"\n",
    "        \"Final Answer: <ONLY LaTeX model. Must include: (i) indices/sets, (ii) parameter definitions (tables->arrays), (iii) explicit domain lines for EVERY variable, (iv) initial/terminal conditions if any. No prose.>\\n\"\n",
    "        \"Do NOT output anything else.\"\n",
    "        \"You should decide VARIABLE–TYPE first!\\n (If not mentioned or ambiguous, integer by default!)\"\n",
    "    )\n",
    "\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=[qa_tool],\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "            \"input_variables\": [\"input\"]\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,  # Enable error handling\n",
    "        max_iterations=5,\n",
    "    )\n",
    "\n",
    "    query = query.replace('{','{{').replace('}','}}')\n",
    "    output = agent.run({\"input\": query})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_test(test, classify_problem):\n",
    "    output_model = []\n",
    "    output_code = []\n",
    "    classification = []\n",
    "    for index, row in test.iterrows():\n",
    "        try:\n",
    "            query = row['Query']\n",
    "            print(query)\n",
    "            \n",
    "            def extract_problem_type(output_text):\n",
    "                pattern = r'(Network Revenue Management|Network Revenue Management Problem|Resource Allocation|Resource Allocation Problem|Transportation|Transportation Problem|Facility Location Problem|Assignment Problem|AP|Uncapacited Facility Location Problem|NRM|RA|TP|FLP|UFLP|Others without CSV|Sales-Based Linear Programming|SBLP|Others with CSV)'\n",
    "                match = re.search(pattern, output_text, re.IGNORECASE)\n",
    "                return match.group(0) if match else None\n",
    "            \n",
    "            def csv_detect(row):\n",
    "                return 1 if 'Dataset_address' in row.index else 0\n",
    "    \n",
    "            if csv_detect(row):\n",
    "                response = classify_problem(query)\n",
    "                selected_problem = extract_problem_type(response)\n",
    "                classification.append(selected_problem)\n",
    "                dataset_address = row['Dataset_address']\n",
    "                if selected_problem == \"Network Revenue Management\" or selected_problem == \"NRM\" or selected_problem == \"Network Revenue Management Problem\":\n",
    "                    print(\"----------Network Revenue Management-----------\")\n",
    "                    output = get_NRM_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "                    code_response = get_code(output,selected_problem)\n",
    "                    output_code.append(code_response)\n",
    "    \n",
    "                elif selected_problem == \"Resource Allocation\" or selected_problem == \"RA\" or selected_problem == \"Resource Allocation Problem\":\n",
    "                    print(\"----------Resource Allocation-----------\")\n",
    "                    output = get_RA_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "                    code_response = get_code(output,selected_problem)\n",
    "                    output_code.append(code_response)\n",
    "    \n",
    "                elif selected_problem == \"Transportation\" or selected_problem == \"TP\" or selected_problem == \"Transportation Problem\":\n",
    "                    print(\"----------Transportation-----------\")\n",
    "                    output = get_TP_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "                    code_response = get_code(output,selected_problem)\n",
    "                    output_code.append(code_response)    \n",
    "    \n",
    "                elif selected_problem == \"Facility Location Problem\" or selected_problem == \"FLP\" or selected_problem == \"Uncapacited Facility Location\" or selected_problem == \"UFLP\":\n",
    "                    print(\"----------Facility Location Problem-----------\")\n",
    "                    output = get_FLP_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "                    code_response = get_code(output,selected_problem)\n",
    "                    output_code.append(code_response)\n",
    "                \n",
    "                elif selected_problem == \"Assignment Problem\" or selected_problem == \"AP\":\n",
    "                    print(\"----------Assignment Problem-----------\")\n",
    "                    output = get_AP_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "                    code_response = get_code(output,selected_problem)\n",
    "                    output_code.append(code_response)\n",
    "                else:\n",
    "                    print(\"----------Others with CSV-----------\")\n",
    "                    output = get_Others_response(query,dataset_address)\n",
    "                    output_model.append(output)\n",
    "\n",
    "                    code_response = \"\"\n",
    "    \n",
    "                    if output.strip().startswith(\"```python\"):\n",
    "                        print(\"[INFO] Output is already Gurobi code. Skipping get_code.\")\n",
    "                        code_response = output.strip().replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"[INFO] Output is a Math Model. Calling get_code to convert.\")\n",
    "                        code_response = get_code(output, selected_problem) \n",
    "\n",
    "                    output_code.append(code_response)\n",
    "    \n",
    "            else:\n",
    "                print(\"----------Others without CSV-----------\")\n",
    "                output = get_others_without_CSV_response(query)\n",
    "                print(output)\n",
    "                output_model.append(output)\n",
    "                selected_problem=\"Others without CSV\"\n",
    "                code_response = get_code(output,selected_problem)\n",
    "                output_code.append(code_response)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "            continue\n",
    "        time.sleep(15)\n",
    "    return output_model, output_code\n",
    "\n",
    "def read_and_combine_csvs(file_order):\n",
    "    dfs = []\n",
    "    for fname in file_order:\n",
    "        if os.path.exists(fname):\n",
    "            df = pd.read_csv(fname)\n",
    "            dfs.append(df)\n",
    "            print(f\"Read file: {fname} (Row length: {len(df)})\")\n",
    "        else:\n",
    "            print(f\"File doesn't exist: {fname}, already skipped\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No effective files\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def run_gurobi_code(code_str):\n",
    " \n",
    "    try:\n",
    "      \n",
    "        with StringIO() as buf, contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "            env = {\n",
    "                '__builtins__': __builtins__,\n",
    "                'gp': gp,\n",
    "                'GRB': GRB\n",
    "            }\n",
    "            \n",
    "           \n",
    "            code_str += \"\\n\\n# Added by executor\\n\"\n",
    "            code_str += \"if hasattr(m, 'status') and m.status == GRB.OPTIMAL:\\n\"\n",
    "            code_str += \"    __result__ = m.ObjVal\\n\"\n",
    "            code_str += \"else:\\n\"\n",
    "            code_str += \"    __result__ = None\\n\"\n",
    "            \n",
    "            \n",
    "            exec(code_str, env)\n",
    "            result = env.get('__result__', None)\n",
    "            \n",
    "     \n",
    "            if 'm' in env:\n",
    "                env['m'].dispose()\n",
    "                del env['m']\n",
    "            \n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(f\"Execution error: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Large-Scale-OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Test_Dataset/Large-scale-or/Large-scale-or-101.csv')\n",
    "output_model, output_code,classification = run_test(test,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test['Query'], 'model_output':output_model, 'code_output':output_code,'classification':classification})\n",
    "output_df.to_csv(\"Large-scale-or-Lean-Oss-20b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Small-Scale Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NL4OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nl4opt = pd.read_excel('Test_Dataset/Small-scale/NL4OPT NEW.xlsx')\n",
    "test_nl4opt1=test_nl4opt[:50]\n",
    "test_nl4opt2=test_nl4opt[50:100]\n",
    "test_nl4opt3=test_nl4opt[100:150]\n",
    "test_nl4opt4=test_nl4opt[150:200]\n",
    "test_nl4opt5=test_nl4opt[200:]\n",
    "test_nl4opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_nl4opt1, output_code_nl4opt1 = run_test(test_nl4opt1,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_nl4opt1['Query'], 'model_output':output_model_nl4opt1, 'code_output':output_code_nl4opt1})\n",
    "output_df.to_csv(\"nl4opt_1-50_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_nl4opt2, output_code_nl4opt2 = run_test(test_nl4opt2,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_nl4opt2['Query'], 'model_output':output_model_nl4opt2, 'code_output':output_code_nl4opt2})\n",
    "output_df.to_csv(\"nl4opt_51-100_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_nl4opt3, output_code_nl4opt3 = run_test(test_nl4opt3,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_nl4opt3['Query'], 'model_output':output_model_nl4opt3, 'code_output':output_code_nl4opt3})\n",
    "output_df.to_csv(\"nl4opt_101-150_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_nl4opt4, output_code_nl4opt4 = run_test(test_nl4opt4,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_nl4opt4['Query'], 'model_output':output_model_nl4opt4, 'code_output':output_code_nl4opt4})\n",
    "output_df.to_csv(\"nl4opt_151-200_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_nl4opt5, output_code_nl4opt5= run_test(test_nl4opt5,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_nl4opt5['Query'], 'model_output':output_model_nl4opt5, 'code_output':output_code_nl4opt5})\n",
    "output_df.to_csv(\"nl4opt_201-_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=[\n",
    "    \"nl4opt_1-50_Lean(Oss-20b).csv\",\n",
    "    \"nl4opt_51-100_Lean(Oss-20b).csv\",\n",
    "    \"nl4opt_101-150_Lean(Oss-20b).csv\",\n",
    "    \"nl4opt_151-200_Lean(Oss-20b).csv\",\n",
    "    \"nl4opt_201-_Lean(Oss-20b).csv\",\n",
    "]\n",
    "try:\n",
    "    combined_df = read_and_combine_csvs(file_order)\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"File processing failed: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(\"\\nRunning Gurobi Code...\")\n",
    "objective_values = []\n",
    "start_time = time.time()\n",
    "success_count = 0\n",
    "\n",
    "for i, row in combined_df.iterrows():\n",
    "    full_text = row['model_output']\n",
    "    \n",
    "    code = extract_python_code(full_text)\n",
    "    \n",
    "    if code:\n",
    "        print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
    "        result = run_gurobi_code(code)\n",
    "        objective_values.append(result)\n",
    "        if result is not None:\n",
    "            success_count += 1\n",
    "    else:\n",
    "        print(f\"Warning: No Python code found in row {i+1}\")\n",
    "        objective_values.append(None)\n",
    "\n",
    "combined_df['best_objective'] = objective_values\n",
    "\n",
    "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
    "print(f\"Success: {success_count}/{len(combined_df)}\")\n",
    "\n",
    "print(\"\\nFirst few objective values:\")\n",
    "for i in range(min(5, len(combined_df))):\n",
    "    print(f\"Row {i+1}: Best objective = {objective_values[i]}\")\n",
    "\n",
    "output_file = \"nl4opt_Lean(Oss-20b)_result.xlsx\"\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test IndustryOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_industryOR = pd.read_csv('Test_Dataset/Small-scale/IndustryOR_fixedV2.csv')\n",
    "test_industryOR1=test_industryOR[:25]\n",
    "test_industryOR2=test_industryOR[25:50]\n",
    "test_industryOR3=test_industryOR[50:75]\n",
    "test_industryOR4=test_industryOR[75:]\n",
    "test_industryOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_industryOR1, output_code_industryOR1,= run_test(test_industryOR1,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_industryOR1['Query'], 'model_output':output_model_industryOR1, 'code_output':output_code_industryOR1})\n",
    "output_df.to_csv(\"IndustryOR_1-25_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_industryOR2, output_code_industryOR2, = run_test(test_industryOR2,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_industryOR2['Query'], 'model_output':output_model_industryOR2, 'code_output':output_code_industryOR2})\n",
    "output_df.to_csv(\"IndustryOR_26-50_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_industryOR3, output_code_industryOR3, = run_test(test_industryOR3,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_industryOR3['Query'], 'model_output':output_model_industryOR3, 'code_output':output_code_industryOR3})\n",
    "output_df.to_csv(\"IndustryOR_51-75_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_industryOR4, output_code_industryOR4, = run_test(test_industryOR4,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_industryOR4['Query'], 'model_output':output_model_industryOR4, 'code_output':output_code_industryOR4})\n",
    "output_df.to_csv(\"IndustryOR_76-100_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=[\n",
    "    \"IndustryOR_1-25_Lean(Oss-20b).csv\",\n",
    "    \"IndustryOR_26-50_Lean(Oss-20b).csv\",\n",
    "    \"IndustryOR_51-75_Lean(Oss-20b).csv\",\n",
    "    \"IndustryOR_76-100_Lean(Oss-20b).csv\",\n",
    "]\n",
    "try:\n",
    "    combined_df = read_and_combine_csvs(file_order)\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"File processing failed: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"\\nRunning Gurobi Code...\")\n",
    "objective_values = []\n",
    "start_time = time.time()\n",
    "success_count = 0\n",
    "\n",
    "for i, row in combined_df.iterrows():\n",
    "    full_text = row['model_output']\n",
    "    \n",
    "    code = extract_python_code(full_text)\n",
    "    \n",
    "    if code:\n",
    "        print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
    "        result = run_gurobi_code(code)\n",
    "        objective_values.append(result)\n",
    "        if result is not None:\n",
    "            success_count += 1\n",
    "    else:\n",
    "        print(f\"Warning: No Python code found in row {i+1}\")\n",
    "        objective_values.append(None)\n",
    "\n",
    "combined_df['best_objective'] = objective_values\n",
    "\n",
    "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
    "print(f\"Success: {success_count}/{len(combined_df)}\")\n",
    "\n",
    "print(\"\\nFirst few objective values:\")\n",
    "for i in range(min(5, len(combined_df))):\n",
    "    print(f\"Row {i+1}: Best objective = {objective_values[i]}\")\n",
    "\n",
    "output_file = \"IndustryOR_Lean(Oss-20b)_result.xlsx\"\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MAMO easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MAMOe = pd.read_csv('Test_Dataset/Small-scale/MAMO_EasyLP_fixed.csv')\n",
    "test_MAMOe1=test_MAMOe[:100]\n",
    "test_MAMOe2=test_MAMOe[100:200]\n",
    "test_MAMOe3=test_MAMOe[200:300]\n",
    "test_MAMOe4=test_MAMOe[300:400]\n",
    "test_MAMOe5=test_MAMOe[400:500]\n",
    "test_MAMOe6=test_MAMOe[500:600]\n",
    "test_MAMOe7=test_MAMOe[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe1, output_code_MAMOe1 = run_test(test_MAMOe1,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe1['Query'], 'model_output':output_model_MAMOe1, 'code_output':output_code_MAMOe1})\n",
    "output_df.to_csv(\"MAMOe1_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe2, output_code_MAMOe2 = run_test(test_MAMOe2,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe2['Query'], 'model_output':output_model_MAMOe2, 'code_output':output_code_MAMOe2})\n",
    "output_df.to_csv(\"MAMOe2_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe3, output_code_MAMOe3 = run_test(test_MAMOe3,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe3['Query'], 'model_output':output_model_MAMOe3, 'code_output':output_code_MAMOe3})\n",
    "output_df.to_csv(\"MAMOe3_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe4, output_code_MAMOe4 = run_test(test_MAMOe4,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe4['Query'], 'model_output':output_model_MAMOe4, 'code_output':output_code_MAMOe4})\n",
    "output_df.to_csv(\"MAMOe4_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe5, output_code_MAMOe5 = run_test(test_MAMOe5,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe5['Query'], 'model_output':output_model_MAMOe5, 'code_output':output_code_MAMOe5})\n",
    "output_df.to_csv(\"MAMOe5_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe6, output_code_MAMOe6 = run_test(test_MAMOe6,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe6['Query'], 'model_output':output_model_MAMOe6, 'code_output':output_code_MAMOe6})\n",
    "output_df.to_csv(\"MAMOe6_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOe7, output_code_MAMOe7 = run_test(test_MAMOe7,llm1)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOe7['Query'], 'model_output':output_model_MAMOe7, 'code_output':output_code_MAMOe7})\n",
    "output_df.to_csv(\"MAMOe7_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=[\n",
    "    \"MAMOe1_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe2_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe3_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe4_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe5_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe6_Lean(Oss-20b).csv\",\n",
    "    \"MAMOe7_Lean(Oss-20b).csv\",\n",
    "]\n",
    "try:\n",
    "    combined_df = read_and_combine_csvs(file_order)\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"File processing failed: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(\"\\nRunning Gurobi Code...\")\n",
    "objective_values = []\n",
    "start_time = time.time()\n",
    "success_count = 0\n",
    "\n",
    "for i, row in combined_df.iterrows():\n",
    "    full_text = row['model_output']\n",
    "    \n",
    "    code = extract_python_code(full_text)\n",
    "    \n",
    "    if code:\n",
    "        print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
    "        result = run_gurobi_code(code)\n",
    "        objective_values.append(result)\n",
    "        if result is not None:\n",
    "            success_count += 1\n",
    "    else:\n",
    "        print(f\"Warning: No Python code found in row {i+1}\")\n",
    "        objective_values.append(None)\n",
    "\n",
    "combined_df['best_objective'] = objective_values\n",
    "\n",
    "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
    "print(f\"Success: {success_count}/{len(combined_df)}\")\n",
    "\n",
    "print(\"\\nFirst few objective values:\")\n",
    "for i in range(min(5, len(combined_df))):\n",
    "    print(f\"Row {i+1}: Best objective = {objective_values[i]}\")\n",
    "\n",
    "output_file = \"MAMOe_Lean(Oss-20b)_result.xlsx\"\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MAMO Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MAMOc = pd.read_csv('Test_Dataset/Small-scale/MAMO_ComplexLP_fixed.csv')\n",
    "test_MAMOc1=test_MAMOc[:25]\n",
    "test_MAMOc2=test_MAMOc[25:50]\n",
    "test_MAMOc3=test_MAMOc[50:75]\n",
    "test_MAMOc4=test_MAMOc[75:100]\n",
    "test_MAMOc5=test_MAMOc[100:125]\n",
    "test_MAMOc6=test_MAMOc[125:150]\n",
    "test_MAMOc7=test_MAMOc[150:175]\n",
    "test_MAMOc8=test_MAMOc[175:]\n",
    "test_MAMOc8\n",
    "\n",
    "classify_problem = llm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc1, output_code_MAMOc1 = run_test(test_MAMOc1,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc1['Query'], 'model_output':output_model_MAMOc1, 'code_output':output_code_MAMOc1})\n",
    "output_df.to_csv(\"MAMOc_1-25_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc2, output_code_MAMOc2 = run_test(test_MAMOc2,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc2['Query'], 'model_output':output_model_MAMOc2, 'code_output':output_code_MAMOc2})\n",
    "output_df.to_csv(\"MAMOc_26-50_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc3, output_code_MAMOc3 = run_test(test_MAMOc3,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc3['Query'], 'model_output':output_model_MAMOc3, 'code_output':output_code_MAMOc3})\n",
    "output_df.to_csv(\"MAMOc_51-75_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc4,output_code_MAMOc4 = run_test(test_MAMOc4,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc4['Query'], 'model_output':output_model_MAMOc4, 'code_output':output_code_MAMOc4})\n",
    "output_df.to_csv(\"MAMOc_76-100_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc5,output_code_MAMOc5 = run_test(test_MAMOc5,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc5['Query'], 'model_output':output_model_MAMOc5, 'code_output':output_code_MAMOc5})\n",
    "output_df.to_csv(\"MAMOc_101-125_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc6,output_code_MAMOc6 = run_test(test_MAMOc6,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc6['Query'], 'model_output':output_model_MAMOc6, 'code_output':output_code_MAMOc6})\n",
    "output_df.to_csv(\"MAMOc_126-150_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc7,output_code_MAMOc7 = run_test(test_MAMOc7,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc7['Query'], 'model_output':output_model_MAMOc7, 'code_output':output_code_MAMOc7})\n",
    "output_df.to_csv(\"MAMOc_151-175_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_MAMOc8,output_code_MAMOc8 = run_test(test_MAMOc8,classify_problem)\n",
    "output_df = pd.DataFrame({'Query': test_MAMOc8['Query'], 'model_output':output_model_MAMOc8, 'code_output':output_code_MAMOc8})\n",
    "output_df.to_csv(\"MAMOc_176-202_Lean(Oss-20b).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=[\n",
    "    \"MAMOc_1-25_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_26-50_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_51-75_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_76-100_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_101-125_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_126-150_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_151-175_Lean(Oss-20b).csv\",\n",
    "    \"MAMOc_176-202_Lean(Oss-20b).csv\",\n",
    "]\n",
    "try:\n",
    "    combined_df = read_and_combine_csvs(file_order)\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"File processing failed: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(\"\\nRunning Gurobi Code...\")\n",
    "objective_values = []\n",
    "start_time = time.time()\n",
    "success_count = 0\n",
    "\n",
    "for i, row in combined_df.iterrows():\n",
    "    full_text = row['code_output']\n",
    "    \n",
    "    code = extract_python_code(full_text)\n",
    "    code = full_text\n",
    "    if code:\n",
    "        print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
    "        result = run_gurobi_code(code)\n",
    "        objective_values.append(result)\n",
    "        if result is not None:\n",
    "            success_count += 1\n",
    "    else:\n",
    "        print(f\"Warning: No Python code found in row {i+1}\")\n",
    "        objective_values.append(None)\n",
    "\n",
    "combined_df['best_objective'] = objective_values\n",
    "\n",
    "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
    "print(f\"Success: {success_count}/{len(combined_df)}\")\n",
    "\n",
    "print(\"\\nFirst few objective values:\")\n",
    "for i in range(min(5, len(combined_df))):\n",
    "    print(f\"Row {i+1}: Best objective = {objective_values[i]}\")\n",
    "\n",
    "output_file = \"MAMOc_Lean(Oss-20b)_result.xlsx\"\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
