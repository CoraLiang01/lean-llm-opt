{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33012,
     "status": "ok",
     "timestamp": 1747133587613,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "6_tk5ToJnv1T",
    "outputId": "1d8d81c6-d2f1-487e-cc6e-0eda5880f8e3"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.agents import initialize_agent, AgentType, Tool\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import time as time_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-CG3Rdib_UA"
   },
   "source": [
    "# Problem Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1747133587728,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "yJSQHYb2u_Te"
   },
   "outputs": [],
   "source": [
    "def build_llm(model: str = \"gpt-oss:20b\", temperature: float = 0.0) -> ChatOllama:\n",
    "    return ChatOllama(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        base_url=\"http://localhost:11434\",   \n",
    "        timeout=120                          \n",
    "    )\n",
    "\n",
    "llm1 = build_llm()         \n",
    "llm2 = build_llm()       \n",
    "llm3 = build_llm()     \n",
    "llm_code = build_llm()    \n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "def Classification_Agent(file_path=\"Large_Scale_Or_Files/RefData.csv\"):\n",
    "    # Load and process the data\n",
    "    loader1 = CSVLoader(file_path=\"Large_Scale_Or_Files/RefData.csv\", encoding=\"utf-8\")\n",
    "    refdata = loader1.load()\n",
    "    refdocuments = refdata\n",
    "\n",
    "    vectors1 = FAISS.from_documents(refdocuments, embeddings)\n",
    "    retriever1 = vectors1.as_retriever(search_kwargs={'k': 5})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm1,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever1,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    qa_tool1 = Tool(\n",
    "        name=\"FileQA\",\n",
    "        func=qa_chain.invoke,\n",
    "        description=(\n",
    "            \"Use this tool to answer questions about the problem type of the text. \"\n",
    "            \"Provide the question as input, and the tool will retrieve the relevant information from the file and use it to answer the question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    few_shot_examples = \"\"\"\n",
    "\n",
    "    Question: What is the problem type in operation of the text? Please give the answer directly. Text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1,w_2,w_3.There is an independent demand stream for each of the products. The objective of the company is to decide which demands to be fufilled over a ﬁnite sales horizon [0,10] to maximize the total expected revenue from ﬁxed initial inventories. The on-hand inventories for the three items are c_1,c_2,c_3 respectively. During the sales horizon, replenishment is not allowed and there is no any in-transit inventories. Customers who want to purchase P1,P2,P3 arrive at each period accoring to a Poisson process with a_1,a_2,a_3 the arrival rates respectively. Decision variables y_1,y_2,y_3 correspond to the number of requests that the firm plans to fulfill for product 1,2,3. These variables are all positive integers.\n",
    "\n",
    "    Thought: I need to determine the problem type of the text. I'll use the FileQA tool to retrieve the relevant information.\n",
    "\n",
    "    Action: FileQA\n",
    "\n",
    "    Action Input: \"What is the problem type in operation of the text? text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1, w_2, w_3. ...\"\n",
    "\n",
    "    Observation: The problem type of the text is Network Revenue Management.\n",
    "\n",
    "    Final Answer: Network Revenue Management.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prefix = f\"\"\"You are a helpful assistant that can answer questions about operation problems.\n",
    "\n",
    "    Use the following examples as a guide. Always use the FileQA tool when you need to retrieve information from the file:\n",
    "\n",
    "\n",
    "    {few_shot_examples}\n",
    "\n",
    "    When you need to find information from the file, use the provided tools. And answer the question by given the answer directly. For example,\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    suffix = \"\"\"\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Question: {input}\n",
    "    {agent_scratchpad}\"\"\"\n",
    "\n",
    "    agent_pc = initialize_agent(\n",
    "        tools=[qa_tool1],\n",
    "        llm=llm1,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix.format(few_shot_examples=few_shot_examples),\n",
    "            \"suffix\": suffix,\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent_pc\n",
    "\n",
    "def Problemtype(query):\n",
    "    agent_pc = Classification_Agent(file_path=\"Large_Scale_Or_Files/RefData.csv\")\n",
    "    category_original=agent_pc.invoke(f\"What is the problem type in operation of the text? text:{query}\")\n",
    "    type_output = category_original['output']\n",
    "    return type_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdmJx7Wwfye9"
   },
   "source": [
    "# ProblemSolvingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0wro12YcDS1"
   },
   "source": [
    "## PreProcess: Sharing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1747133587729,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "jooHQYMjZUj9"
   },
   "outputs": [],
   "source": [
    "def LoadFiles():\n",
    "  v1 = pd.read_csv('Test_Dataset/Air_NRM/v1.csv')\n",
    "  v2 = pd.read_csv('Test_Dataset/Air_NRM/v2.csv')\n",
    "  demand = pd.read_csv('Test_Dataset/Air_NRM/od_demand.csv')\n",
    "  flight = pd.read_csv('Test_Dataset/Air_NRM/flight.csv')\n",
    "  return v1,v2,demand,flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747133587730,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "-Im3k7d-YncG"
   },
   "outputs": [],
   "source": [
    "def New_Vectors_Flight(query):\n",
    "  v1,v2,demand,df = LoadFiles()\n",
    "  new_docs = []\n",
    "  for _, row in df.iterrows():\n",
    "      new_docs.append(Document(\n",
    "          page_content=f\"OD={row['Oneway_OD']},Departure_Time_Flight1={row['Departure Time']},Oneway_Product={row['Oneway_Product']},avg_price={row['Avg Price']}\",\n",
    "          metadata={\n",
    "              'OD': row['Oneway_OD'],\n",
    "              'time': row['Departure Time'],\n",
    "              'product': row['Oneway_Product'],\n",
    "              'avg_price': row['Avg Price']\n",
    "          }\n",
    "      ))\n",
    "\n",
    "  new_vectors = FAISS.from_documents(new_docs, embeddings)\n",
    "  return new_vectors\n",
    "\n",
    "def New_Vectors_Demand(query):\n",
    "  v1,v2,demand,df = LoadFiles()\n",
    "  new_docs = []\n",
    "  for _, row in demand.iterrows():\n",
    "      new_docs.append(Document(\n",
    "          page_content=f\"OD={row['Oneway_OD']}, avg_pax={row['Avg Pax']}\",\n",
    "          metadata={\n",
    "              'OD': row['Oneway_OD'],\n",
    "              'avg_pax': row['Avg Pax']\n",
    "          }\n",
    "      ))\n",
    "\n",
    "  new_vectors = FAISS.from_documents(new_docs, embeddings)\n",
    "  return new_vectors\n",
    "\n",
    "\n",
    "def retrieve_key_information(query):\n",
    "    flight_pattern = r\"\\(OD\\s*=\\s*\\(\\s*'(\\w+)'\\s*,\\s*'(\\w+)'\\s*\\)\\s+AND\\s+Departure\\s+Time='(\\d{1,2}:\\d{2})'\\)\"\n",
    "    matches = re.findall(flight_pattern, query)\n",
    "\n",
    "    flight_strings = [f\"(OD = ('{origin}', '{destination}') AND Departure Time='{departure_time}')\" for origin, destination, departure_time in matches]\n",
    "\n",
    "    new_query = \"Retrieve avg_price, avg_pax, value_list, ratio_list, value_0_list, ratio_0_list, and flight_capacity for the following flights:\\n\" + \", \".join(flight_strings) + \".\"\n",
    "\n",
    "    return new_query\n",
    "def retrieve_time_period(departure_time):\n",
    "    intervals = {\n",
    "        '12pm~6pm': (time_class(12, 0), time_class(18, 0)),\n",
    "        '6pm~10pm': (time_class(18, 0), time_class(22, 0)),\n",
    "        '10pm~8am': (time_class(22, 0), time_class(8, 0)),\n",
    "        '8am~12pm': (time_class(8, 0), time_class(12, 0))\n",
    "    }\n",
    "\n",
    "    if isinstance(departure_time, str):\n",
    "        try:\n",
    "            hours, minutes = map(int, departure_time.split(':'))\n",
    "            departure_time = time_class(hours, minutes)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Time format should be: 'HH:MM'\")\n",
    "\n",
    "    for interval_name, (start, end) in intervals.items():\n",
    "        if start < end:\n",
    "            if start <= departure_time < end:\n",
    "                return interval_name\n",
    "        else:\n",
    "            if departure_time >= start or departure_time < end:\n",
    "                return interval_name\n",
    "\n",
    "    return \"Unknown\"\n",
    "def retrieve_parameter(O,time_interval,product):\n",
    "    v1,v2,df,demand = LoadFiles()\n",
    "    time_interval = f'({time_interval})'\n",
    "    key = product + '*' + time_interval\n",
    "    _value_ = 0\n",
    "    _ratio_ = 0\n",
    "    no_purchase_value = 0\n",
    "    no_purchase_value_ratio = 0\n",
    "    subset = v1[v1['OD Pairs'] == O]\n",
    "    if key in subset.columns and not subset.empty:\n",
    "        _value_ = subset[key].values[0]\n",
    "\n",
    "    subset2 = v2[v2['OD Pairs'] == O]\n",
    "    if key in subset2.columns and not subset2.empty:\n",
    "        _ratio_ = subset2[key].values[0]\n",
    "\n",
    "    if 'no_purchase' in subset.columns and not subset.empty:\n",
    "        no_purchase_value = subset['no_purchase'].values[0]\n",
    "\n",
    "    if 'no_purchase' in subset2.columns and not subset2.empty:\n",
    "        no_purchase_value_ratio = subset2['no_purchase'].values[0]\n",
    "    return _value_,_ratio_,no_purchase_value,no_purchase_value_ratio\n",
    "\n",
    "def generate_coefficients(OD,time):\n",
    "    departure_time = datetime.strptime(time, '%H:%M').time()\n",
    "    time_interval = retrieve_time_period(departure_time)\n",
    "    value_1,ratio_1,value_0,ratio_0 = retrieve_parameter(OD,time_interval,'Eco_flexi')\n",
    "\n",
    "    value_2,ratio_2,value_0,ratio_0 = retrieve_parameter(OD,time_interval,'Eco_lite')\n",
    "\n",
    "    return value_1,ratio_1,value_2,ratio_2,value_0,ratio_0\n",
    "\n",
    "\n",
    "def clean_text_preserve_newlines(text):\n",
    "    cleaned = re.sub(r'\\x1b\\[[0-9;]*[mK]', '', text)\n",
    "    cleaned = re.sub(r'[^\\x20-\\x7E\\n]', '', cleaned)\n",
    "    cleaned = re.sub(r'(\\n\\s+)(\\w+\\s*=)', r'\\n\\2', cleaned)\n",
    "    cleaned = re.sub(r'\\[\\s+', '[', cleaned)\n",
    "    cleaned = re.sub(r'\\s+\\]', ']', cleaned)\n",
    "    cleaned = re.sub(r',\\s+', ', ', cleaned)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6qed6ticWeL"
   },
   "source": [
    "## Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1747133587901,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "pdfTmljSVvXt"
   },
   "outputs": [],
   "source": [
    "def csv_qa_tool_flow(query: str,raw_query):\n",
    "    new_vectors = New_Vectors_Flight(query)\n",
    "    matches = re.findall(r\"\\(OD\\s*=\\s*(\\(\\s*'[^']+'\\s*,\\s*'[^']+'\\s*\\))\\s+AND\\s+Departure\\s*Time\\s*=\\s*'(\\d{1,2}:\\d{2})'\\)\", query)\n",
    "    num_match = re.search(r\"optimal\\s+(\\d+)\\s+flights\", raw_query, re.IGNORECASE)\n",
    "    num_flights = int(num_match.group(1)) if num_match else None\n",
    "    capacity_match = re.search(r\"Eco_flex ticket consumes (\\d+\\.?\\d*)\\s*units\", raw_query)\n",
    "    if matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\),\\s*'(\\d{1,2}:\\d{2})'\\)\"\n",
    "        matches_2 = re.findall(pattern, query)\n",
    "\n",
    "    if capacity_match:\n",
    "        eco_flex_capacity = capacity_match.group(1)\n",
    "    else:\n",
    "        eco_flex_capacity = 1.2\n",
    "\n",
    "    sigma_inflow_A = []\n",
    "    sigma_outflow_A = []\n",
    "    sigma_inflow_B = []\n",
    "    sigma_outflow_B = []\n",
    "    sigma_inflow_C = []\n",
    "    sigma_outflow_C = []\n",
    "\n",
    "    if matches == []:\n",
    "        matches = matches_2\n",
    "        for origin,destination,time in matches:\n",
    "            if origin == 'A':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_A.append(flight_name)\n",
    "            elif origin == 'B':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_B.append(flight_name)\n",
    "            elif origin == 'C':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_C.append(flight_name)\n",
    "            elif destination == 'A':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_A.append(flight_name)\n",
    "            elif destination == 'B':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_B.append(flight_name)\n",
    "            elif destination == 'C':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_C.append(flight_name)\n",
    "    \n",
    "    else:\n",
    "        a_origin_flights_A_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'A'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_B_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'B'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_C_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'C'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_A = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'A'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_B = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'B'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_C = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'C'\n",
    "        ]\n",
    "\n",
    "        for od, time in a_origin_flights_A:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_A.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_B:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_B.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_C:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_C.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_A_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_A.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_B_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_B.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_C_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_C.append(flight_name)\n",
    "\n",
    "    avg_price, x, x_o, ratio_0_list, ratio_list, value_list, value_0_list, avg_pax = {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    y = {}\n",
    "    N_l = {\"AB\":[],\"AC\":[],\"BA\":[],\"CA\":[]}\n",
    "\n",
    "    if matches == []:\n",
    "        matches = matches_2\n",
    "        for origin,destination,time in matches:\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            code_y = f\"({origin}{destination},{time})\"\n",
    "            y[code_y] = f\"y_{origin}{destination}_{time}\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1 = retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "    else:\n",
    "        for match in matches:\n",
    "            origin = match[0][2]\n",
    "            destination = match[0][7]\n",
    "            time = match[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            code_y = f\"({origin}{destination},{time})\"\n",
    "            y[code_y] = f\"y_{origin}{destination}_{time}\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "\n",
    "\n",
    "    od_matches = re.findall(\n",
    "    r\"OD\\s*=\\s*\\(\\s*'([^']+)'\\s*,\\s*'([^']+)'\\s*\\)\", \n",
    "    query\n",
    "    )\n",
    "    if od_matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\)\"\n",
    "        matches = re.findall(pattern, query)\n",
    "        od_matches = matches\n",
    "    \n",
    "    od_matches = list(set(od_matches))\n",
    "\n",
    "    new_vectors_demand = New_Vectors_Demand(query)\n",
    "    for origin, dest in od_matches:\n",
    "        od = str((origin, dest))\n",
    "        code_o = f\"{origin}{dest}\"\n",
    "        x_o[code_o] = f\"x_{origin}{dest}_o\"\n",
    "        retriever = new_vectors_demand.as_retriever(search_kwargs={'k': 1})\n",
    "    \n",
    "        doc_1= retriever.invoke(f\"OD={od}, avg_pax=\")\n",
    "        content = doc_1[0].page_content\n",
    "\n",
    "        pattern = r',\\s*(?=\\w+=)'\n",
    "        parts = re.split(pattern, content)\n",
    "        pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')\n",
    "            if key == 'avg_pax':\n",
    "                avg_pax[code_o] = value\n",
    "            \n",
    "    doc = f\"y = {y}\\n\"\n",
    "    doc += f\"avg_price={avg_price}\\n\"\n",
    "    doc += f\"value_list ={value_list}\\n\"\n",
    "    doc += f\"ratio_list={ratio_list}\\n\"\n",
    "    doc += f\"value_0_list={value_0_list}\\n\"\n",
    "    doc += f\"ratio_0_list={ratio_0_list}\\n\"\n",
    "    doc += f\"avg_pax={avg_pax}\\n\"\n",
    "    doc += f\"capacity_consum={eco_flex_capacity}\\n\"\n",
    "    doc += f\"option_num={num_flights}\\n\"\n",
    "    doc += f\"sigma_inflow_A={sigma_inflow_A}\\n\"\n",
    "    doc += f\"sigma_outflow_A={sigma_outflow_A}\\n\"\n",
    "    doc += f\"sigma_inflow_B={sigma_inflow_B}\\n\"\n",
    "    doc += f\"sigma_outflow_B={sigma_outflow_B}\\n\"\n",
    "    doc += f\"sigma_inflow_C={sigma_inflow_C}\\n\"\n",
    "    doc += f\"sigma_outflow_C={sigma_outflow_C}\\n\"\n",
    "    doc += f\"M = 10000000\\n\"\n",
    "    doc += f\"flight_capacity=187\\n\"\n",
    "\n",
    "    return doc\n",
    "\n",
    "def retrieve_similar_docs(query,retriever):\n",
    "    \n",
    "    similar_docs = retriever.invoke(query)\n",
    "\n",
    "    results = []\n",
    "    for doc in similar_docs:\n",
    "        results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def final_tool_flow(problem_description):\n",
    "    example_matches = retrieve_key_information(problem_description)\n",
    "    example_data_description = csv_qa_tool_flow(example_matches,problem_description) \n",
    "    example_data_description = example_data_description.replace('np.float64', 'float')\n",
    "    example_data_description = example_data_description.replace('np.int64', 'int') \n",
    "    return example_data_description\n",
    "\n",
    "def FlowAgent(query):\n",
    "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_SBLP_Flow.csv\", encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    documents = data\n",
    "    vectors = FAISS.from_documents(documents, embeddings)\n",
    "    retriever = vectors.as_retriever(search_kwargs={'k': 1})\n",
    "    similar_results = retrieve_similar_docs(query,retriever)\n",
    "    problem_description = similar_results[0]['content'].replace(\"prompt:\", \"\").strip()  \n",
    "    example_data_description = final_tool_flow(problem_description)\n",
    "\n",
    "    fewshot_example = f'''\n",
    "    Question: {problem_description}\n",
    "\n",
    "    Thought: I need to retrive the required OD and Departure Time information.\n",
    "\n",
    "    Action: CSVQA\n",
    "\n",
    "    Action Input: {problem_description}\n",
    "\n",
    "    Observation: {example_data_description}\n",
    "\n",
    "    Thought: I have successfully received the data. I must now return it.\n",
    "    Final Answer: {example_data_description}\n",
    "    '''\n",
    "    tools = [Tool(name=\"CSVQA\", func=final_tool_flow, description=\"Retrieve flight data.\")]\n",
    "    fewshot_example = fewshot_example.replace('{', '{{')\n",
    "    fewshot_example = fewshot_example.replace('}', '}}')    \n",
    "    prefix = (\n",
    "f\"{fewshot_example}\\n\\n\"\n",
    "\"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "\"Output **ONLY** the following three lines:\\n\\n\"\n",
    "\"Thought: I must first use the CSVQA tool to retrieve necessary data based on the user query.\\n\"\n",
    "\"Action: CSVQA\\n\"\n",
    "\"Action Input: {input}\\n\"\n",
    ")\n",
    "\n",
    "    suffix = (\n",
    "\"### FINAL STEP\\n\"\n",
    "\"Use the latest Observation above.\\n\"\n",
    "\"Output exactly two lines:\\n\"\n",
    "\"Thought: Data retrieval complete. I will now return it.\\n\"\n",
    "\"Final Answer: <copy the Observation content above EXACTLY as is>\"\n",
    ")\n",
    "    agent2 = initialize_agent(\n",
    "        tools,\n",
    "        llm=llm2,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    return agent2\n",
    "\n",
    "def policy_sblp_flow_model_code(query):\n",
    "    query = query.replace('{', '{{')\n",
    "    query = query.replace('}', '}}')    \n",
    "    agent2 = FlowAgent(query)\n",
    "    result = agent2.invoke({\"input\": query})\n",
    "    example_data_description = result['output']\n",
    "    final_answer = f'''### Objective Function\n",
    "\n",
    "\\[\n",
    "\\max \\quad \\sum_(l,k,j) avg\\_price[(l,k,j)] \\cdot x[(l,k,j)]\n",
    "\\]\n",
    "\n",
    "### Constraints\n",
    "\n",
    "#### 1. Capacity Constraints\n",
    "\n",
    "For each flight (OD pair \\( l \\), departure time \\( k \\)):\n",
    "\n",
    "\\[\n",
    "capacity\\_consum \\cdot x[(l,k,f)] + x[(l,k,l)] \\leq flight\\_capacity\n",
    "\\]\n",
    "\n",
    "#### 2. Balance Constraints\n",
    "\n",
    "For each OD pair \\( l \\):\n",
    "\n",
    "\\[\n",
    "ratio\\_0\\_list[l] \\cdot x_o[l] + \\sum_k \\sum_j ratio\\_list[(l,k,j)] \\cdot x[(l,k,j)] = avg\\_pax[l]\n",
    "\\]\n",
    "\n",
    "#### 3. Scale Constraints\n",
    "\n",
    "For each ticket option \\((l,k,j)\\):\n",
    "\n",
    "\\[\n",
    "x[(l,k,j)]/value\\_list[(l,k,j)] - x_o[l]/value\\_0\\_list[l] \\leq 0\n",
    "\\]\n",
    "\n",
    "#### 4. Big M Constraints\n",
    "\n",
    "For each ticket option \\((l,k,j)\\):\n",
    "\n",
    "\\[\n",
    "x[(l,k,j)] \\leq 10000000 \\cdot y[(l,k)]\n",
    "\\]\n",
    "\n",
    "#### 5. Cardinality Constraint\n",
    "\n",
    "\\[\n",
    "\\sum_(l,k) y[(l,k)] \\leq option\\_num\n",
    "\\]\n",
    "\n",
    "\n",
    "#### 6. Flow Conservation Constraints\n",
    "\n",
    "\\[\n",
    "\\sum_(l,k)  \\in sigma_inflow_A y[(l,k)] = \\sum_(l,k)  \\in sigma_outflow_A y[(l,k)]\n",
    "\\]\n",
    "\\[\n",
    "\\sum_(l,k)  \\in sigma_inflow_B y[(l,k)] = \\sum_(l,k)  \\in sigma_outflow_B y[(l,k)]\n",
    "\\]\n",
    "\\[\n",
    "\\sum_(l,k)  \\in sigma_inflow_C y[(l,k)] = \\sum_(l,k)  \\in sigma_outflow_C y[(l,k)]\n",
    "\\]\n",
    "\n",
    "#### 7. Nonnegativity Constraints\n",
    "\n",
    "\\[\n",
    "x[(l,k,j)] \\geq 0, \\quad x_o[l] \\geq 0\n",
    "\\]\n",
    "- x = avg_price.keys()\n",
    "- x_o = avg_pax.keys()\n",
    "\n",
    "#### 8. Binary Constraints\n",
    "\n",
    "\\[\n",
    "y[(l,k)] is binary\n",
    "\\]\n",
    "\n",
    "### Retrieved Information\n",
    "{example_data_description}\n",
    "\n",
    "### Generated Code\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "{example_data_description}\n",
    "total_set = sigma_inflow_A + sigma_outflow_A + sigma_inflow_B + sigma_outflow_B + sigma_inflow_C + sigma_outflow_C\n",
    "total_set = list(set(total_set)) \n",
    "model = gp.Model(\"sales_based_lp\")\n",
    "y = model.addVars(total_set, vtype=GRB.BINARY, name=\"y\")  \n",
    "x = model.addVars(avg_price.keys(), lb=0, name=\"x\") \n",
    "x_o = model.addVars(value_0_list.keys(), lb=0, name=\"x_o\")  \n",
    "\n",
    "model.setObjective(gp.quicksum(avg_price[key] * x[key] for key in avg_price.keys()), GRB.MAXIMIZE)\n",
    "\n",
    "paired_keys = []\n",
    "for i in range(0, len(avg_price.keys()), 2):  \n",
    "    if i + 1 < len(avg_price.keys()):  \n",
    "        names = list(avg_price.keys())\n",
    "        model.addConstr(\n",
    "            capacity_consum* x[names[i]] + x[names[i + 1]] <= 187,\n",
    "            name=f\"capacity_constraint\"\n",
    "        )\n",
    "\n",
    "for l in ratio_0_list.keys():\n",
    "    temp = 0\n",
    "    for key in ratio_list.keys():\n",
    "        if l in key:\n",
    "            temp += ratio_list[key] * x[key]\n",
    "    model.addConstr(\n",
    "        float(ratio_0_list[l]) * x_o[l] + temp == float(avg_pax[l])\n",
    "    )\n",
    "\n",
    "for i in value_list.keys():\n",
    "    for l in ratio_0_list.keys():\n",
    "        if l in i:\n",
    "            model.addConstr(\n",
    "                value_0_list[l]  * x[i] <= value_list[i] * x_o[l]\n",
    "            )\n",
    "\n",
    "\n",
    "for key in avg_price.keys():\n",
    "    for lk in total_set:\n",
    "       if lk.split(',')[0].strip('(').strip(')')==key.split(',')[0].strip('(').strip(')') and lk.split(',')[1].strip('(').strip(')')==key.split(',')[1].strip('(').strip(')'):\n",
    "            model.addConstr(\n",
    "                x[key] <= 10000000 * y[lk],  \n",
    "                name=f\"big_m_constraint\"\n",
    "            )\n",
    "\n",
    "model.addConstr(gp.quicksum(y[lk] for lk in total_set) <= option_num,\n",
    "        name=f\"cardinality_constraint\"\n",
    "    )\n",
    "\n",
    "\n",
    "model.addConstr(\n",
    "    gp.quicksum(y[inflow] for inflow in sigma_inflow_A) == gp.quicksum(y[outflow] for outflow in sigma_outflow_A),\n",
    "    name=f\"flow_conservation_A\"\n",
    ")\n",
    "\n",
    "\n",
    "model.addConstr(\n",
    "    gp.quicksum(y[inflow] for inflow in sigma_inflow_B) == gp.quicksum(y[outflow] for outflow in sigma_outflow_B),\n",
    "    name=f\"flow_conservation_B\"\n",
    ")\n",
    "    \n",
    "model.addConstr(\n",
    "    gp.quicksum(y[inflow] for inflow in sigma_inflow_C) == gp.quicksum(y[outflow] for outflow in sigma_outflow_C),\n",
    "    name=f\"flow_conservation_C\"\n",
    ")\n",
    "\n",
    "model.optimize()\n",
    "\n",
    "\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    print(\"Optimal solution found:\")\n",
    "    for v in model.getVars():\n",
    "        print(v.varName, v.x)\n",
    "    print(\"Optimal objective value:\", model.objVal)\n",
    "else:\n",
    "    print(\"No optimal solution found.\")\n",
    "```\n",
    "'''\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "def ProcessPolicyFlow(query):\n",
    "    output_model = policy_sblp_flow_model_code(query)\n",
    "\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y_VhDMHij2M"
   },
   "source": [
    "## CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1747133588009,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "iWJr_yg3ilVJ"
   },
   "outputs": [],
   "source": [
    "def csv_qa_tool_CA(query: str,raw_query):\n",
    "    new_vectors = New_Vectors_Flight(query)\n",
    "    matches = re.findall(r\"\\(OD\\s*=\\s*(\\(\\s*'[^']+'\\s*,\\s*'[^']+'\\s*\\))\\s+AND\\s+Departure\\s*Time\\s*=\\s*'(\\d{1,2}:\\d{2})'\\)\", query)\n",
    "    capacity_match = re.search(r\"Eco_flex ticket consumes (\\d+\\.?\\d*)\\s*units\", raw_query)\n",
    "\n",
    "    if capacity_match:\n",
    "        eco_flex_capacity = capacity_match.group(1)\n",
    "    else:\n",
    "        eco_flex_capacity = 1.2\n",
    "\n",
    "    avg_price, x, x_o, ratio_0_list, ratio_list, value_list, value_0_list, avg_pax = {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    if matches == []:\n",
    "        pattern = r\"\\('(\\w+)'\\s*,\\s*'(\\d{1,2}:\\d{2})'\\)\"\n",
    "        matches_2 = re.findall(pattern, query)\n",
    "        matches = matches_2\n",
    "        for od,time in matches:\n",
    "            origin = od[0]\n",
    "            destination = od[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = float(value)\n",
    "\n",
    "\n",
    "            doc_2= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] =float(value)\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "    else:\n",
    "        for match in matches:\n",
    "            origin = match[0][2]\n",
    "            destination = match[0][7]\n",
    "            time = match[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "            doc_1= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = float(value)\n",
    "\n",
    "\n",
    "            doc_2= retriever.invoke(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = float(value)\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "\n",
    "    od_matches = re.findall(\n",
    "    r\"OD\\s*=\\s*\\(\\s*'([^']+)'\\s*,\\s*'([^']+)'\\s*\\)\", \n",
    "    query\n",
    "    )\n",
    "\n",
    "    if od_matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\)\"\n",
    "        matches = re.findall(pattern, query)\n",
    "        od_matches = matches\n",
    "    \n",
    "    od_matches = list(set(od_matches))\n",
    "\n",
    "    new_vectors_demand = New_Vectors_Demand(query)\n",
    "    for origin, dest in od_matches:\n",
    "        od = str((origin, dest))\n",
    "        code_o = f\"{origin}{dest}\"\n",
    "        x_o[code_o] = f\"x_{origin}{dest}_o\"\n",
    "        retriever = new_vectors_demand.as_retriever(search_kwargs={'k': 1})\n",
    "       \n",
    "        doc_1= retriever.invoke(f\"OD={od}, avg_pax=\")\n",
    "        content = doc_1[0].page_content\n",
    "\n",
    "        pattern = r',\\s*(?=\\w+=)'\n",
    "        parts = re.split(pattern, content)\n",
    "        pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')\n",
    "            if key == 'avg_pax':\n",
    "                avg_pax[code_o] = value\n",
    "        \n",
    "    doc =f\"avg_price={avg_price}\\n\"\n",
    "    doc +=f\"value_list ={value_list}\\n\"\n",
    "    doc +=f\"ratio_list={ratio_list}\\n\"\n",
    "    doc +=f\"value_0_list={value_0_list}\\n\"\n",
    "    doc +=f\"ratio_0_list={ratio_0_list}\\n\"\n",
    "    doc +=f\"avg_pax={avg_pax}\\n\"\n",
    "    doc +=f\"capacity_consum = {eco_flex_capacity}\\n\"\n",
    "    doc +=f\"flight_capacity = 187 \\n\"\n",
    "    return doc\n",
    "\n",
    "def final_tool_CA(problem_description):\n",
    "    example_matches = retrieve_key_information(problem_description)\n",
    "    example_data_description = csv_qa_tool_CA(example_matches,problem_description)\n",
    "    example_data_description = example_data_description.replace('np.float64', 'float')\n",
    "    example_data_description = example_data_description.replace('np.int64', 'int')\n",
    "    return example_data_description\n",
    "def CA_Agent(query):\n",
    "\n",
    "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_SBLP_CA.csv\", encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    documents = data\n",
    "    vectors = FAISS.from_documents(documents, embeddings)\n",
    "    retriever = vectors.as_retriever(search_kwargs={'k': 1})\n",
    "    similar_results = retrieve_similar_docs(query,retriever)\n",
    "    problem_description = similar_results[0]['content'].replace(\"prompt:\", \"\").strip()   \n",
    "    example_data_description = final_tool_CA(problem_description)\n",
    "    fewshot_example = f'''\n",
    "    Question: {problem_description}\n",
    "\n",
    "    Thought: I need to retrive the required OD and Departure Time information.\n",
    "\n",
    "    Action: CSVQA\n",
    "\n",
    "    Action Input: {problem_description}\n",
    "\n",
    "    Observation: {example_data_description}\n",
    "\n",
    "    Thought: I have successfully received the data. I must now return it.\n",
    "    Final Answer: {example_data_description}\n",
    "    '''\n",
    "    tools = [Tool(name=\"CSVQA\", func=final_tool_CA, description=\"Retrieve flight data.\")]\n",
    "    fewshot_example = fewshot_example.replace('{', '{{')\n",
    "    fewshot_example = fewshot_example.replace('}', '}}')    \n",
    "\n",
    "    prefix = (\n",
    "f\"{fewshot_example}\\n\\n\"\n",
    "\"You MUST follow EXACTLY the pattern below for the **FIRST** response.\\n\"\n",
    "\"Output **ONLY** the following three lines:\\n\\n\"\n",
    "\"Thought: I must first use the CSVQA tool to retrieve necessary data based on the user query.\\n\"\n",
    "\"Action: CSVQA\\n\"\n",
    "\"Action Input: {input}\\n\"\n",
    ")\n",
    "\n",
    "    suffix = (\n",
    "    \"### FINAL STEP\\n\"\n",
    "    \"Use the latest Observation above.\\n\"\n",
    "    \"Output exactly two lines:\\n\"\n",
    "    \"Thought: Data retrieval complete. I will now return it.\\n\"\n",
    "    \"Final Answer: <copy the Observation content above EXACTLY as is>\"\n",
    ")\n",
    "\n",
    "    agent2 = initialize_agent(\n",
    "    tools,\n",
    "    llm=llm2,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix,\n",
    "    },\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations = 5\n",
    "    )\n",
    "\n",
    "    return agent2\n",
    "\n",
    "\n",
    "def get_answer(query):\n",
    "    agent2 = CA_Agent(query)\n",
    "    result = agent2.invoke({\"input\": query})\n",
    "    output_model = result['output']\n",
    "    final_answer = f'''### Objective Function\n",
    "\n",
    "\\[\n",
    "\\max \\quad \\sum_(l,k,j) avg\\_price[(l,k,j)] \\cdot x[(l,k,j)]\n",
    "\\]\n",
    "\n",
    "### Constraints\n",
    "\n",
    "#### 1. Capacity Constraints\n",
    "\n",
    "For each flight (OD pair \\( l \\), departure time \\( k \\)):\n",
    "\n",
    "\\[\n",
    "capacity\\_consum \\cdot x[(l,k,f)] + x[(l,k,l)] \\leq flight\\_capacity\n",
    "\\]\n",
    "\n",
    "#### 2. Balance Constraints\n",
    "\n",
    "For each OD pair \\( l \\):\n",
    "\n",
    "\\[\n",
    "ratio\\_0\\_list[l] \\cdot x_o[l] + \\sum_k \\sum_j ratio\\_list[(l,k,j)] \\cdot x[(l,k,j)] = avg\\_pax[l]\n",
    "\\]\n",
    "\n",
    "#### 3. Scale Constraints\n",
    "\n",
    "For each ticket option \\((l,k,j)\\):\n",
    "\n",
    "\\[\n",
    "x[(l,k,j)]/value\\_list[(l,k,j)] - x_o[l]/value\\_0\\_list[l] \\leq 0\n",
    "\\]\n",
    "\n",
    "#### 4. Nonnegativity Constraints\n",
    "\n",
    "\\[\n",
    "x[(l,k,j)] \\geq 0, \\quad x_o[l] \\geq 0\n",
    "\\]\n",
    "- x = avg_price.keys()\n",
    "- x_o = avg_pax.keys()\n",
    "\n",
    "### Retrieved Information\n",
    "{output_model}\n",
    "\n",
    "### Generated Code\n",
    "\n",
    "```python\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "{output_model}\n",
    "model = gp.Model(\"sales_based_lp\")\n",
    "x = model.addVars(avg_price.keys(), lb=0, name=\"x\") \n",
    "x_o = model.addVars(value_0_list.keys(), lb=0, name=\"x_o\")  \n",
    "\n",
    "model.setObjective(gp.quicksum(avg_price[key] * x[key] for key in avg_price.keys()), GRB.MAXIMIZE)\n",
    "\n",
    "paired_keys = []\n",
    "for i in range(0, len(avg_price.keys()), 2):  \n",
    "    if i + 1 < len(avg_price.keys()):  \n",
    "        names = list(avg_price.keys())\n",
    "        model.addConstr(\n",
    "            capacity_consum* x[names[i]] + x[names[i + 1]] <= 187,\n",
    "            name=f\"capacity_constraint\"\n",
    "        )\n",
    "\n",
    "for l in ratio_0_list.keys():\n",
    "    temp = 0\n",
    "    for key in ratio_list.keys():\n",
    "        if l in key:\n",
    "            temp += ratio_list[key] * x[key]\n",
    "    model.addConstr(\n",
    "        float(ratio_0_list[l]) * x_o[l] + temp == float(avg_pax[l])\n",
    "    )\n",
    "\n",
    "for i in value_list.keys():\n",
    "    for l in ratio_0_list.keys():\n",
    "        if l in i:\n",
    "            model.addConstr(\n",
    "                value_0_list[l]  * x[i] <= value_list[i] * x_o[l]\n",
    "            )\n",
    "\n",
    "model.optimize()\n",
    "\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    print(\"Optimal solution found:\")\n",
    "    for v in model.getVars():\n",
    "        print(v.varName, v.x)\n",
    "    print(\"Optimal objective value:\", model.objVal)\n",
    "else:\n",
    "    print(\"No optimal solution found.\")\n",
    "```\n",
    "Remember:\n",
    "- Include all parameters in full form.\n",
    "- Follow the structure and formatting in the example.'''\n",
    "\n",
    "    return final_answer\n",
    "def ProcessCA(query):\n",
    "    CA_model = get_answer(query)\n",
    "    return CA_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8BpbHLEf5Qw"
   },
   "source": [
    "# Combined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747133593331,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "NAKk6nmRTsE6"
   },
   "outputs": [],
   "source": [
    "def Process_Input(query):\n",
    "  category_original = Problemtype(query)\n",
    "  print(f\"Problem type classification finished, it belongs to {category_original}.\")\n",
    "  if \"Sales-Based Linear Programming\" or \"Sales-Based\"in    category_original:\n",
    "    print(\"Processing AirNRM queries\")\n",
    "    if \"flow conservation constraints\" in query or \"flow conservation constraint\" in query:\n",
    "        print('----------Flow Constraints----------')\n",
    "        print(\"Recommend Optimal Flights With Flow Conervation Constraints\")\n",
    "        output_model= ProcessPolicyFlow(query)\n",
    "        Type = \"Policy_Flow\"\n",
    "\n",
    "    else:\n",
    "        print('----------CA----------')\n",
    "        print(\"Only Develop Mathematic Formulations. No Recommendation for Flights.\")\n",
    "        output_model = ProcessCA(query)\n",
    "        Type = \"CA\"\n",
    "  return Type,output_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h11P29Fhqn-x"
   },
   "source": [
    "# Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1747133826275,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "vMuGJElru6RR"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def Batch_Process_Queries(df, query_column='Query'):\n",
    "    \"\"\"Process in Batches\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for query in tqdm(df['Query'], desc=\"Processing Queries\"):\n",
    "        category, output_model = Process_Input(query)\n",
    "        record = {\n",
    "            \"Category\": category,\n",
    "            \"Original_Query\": query,\n",
    "            \"Output\": output_model,\n",
    "        }\n",
    "        results.append(record)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_objective(log_text):\n",
    "   \n",
    "    if \"Optimal objective\" in log_text:\n",
    "        pattern = r\"Optimal objective value: ([-+]?\\d*\\.?\\d+)\"\n",
    "        match = re.search(pattern, log_text)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "\n",
    "    elif \"Best objective\" in log_text:\n",
    "        pattern = r\"Best objective\\s+([-+]?\\d*\\.?\\d+e[-+]?\\d+)\"\n",
    "        match = re.search(pattern, log_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return None  \n",
    "\n",
    "\n",
    "\n",
    "def gain_obj(df_out):\n",
    "    output_code = df_out['Code'].tolist()\n",
    "    obj = []\n",
    "    i = 1\n",
    "\n",
    "    for code in output_code:\n",
    "        print(i)\n",
    "        namespace = {'gp': gp, 'GRB': GRB}\n",
    "        \n",
    "  \n",
    "        log_output = []\n",
    "        \n",
    "\n",
    "        class LogCapture:\n",
    "            def write(self, message):\n",
    "                log_output.append(message)\n",
    "\n",
    "            def flush(self):  \n",
    "                pass\n",
    "            \n",
    "        import sys\n",
    "        log_capture = LogCapture()\n",
    "        original_stdout = sys.stdout  \n",
    "        sys.stdout = log_capture  \n",
    "\n",
    "        try:\n",
    "            exec(code, namespace)\n",
    "        except Exception as e:\n",
    "            log_output.append(f\"Error: {e}\\n\")\n",
    "            print(f\"Error executing code block {i}: {e}\")\n",
    "        finally:\n",
    "            sys.stdout = original_stdout \n",
    "\n",
    "     \n",
    "        log_text = ''.join(log_output)\n",
    "\n",
    "    \n",
    "        optimal_value = extract_objective(log_text)\n",
    "        if optimal_value is not None:\n",
    "            obj.append(optimal_value)\n",
    "            print(f\"Optimal Value: {optimal_value}\")\n",
    "        else:\n",
    "\n",
    "            obj.append(\"No optimal value found.\")\n",
    "            print(\"No optimal value found.\")\n",
    "        i += 1\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NP-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_Dataset/Air_NRM/query_NP_Flow.csv')\n",
    "result_df = Batch_Process_Queries(df)\n",
    "result_df.to_csv(\"example_answer_NP_Flow_test_oss_New.csv\", index=False)\n",
    "\n",
    "Code = []\n",
    "for i in range(len(result_df['Output'])):\n",
    "    text = result_df['Output'][i]\n",
    "    pattern = r'```python(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    code = matches[0]\n",
    "    if 'gurobipy' in code:\n",
    "        Code.append(code)\n",
    "    else:\n",
    "        if len(matches) > 0:\n",
    "            Code.append(matches[1])\n",
    "        else:\n",
    "            Code.append(\"No code found\")\n",
    "\n",
    "code_df = pd.DataFrame(Code, columns=['Code'])\n",
    "code_df.to_csv(\"example_code_NP_Flow_test_oss_New.csv\", index=False)\n",
    "\n",
    "obj = gain_obj(code_df)\n",
    "obj_df = pd.DataFrame({'Optimal Value': obj})\n",
    "obj_df.to_csv('OBJ_NP_oss_New.csv', index=False)\n",
    "combined_df = pd.concat([result_df, code_df,obj_df], axis=1)\n",
    "combined_df.to_csv(\"final_answer_NP_Flow_test_oss_New.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_Dataset/Air_NRM/query_CA.csv')\n",
    "result_df = Batch_Process_Queries(df)\n",
    "result_df.to_csv(\"answer_CA_test_oss_New.csv\", index=False)\n",
    "\n",
    "Code = []\n",
    "for i in range(len(result_df['Output'])):\n",
    "    text = result_df['Output'][i]\n",
    "    pattern = r'```python(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    code = matches[0]\n",
    "    if 'gurobipy' in code:\n",
    "        Code.append(code)\n",
    "    else:\n",
    "        if len(matches) > 0:\n",
    "            Code.append(matches[1])\n",
    "        else:\n",
    "            Code.append(\"No code found\")\n",
    "\n",
    "code_df = pd.DataFrame(Code, columns=['Code'])\n",
    "code_df.to_csv(\"code_CA_test_oss_New.csv\", index=False)\n",
    "\n",
    "obj = gain_obj(code_df)\n",
    "obj_df = pd.DataFrame({'Optimal Value': obj})\n",
    "obj_df.to_csv('OBJ_CA_oss_New.csv', index=False)\n",
    "combined_df = pd.concat([result_df, code_df,obj_df], axis=1)\n",
    "combined_df.to_csv(\"final_answer_CA_test_oss_New.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NP Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_Dataset/Air_NRM/query_NP_Flow.csv')\n",
    "result_df = Batch_Process_Queries(df)\n",
    "result_df.to_csv(\"answer_NP_Flow_test_GPT4.1_New_V1.csv\", index=False)\n",
    "\n",
    "Code = []\n",
    "for i in range(len(result_df['Output'])):\n",
    "    text = result_df['Output'][i]\n",
    "    pattern = r'```python(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    code = matches[0]\n",
    "    if 'gurobipy' in code:\n",
    "        Code.append(code)\n",
    "    else:\n",
    "        if len(matches) > 0:\n",
    "            Code.append(matches[1])\n",
    "        else:\n",
    "            Code.append(\"No code found\")\n",
    "\n",
    "code_df = pd.DataFrame(Code, columns=['Code'])\n",
    "code_df.to_csv(\"code_NP_Flow_test_GPT4.1_New_V1.csv\", index=False)\n",
    "\n",
    "obj = gain_obj(code_df)\n",
    "obj_df = pd.DataFrame({'Optimal Value': obj})\n",
    "obj_df.to_csv('OBJ_NP_New_4.1.csv', index=False)\n",
    "\n",
    "combined_df = pd.concat([result_df, code_df,obj_df], axis=1)\n",
    "combined_df.to_csv(\"final_answer_NP_Flow_test_GPT4.1_New_V1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTqERTx6Hs5dIuFpikiUJo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
