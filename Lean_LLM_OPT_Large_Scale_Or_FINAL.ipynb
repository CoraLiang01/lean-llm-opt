{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "import tempfile\n",
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.agents import initialize_agent, AgentType, Tool\n",
        "import re\n",
        "from datetime import datetime, time\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import openai\n",
        "import requests\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from faiss import IndexHNSWFlat \n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from typing import List, Tuple\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "user_api_key = \"YOUR_OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm1 = ChatOpenAI(\n",
        "    temperature=0.0, model_name=\"gpt-4\", openai_api_key=user_api_key\n",
        ")\n",
        "\n",
        "loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RefData.csv\", encoding=\"utf-8\")\n",
        "data = loader.load()\n",
        "documents = data\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "vectors = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "retriever = vectors.as_retriever(search_kwargs={'k': 5})\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm1,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")\n",
        "qa_tool = Tool(\n",
        "    name=\"FileQA\",\n",
        "    func=qa_chain.invoke,\n",
        "    description=(\n",
        "        \"Use this tool to answer questions about the problem type of the text. \"\n",
        "    ),\n",
        ")\n",
        "\n",
        "few_shot_examples_csv = \"\"\"\n",
        "\n",
        "Query: What is the problem type in operation of the text? Please give the answer directly. Text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1,w_2,w_3.There is an independent demand stream for each of the products. The objective of the company is to decide which demands to be fufilled over a ﬁnite sales horizon [0,10] to maximize the total expected revenue from ﬁxed initial inventories. The on-hand inventories for the three items are c_1,c_2,c_3 respectively. During the sales horizon, replenishment is not allowed and there is no any in-transit inventories. Customers who want to purchase P1,P2,P3 arrive at each period accoring to a Poisson process with a_1,a_2,a_3 the arrival rates respectively. Decision variables y_1,y_2,y_3 correspond to the number of requests that the firm plans to fulfill for product 1,2,3. These variables are all positive integers.\n",
        "\n",
        "Thought: I need to determine the problem type of the text. The Query contains descriptions like '.csv' or 'column'. I'll use the FileQA tool to retrieve the relevant information.\n",
        "\n",
        "Action: FileQA\n",
        "\n",
        "Action Input: \"What is the problem type in operation of the text? text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1, w_2, w_3. ...\"\n",
        "\n",
        "Observation: The problem type of the text is Network Revenue Management.\n",
        "\n",
        "Thought: The problem type Network Revenue Management is in the allowed list [Network Revenue Management, Resource Allocation, Transportation, Facility Location Problem, Assignment Problem]. I could get the final answer and finish.\n",
        "\n",
        "Final Answer: Network Revenue Management.\n",
        "\n",
        "---\n",
        "Query: What is the problem type in operation of the text? Please give the answer directly. Text:A supermarket needs to allocate various products, including high-demand items like the Sony Alpha Refrigerator, Sony Bravia XR, and Sony PlayStation 5, across different retail shelves. The product values and space requirements are provided in the \"Products.csv\" dataset. Additionally, the store has multiple shelves, each with a total space limit and specific space constraints for Sony and Apple products, as outlined in the \"Capacity.csv\" file. The goal is to determine the optimal number of units of each Sony product to place on each shelf to maximize total value while ensuring that the space used by Sony products on each shelf does not exceed the brand-specific limits. The decision variables x_ij represent the number of units of product i to be placed on shelf j.\n",
        "\n",
        "Thought: I need to determine the problem type of the text. The Query contains descriptions like '.csv' or 'column'. I'll use the FileQA tool to retrieve the relevant information.\n",
        "\n",
        "Action: FileQA\n",
        "\n",
        "Action Input: \"What is the problem type in operation of the text? Text:A supermarket needs to allocate various products, including high-demand items like the Sony Alpha Refrigerator, Sony Bravia XR, ....\"\n",
        "\n",
        "Observation: The problem type of the text is Inventory Management.\n",
        "\n",
        "Thought: The problem type Inventory Management is not in the allowed list [Network Revenue Management, Resource Allocation, Transportation, Facility Location Problem, Assignment Problem]. I need to review the query again and classify it to a type in the allowed list. According to the text, the problem type should be Resource Allocation. \n",
        "\n",
        "Final Answer: Resource Allocation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "few_shot_examples_without_csv = \"\"\"\n",
        "Query: A book distributor needs to shuffle a bunch of books from two warehouses (supply points: W1, W2) to libraries (demand points: L1, L2), using a pair of sorting centers (transshipment points: C1, C2). W1 has a stash of up to p_1 books per day it can send out. W2 can send out up to p_2 books daily. Library L1 needs a solid d_1 books daily. L2 requires d_2 books daily. Storage at the sorting centers has no cap. Transportation costs: From W1 to C1 is t_11 dollars, to C2 is t_12 dollars. From W2 to C1 is t_21 dollars, and to C2 it__ t_22 dollars. From the centers to the libraries: From C1 to L1, it__l cost t_31 dollars, to L2 it__ t_32 dollars. From C2 to L1, it__ t_41 dollars, to L2 it__ t_42 dollars. The strategy here is all about minimizing transportation spend while making sure those libraries get their books on time. We__l use x_11 and x_12 to track shipments from W1 to C1 and C2, and x_21 and x_22 for shipments from W2. For the books going out to the libraries, y_11 and y_12 will handle the flow from C1 to L1 and L2, and y_21 and y_22 from C2. Variables are all positive integers.\n",
        "\n",
        "Thought: I need to determine the problem type of the text. The Query doesn't contain any descriptions like '.csv' and 'column'. I'll direcrly classify the problem type as 'Others without CSV'.\n",
        "\n",
        "Final Answer: Others without CSV\n",
        "\n",
        "\"\"\"\n",
        "prefix = f\"\"\"I am a helpful assistant that can answer Querys about operation problems. My response must align with one of the following categories: Network Revenue Management, Resource Allocation, Transportation, Facility Location Problem, SBLP, Others with CSV, and Others without CSV. Firstly you need to identify whether the text contains any descriptions like '.csv' and 'column'.\n",
        "\n",
        "Always remember! If the input does not contain any description like '.csv' and 'column', and the values for all the variables are given directly, I will directly classify the problem type as 'Others without CSV'. Like the example {few_shot_examples_without_csv}. \n",
        "\n",
        "However, if the text contains descriptions like '.csv' or 'column', and the values for all the variables are not given directly, I will use the following examples {few_shot_examples_csv} as a guide. And answer the Query by given the answer directly.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "\n",
        "Begin!\n",
        "\n",
        "Query: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "classification_agent = initialize_agent(\n",
        "    tools=[qa_tool],\n",
        "    llm=llm1,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    agent_kwargs={\n",
        "        \"prefix\": prefix,\n",
        "        \"suffix\": suffix,\n",
        "    },\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,  \n",
        ")\n",
        "openai.api_request_timeout = 60  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Scale OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NRM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def retrieve_similar_docs(query,retriever):\n",
        "    \n",
        "    similar_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    results = []\n",
        "    for doc in similar_docs:\n",
        "        results.append({\n",
        "            \"content\": doc.page_content,\n",
        "            \"metadata\": doc.metadata\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def process_dataset_address(dataset_address: str) -> List[Document]:\n",
        "\n",
        "    documents = []\n",
        "    file_addresses = dataset_address.strip().split('\\n')  \n",
        "    for file_idx, file_address in enumerate(file_addresses, start=1):\n",
        "        try:\n",
        "            df = pd.read_csv(file_address.strip())  \n",
        "            file_name = file_address.strip().split('/')[-1]  \n",
        "            for row_idx, row in df.iterrows():\n",
        "                page_content = \", \".join([f\"{col} = {row[col]}\" for col in df.columns])\n",
        "                documents.append(Document(page_content=page_content))\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_address}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return documents\n",
        "\n",
        "def get_NRM_response(query,dataset_address):\n",
        "    retrieve='product'\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_NRM2_MD.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "    documents = data\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 1})\n",
        "    few_shot_examples = []\n",
        "\n",
        "    similar_results = retrieve_similar_docs(query,retriever)\n",
        "\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip()  \n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip()  \n",
        "        Related = split_at_label[1].strip()\n",
        "        information = pd.read_csv(data_address)\n",
        "        information_head = information[:36]\n",
        "\n",
        "        example_data_description = \"\\nHere is the product data:\\n\"\n",
        "        for i, r in information_head.iterrows():\n",
        "            example_data_description += f\"Product {i + 1}: {r['Product Name']}, revenue w_{i + 1} = {r['Revenue']}, demand rate a_{i + 1} = {r['Demand']}, initial inventory c_{i + 1} = {r['Initial Inventory']}\\n\"\n",
        "\n",
        "\n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        few_shot_examples.append(f\"\"\"\n",
        "\n",
        "Question: Based on the following problem description and data, please formulate a complete mathematical model using real data from retrieval. {problem_description}\n",
        "\n",
        "Thought: I need to formulate the objective function and constraints of the linear programming model based on the user's description and the provided data. I should retrieve the relevant information from the CSV file. Pay attention: 1. If the data to be retrieved is not specified, retrieve the whole dataset instead. 2. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. 3. The final expressions should not be simplified or abbreviated.\n",
        "\n",
        "Action: CSVQA\n",
        "\n",
        "Action Input: Retrieve all the {retrieve} data {Related} to formulate the mathematical model with no simplification or abbreviation. Retrieve the documents in order, row by row. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead. Only present final answer in details of row, instead of giving a sheet format.\n",
        "\n",
        "Observation: {example_data_description}\n",
        "\n",
        "Thought: Now that I have the necessary data, construct the objective function and constraints using the retrieved data as parameters of the formula. Ensure to include any additional detailed constraints present in the problem description. Always pay attention to the variable type. If not mentioned, use nonnegative integer. Do NOT include any explanations, notes, or extra text. Format the expressions strictly in markdown ONLY in this exact format: {label}. Following this example. The expressions should not be simplified or abbreviated. Besides, I need to use the $$ or $ to wrap the mathematical expressions instead of \\[, \\], \\( or \\). I also should avoid using align, align* and other latex environments. Besides, I should also avoid using \\begin, \\end, \\text.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "\n",
        "    data = []\n",
        "    dfs=[]\n",
        "\n",
        "    file_addresses = dataset_address.strip().split('\\n')\n",
        "    for file_address in file_addresses:\n",
        "        try:\n",
        "            df = pd.read_csv(file_address)  \n",
        "            file_name = file_address.split('/')[-1] \n",
        "            dfs.append((file_name, df))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_address}: {e}\")\n",
        "\n",
        "    for df_index, (file_name, df) in enumerate(dfs):\n",
        "        data.append(f\"\\nDataFrame {df_index + 1} - {file_name}:\\n\")\n",
        "\n",
        "        for i, r in df.iterrows():\n",
        "            description = \"\"\n",
        "            description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "            data.append(description + \"\\n\")\n",
        "    document=data\n",
        "   \n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_texts(document, embeddings)\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 1000})\n",
        "    llm2 = ChatOpenAI(temperature=0.0, model_name='gpt-4.1', top_p=1,n = 1, openai_api_key=user_api_key)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Retrieve the documents in order, row by row. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead. Only present final answer in details of row, instead of giving a sheet format.\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
        "    qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "    def qa_wrapper(query: str):\n",
        "        return qa_chain.invoke({\"input\": query})['answer']\n",
        "    qa_tool = Tool(\n",
        "        name=\"CSVQA\",\n",
        "        func=qa_wrapper,\n",
        "        description=\"Use this tool to answer Querys based on the provided CSV data and retrieve product data similar to the input query.\"\n",
        "    )\n",
        "\n",
        "    prefix = f\"\"\"You are an assistant that generates a mathematical models based on the user's description and provided CSV data.\n",
        "\n",
        "    Please refer to the following example and generate the answer in the same format:\n",
        "\n",
        "    {few_shot_examples}\n",
        "\n",
        "    When you need to retrieve information from the CSV file, use the provided tool.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    User Description: {input}\n",
        "    {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent2 = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm2,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True,\n",
        "    )\n",
        "\n",
        "    result = agent2.invoke(query)\n",
        "\n",
        "    return result['output']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_RA_response(query,dataset_address):\n",
        "\n",
        "    retrieve=\"product\"\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_RA2_MD.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "    documents = data\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 3})\n",
        "    few_shot_examples = []\n",
        "    similar_results =  retrieve_similar_docs(query,retriever)\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip() \n",
        "\n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip()  \n",
        "        Related = split_at_label[1].strip()\n",
        "\n",
        "        datas=data_address.split()\n",
        "        information = []\n",
        "\n",
        "        for data in datas:\n",
        "            information.append(pd.read_csv(data))\n",
        "        example_data_description = \"\\nHere is the data:\\n\"\n",
        "        for df_index, df in enumerate(information):\n",
        "            if df_index == 0:\n",
        "                example_data_description += f\"\\nDataFrame {df_index + 1} - Capacity\\n\"\n",
        "            elif df_index == 1:\n",
        "                example_data_description += f\"\\nDataFrame {df_index + 1} - Products\\n\"\n",
        "\n",
        "            for z, r in df.iterrows():\n",
        "                description = \"\"\n",
        "                description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "                example_data_description += description + \"\\n\"\n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        few_shot_examples.append( f\"\"\"\n",
        "Question: Based on the following problem description and data, please formulate a complete mathematical model using real data from retrieval. {problem_description}\n",
        "\n",
        "Thought: I need to formulate the objective function and constraints of the linear programming model based on the user's description and the provided data. I should retrieve the relevant information from the CSV file. Pay attention: 1. If the data to be retrieved is not specified, retrieve the whole dataset instead. 2. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. 3. The final expressions should not be simplified or abbreviated.\n",
        "\n",
        "Action: CSVQA\n",
        "\n",
        "Action Input: Retrieve all the {retrieve} data {Related} to formulate the mathematical model with no simplification or abbreviation. Retrieve the documents in order from top to bottom. Use the retrieved context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead. Only present final answer in details of row, instead of giving a sheet format.\n",
        "\n",
        "Observation: {example_data_description}\n",
        "\n",
        "Thought: Now that I have the necessary data, I would construct the objective function and constraints using the retrieved data as parameters of the formula. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula according to the retrieved 'product id'. Do NOT include any explanations, notes, or extra text. Respond ONLY in this exact format: {label}. Following this example. The expressions should not be simplified or abbreviated. Besides, I need to use the $$ or $ to wrap the mathematical expressions instead of \\[, \\], \\( or \\). I also should avoid using align, align* and other latex environments. Besides, I should also avoid using \\begin, \\end, \\text.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "\n",
        "    data = []\n",
        "    dfs=[]\n",
        "\n",
        "    file_addresses = dataset_address.strip().split('\\n')\n",
        "    for file_address in file_addresses:\n",
        "        try:\n",
        "            df = pd.read_csv(file_address) \n",
        "            file_name = file_address.split('/')[-1]  \n",
        "            dfs.append((file_name, df))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_address}: {e}\")\n",
        "\n",
        "    for df_index, (file_name, df) in enumerate(dfs):\n",
        "        data.append(f\"\\nDataFrame {df_index + 1} - {file_name}:\\n\")\n",
        "\n",
        "        if file_name=='products.csv' or file_name=='Products.csv':\n",
        "            for i, r in df.iterrows():\n",
        "                description = f\"Product id: {i+1}; \"\n",
        "                description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "                data.append(description + \"\\n\")\n",
        "        else:\n",
        "            for i, r in df.iterrows():\n",
        "                description = f\"\"\n",
        "                description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "                data.append(description + \"\\n\")\n",
        "\n",
        "    \n",
        "    documents = [content for content in data]\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_texts(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 220})\n",
        "    llm2 = ChatOpenAI(temperature=0.0, model_name='gpt-4.1',top_p=1,n = 1, openai_api_key=user_api_key)\n",
        "\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Retrieve the documents in order from top to bottom. Use the retrieved context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead.\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
        "    qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "    def qa_wrapper(query: str):\n",
        "        return qa_chain.invoke({\"input\": query})['answer']\n",
        "    qa_tool = Tool(\n",
        "        name=\"CSVQA\",\n",
        "        func=qa_wrapper,\n",
        "        description=\"Use this tool to answer Querys based on the provided CSV data and retrieve product data similar to the input query.\"\n",
        "    )\n",
        "\n",
        "    prefix = f\"\"\"You are an assistant that generates a mathematical models based on the user's description and provided CSV data.\n",
        "\n",
        "    Please refer to the following example and generate the answer in the same format:\n",
        "\n",
        "    {few_shot_examples}\n",
        "\n",
        "    When you need to retrieve information from the CSV file, use the provided tool.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    User Description: {input}\n",
        "    {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent2 = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm2,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True,\n",
        "    )\n",
        "\n",
        "    result = agent2.invoke(query)\n",
        "\n",
        "    return result['output']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_TP_response(query,dataset_address):\n",
        "    retrieve=\"capacity data and products data, \"\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_TP2_MD.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "    documents = data\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 3})\n",
        "    few_shot_examples = []\n",
        "    similar_results = retrieve_similar_docs(query,retriever)\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip()\n",
        "\n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip()  \n",
        "        Related = split_at_label[1].strip()\n",
        "\n",
        "        datas=data_address.split()\n",
        "        information = []\n",
        "\n",
        "        for data in datas:\n",
        "            information.append(pd.read_csv(data))\n",
        "        example_data_description = \"\\nHere is the data:\\n\"\n",
        "        for df_index, df in enumerate(information):\n",
        "            if df_index == 0:\n",
        "                example_data_description += f\"\\nDataFrame {df_index + 1} - Customer Demand\\n\"\n",
        "            elif df_index == 1:\n",
        "                example_data_description += f\"\\nDataFrame {df_index + 1} - Supply Capacity\\n\"\n",
        "            elif df_index == 2:\n",
        "                example_data_description += f\"\\nDataFrame {df_index + 1} - Transportation Cost\\n\"\n",
        "\n",
        "            for z, r in df.iterrows():\n",
        "                description = \"\"\n",
        "                description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "                example_data_description += description + \"\\n\"\n",
        "            retrieve += ', '.join(df.columns)+', '\n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        few_shot_examples.append( f\"\"\"\n",
        "Question: Based on the following problem description and data, please formulate a complete mathematical model using real data from retrieval. {problem_description}\n",
        "\n",
        "Thought: I need to formulate the objective function and constraints of the linear programming model based on the user's description and the provided data. I should retrieve the relevant information from the CSV file. Pay attention: 1. If the data to be retrieved is not specified, retrieve the whole dataset instead. 2. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. 3. The final expressions should not be simplified or abbreviated.\n",
        "\n",
        "Action: CSVQA\n",
        "\n",
        "Action Input: Retrieve all the {retrieve} data {Related} to formulate the mathematical model with no simplification or abbreviation. Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved. Only present final answer in details of row, instead of giving a sheet format.\n",
        "\n",
        "Observation: {example_data_description}\n",
        "\n",
        "Thought: Now that I have the necessary data, I would construct the objective function and constraints using the retrieved data as parameters of the formula. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. Do NOT include any explanations, notes, or extra text. Respond ONLY in this exact format: {label}. Following this example. The expressions should not be simplified or abbreviated. Besides, I need to use the $$ or $ to wrap the mathematical expressions instead of \\[, \\], \\( or \\). I also should avoid using align, align* and other latex environments. Besides, I should also avoid using \\begin, \\end, \\text.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "    data = []\n",
        "    dfs=[]\n",
        "\n",
        "    file_addresses = dataset_address.strip().split('\\n')\n",
        "    for file_address in file_addresses:\n",
        "        try:\n",
        "            df = pd.read_csv(file_address) \n",
        "            file_name = file_address.split('/')[-1] \n",
        "            dfs.append((file_name, df))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_address}: {e}\")\n",
        "\n",
        "    for df_index, (file_name, df) in enumerate(dfs):\n",
        "        data.append(f\"\\nDataFrame {df_index + 1} - {file_name}:\\n\")\n",
        "\n",
        "        for i, r in df.iterrows():\n",
        "            description = \"\"\n",
        "            description += \", \".join([f\"{col} = {r[col]}\" for col in df.columns])\n",
        "            data.append(description + \"\\n\")\n",
        "\n",
        "    print(data)\n",
        "\n",
        "    documents = [content for content in data]\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_texts(documents, embeddings)\n",
        "\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 300})\n",
        "\n",
        "    llm2 = ChatOpenAI(temperature=0.0, model_name='gpt-4.1', top_p=1,n = 1,openai_api_key=user_api_key)\n",
        "\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved.\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
        "    qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "    def qa_wrapper(query: str):\n",
        "        return qa_chain.invoke({\"input\": query})['answer']\n",
        "    qa_tool = Tool(\n",
        "        name=\"CSVQA\",\n",
        "        func=qa_wrapper,\n",
        "        description=\"Use this tool to answer Querys based on the provided CSV data and retrieve product data similar to the input query.\"\n",
        "    )\n",
        "\n",
        "    prefix = f\"\"\"You are an assistant that generates a mathematical models based on the user's description and provided CSV data.\n",
        "\n",
        "    Please refer to the following example and generate the answer in the same format:\n",
        "\n",
        "    {few_shot_examples}\n",
        "\n",
        "    When you need to retrieve information from the CSV file, use the provided tool.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    User Description: {input}\n",
        "    {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent2 = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm2,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True,\n",
        "    )\n",
        "\n",
        "    result = agent2.invoke(query)\n",
        "\n",
        "    return result['output']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_AP_response(query,dataset_address):\n",
        "    retrieve=''\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_AP2_MD.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "\n",
        "    documents = data\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(max_tokens_limit=400,search_kwargs={'k': 1})\n",
        "    few_shot_examples = []\n",
        "    similar_results =  retrieve_similar_docs(query,retriever)\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip() \n",
        "\n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        file_addresses = data_address.strip().split('\\n')\n",
        "        dfs = []\n",
        "        df_index = 0\n",
        "        example_data_description = \" \"\n",
        "        for file_address in file_addresses:\n",
        "            try:\n",
        "                df = pd.read_csv(file_address) \n",
        "                file_name = file_address.split('/')[-1]  \n",
        "                matrix = df.iloc[:,1:].values\n",
        "                example_data_description +=\"C=\" + np.array_str(matrix)+ \".\"\n",
        "                dfs.append((file_name, df))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_address}: {e}\")\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip() \n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        Related=''\n",
        "        few_shot_examples.append( f\"\"\"\n",
        "Question: Based on the following problem description and data, please formulate a complete mathematical model using real data from retrieval. {problem_description}\n",
        "\n",
        "Thought: I need to formulate the objective function and constraints of the linear programming model based on the user's description and the provided data. I should retrieve the relevant information from the CSV file. Pay attention: 1. If the data to be retrieved is not specified, retrieve the whole dataset instead. 2. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. 3. The final expressions should not be simplified or abbreviated.\n",
        "\n",
        "Action: CSVQA\n",
        "\n",
        "Action Input: Retrieve all the {retrieve} data {Related} to formulate the mathematical model with no simplification or abbreviation. Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved. Only present final answer in details of row, instead of giving a sheet format.\n",
        "\n",
        "Observation: {example_data_description}\n",
        "\n",
        "Thought: Now that I have the necessary data, I would construct the objective function and constraints using the retrieved data as parameters of the formula. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula according to the retrieved 'product id'. Do NOT include any explanations, notes, or extra text. Respond ONLY in this exact format: {label}. Following this example. The expressions should not be simplified or abbreviated. Besides, I need to use the $$ or $ to wrap the mathematical expressions instead of \\[, \\], \\( or \\). I also should avoid using align, align* and other latex environments. Besides, I should also avoid using \\begin, \\end, \\text.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "        \n",
        "    data = []\n",
        "    dfs=[]\n",
        "    file_addresses = dataset_address.strip().split('\\n')\n",
        "    df_index = 0\n",
        "    data_description = \" \"\n",
        "    for file_address in file_addresses:\n",
        "        try:\n",
        "            df = pd.read_csv(file_address) \n",
        "            file_name = file_address.split('/')[-1] \n",
        "            matrix = df.iloc[:,1:].values\n",
        "            data_description +=\"C=\" + np.array_str(matrix)+ \".\"\n",
        "            dfs.append((file_name, df))\n",
        "            df_index += 1\n",
        "            dfs.append((file_name, df))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_address}: {e}\")\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_texts([data_description], embeddings)\n",
        "\n",
        "    retriever = vectors.as_retriever(max_tokens_limit=400, search_kwargs={'k': 1}) \n",
        "    llm2 = ChatOpenAI(temperature=0.0, model_name='gpt-4.1', top_p=1,n = 1,openai_api_key=user_api_key)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Retrieve the documents in order from top to bottom. Use the retrieved context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, retrieve all the data instead.\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
        "    qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "    def qa_wrapper(query: str):\n",
        "        return qa_chain.invoke({\"input\": query})['answer']\n",
        "    qa_tool = Tool(\n",
        "        name=\"CSVQA\",\n",
        "        func=qa_wrapper,\n",
        "        description=\"Use this tool to answer Querys based on the provided CSV data and retrieve product data similar to the input query.\"\n",
        "    )\n",
        "\n",
        "    prefix = f\"\"\"You are an assistant that generates a mathematical model based on the user's description and provided CSV data.\n",
        "\n",
        "            Please refer to the following example and generate the answer in the same format:\n",
        "\n",
        "            {few_shot_examples}\n",
        "\n",
        "            Note: Please retrieve all neccessary information from the CSV file to generate the answer. When you generate the answer, please output required parameters in a whole text, including all vectors and matrices.\n",
        "\n",
        "            When you need to retrieve information from the CSV file, use the provided tool.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "\n",
        "            Begin!\n",
        "\n",
        "            User Description: {input}\n",
        "            {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent2 = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm2,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "\n",
        "    result = agent2.invoke(query)\n",
        "    output = result['output']\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_FLP_response(query,dataset_address):\n",
        "    retrieve='supplier'\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_FLP2_MD.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "\n",
        "    documents = data\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(max_tokens_limit=400,search_kwargs={'k': 1})\n",
        "    few_shot_examples = []\n",
        "    similar_results =  retrieve_similar_docs(query,retriever)\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip() \n",
        "\n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        file_addresses = data_address.strip().split('\\n')\n",
        "        dfs = []\n",
        "        df_index = 0\n",
        "        example_data_description = \" \"\n",
        "        for file_address in file_addresses:\n",
        "            try:\n",
        "                df = pd.read_csv(file_address) \n",
        "                file_name = file_address.split('/')[-1]  \n",
        "                if df_index == 0:\n",
        "                    result = df['demand'].values.tolist()\n",
        "                    example_data_description += \"d=\" + str(result) + \"\\n\"\n",
        "                elif df_index == 1:\n",
        "                    result = df['fixed_costs'].values.tolist()\n",
        "                    example_data_description +=\"c=\" + str(result) + \"\\n\"\n",
        "                elif df_index == 2:\n",
        "                    matrix = df.iloc[:,1:].values\n",
        "                    example_data_description +=\"A=\" + np.array_str(matrix)+ \".\"\n",
        "                df_index += 1\n",
        "                dfs.append((file_name, df))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_address}: {e}\")\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip() \n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        Related=''\n",
        "\n",
        "        few_shot_examples.append( f\"\"\"\n",
        "Question: Based on the following problem description and data, please formulate a complete mathematical model using real data from retrieval. {problem_description}\n",
        "\n",
        "Thought: I need to formulate the objective function and constraints of the linear programming model based on the user's description and the provided data. I should retrieve the relevant information from the CSV file. Pay attention: 1. If the data to be retrieved is not specified, retrieve the whole dataset instead. 2. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. 3. The final expressions should not be simplified or abbreviated.\n",
        "\n",
        "Action: CSVQA\n",
        "\n",
        "Action Input: Retrieve all the {retrieve} data {Related} to formulate the mathematical model with no simplification or abbreviation. Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved. Only present final answer in details of row, instead of giving a sheet format.\n",
        "\n",
        "Observation: {example_data_description}\n",
        "\n",
        "Thought: Now that I have the necessary data, I would construct the objective function and constraints using the retrieved data as parameters of the formula. I should pay attention if there is further detailed constraint in the problem description. If so, I should generate additional constraint formula. Do NOT include any explanations, notes, or extra text. Respond ONLY in this exact format: {label}. Following this example. The expressions should not be simplified or abbreviated. Besides, I need to use the $$ or $ to wrap the mathematical expressions instead of \\[, \\], \\( or \\). I also should avoid using align, align* and other latex environments. Besides, I should also avoid using \\begin, \\end, \\text.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "\n",
        "    data = []\n",
        "    dfs=[]\n",
        "    file_addresses = dataset_address.strip().split('\\n')\n",
        "    df_index = 0\n",
        "    data_description = \" \"\n",
        "    for file_address in file_addresses:\n",
        "        try:\n",
        "            df = pd.read_csv(file_address) \n",
        "            file_name = file_address.split('/')[-1] \n",
        "            if df_index == 0:\n",
        "                result = df['demand'].values.tolist()\n",
        "                data_description += \"d=\" + str(result) + \"\\n\"\n",
        "            elif df_index == 1:\n",
        "                result = df['fixed_costs'].values.tolist()\n",
        "                data_description +=\"c=\" + str(result) + \"\\n\"\n",
        "            elif df_index == 2:\n",
        "                matrix = df.iloc[:,1:].values\n",
        "                data_description +=\"A=\" + np.array_str(matrix)+ \"\\n\"\n",
        "            df_index += 1\n",
        "            dfs.append((file_name, df))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_address}: {e}\")\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_texts([data_description], embeddings)\n",
        "\n",
        "    retriever = vectors.as_retriever(max_tokens_limit=400, search_kwargs={'k': 1}) \n",
        "    llm2 = ChatOpenAI(temperature=0.0, model_name='gpt-4.1', top_p=1,n = 1, openai_api_key=user_api_key)\n",
        "    \n",
        "    system_prompt = (\n",
        "        \"Retrieve the documents in order. Use the given context to answer the question. If mention a certain kind of product, retrieve all the relavant product information detail judging by its product name. If not mention a certain kind of product, make sure that all the data is retrieved.\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm2, prompt)\n",
        "    qa_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "    def qa_wrapper(query: str):\n",
        "        return qa_chain.invoke({\"input\": query})['answer']\n",
        "    qa_tool = Tool(\n",
        "        name=\"CSVQA\",\n",
        "        func=qa_wrapper,\n",
        "        description=\"Use this tool to answer Querys based on the provided CSV data and retrieve product data similar to the input query.\"\n",
        "    )\n",
        "\n",
        "    prefix = f\"\"\"You are an assistant that generates a mathematical model based on the user's description and provided CSV data.\n",
        "\n",
        "            Please refer to the following example and generate the answer in the same format:\n",
        "\n",
        "            {few_shot_examples}\n",
        "\n",
        "            Note: Please retrieve all neccessary information from the CSV file to generate the answer. When you generate the answer, please output required parameters in a whole text, including all vectors and matrices.\n",
        "\n",
        "            When you need to retrieve information from the CSV file, use the provided tool.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "            Begin!\n",
        "\n",
        "            User Description: {input}\n",
        "            {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent2 = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm2,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "\n",
        "    result = agent2.invoke(query)\n",
        "    output = result['output']\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Others Without CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_others_without_CSV_response(query):\n",
        "    llm = ChatOpenAI(\n",
        "                    temperature=0.0, model_name=\"gpt-4.1\", top_p=1, n = 1, openai_api_key=user_api_key\n",
        "                )\n",
        "\n",
        "    # Load and process the data\n",
        "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_Others_Without_CSV.csv\", encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "\n",
        "    documents = data\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
        "    vectors = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectors.as_retriever(search_kwargs={'k': 5})\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "    )\n",
        "\n",
        "    qa_tool = Tool(\n",
        "        name=\"ORLM_QA\",\n",
        "        func=qa_chain.invoke,\n",
        "        description=(\n",
        "            \"Use this tool to answer Querys.\"\n",
        "            \"Provide the Query as input, and the tool will retrieve the relevant information from the file and use it to answer the Query.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    few_shot_examples = []\n",
        "    similar_results = retrieve_similar_docs(query,retriever)\n",
        "\n",
        "    for i, result in enumerate(similar_results, 1):\n",
        "        content = result['content']\n",
        "\n",
        "        split_at_formulation = content.split(\"Data_address:\", 1)\n",
        "        problem_description = split_at_formulation[0].replace(\"prompt:\", \"\").strip()\n",
        "\n",
        "        split_at_address = split_at_formulation[1].split(\"Label:\", 1)\n",
        "        data_address = split_at_address[0].strip()\n",
        "\n",
        "        split_at_label = split_at_address[1].split(\"Related:\", 1)\n",
        "        label = split_at_label[0].strip() \n",
        "\n",
        "        split_at_type = split_at_address[1].split(\"problem type:\", 1)\n",
        "        Related = split_at_type[0].strip() \n",
        "\n",
        "        selected_problem = split_at_type[1].strip()\n",
        "\n",
        "        label = label.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "        few_shot_examples.append(f\"\"\"\n",
        "\n",
        "Query: {problem_description}\n",
        "\n",
        "Thought: I need to formulate the mathematical model for this problem. I'll use the ORLM_QA tool to retrieve the most similar use case and learn the method and formulation for generating the answer (label) for user's query. Always note whether to add additional integer constraints (or real number) is decided according to the realistic significance of the problem and the characteristics of the variables.\n",
        "\n",
        "Action: ORLM_QA\n",
        "\n",
        "Action Input: {problem_description}\n",
        "\n",
        "Observation: \n",
        "\n",
        "Thought: Respond ONLY in this exact format: {label}. Do NOT include any explanations, notes, or extra text. The expressions should not be simplified or abbreviated. Just give the formula, no need to generate final answer. Add default constraints that the variables are nonnegative integers, or nonnegative real numbers if specified in the query.\n",
        "\n",
        "Final Answer: \n",
        "{label}\n",
        "\"\"\")\n",
        "\n",
        "    prefix = f\"\"\"You are a helpful assistant that can answer Querys about operation problems. \n",
        "\n",
        "    Use the following examples as a guide. Always use the ORLM_QA tool when you need to retrieve information from the file:\n",
        "\n",
        "    {few_shot_examples}\n",
        "\n",
        "    When you need to find information from the file, use the provided tools.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    suffix = \"\"\"\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Query: {input}\n",
        "    {agent_scratchpad}\"\"\"\n",
        "\n",
        "    agent = initialize_agent(\n",
        "        tools=[qa_tool],\n",
        "        llm=llm,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        agent_kwargs={\n",
        "            \"prefix\": prefix,\n",
        "            \"suffix\": suffix,\n",
        "        },\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True,  \n",
        "    )\n",
        "\n",
        "    openai.api_request_timeout = 60  \n",
        "\n",
        "    output = agent.invoke(query)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_code(output,selected_problem):\n",
        "    llm_code = ChatOpenAI(\n",
        "        temperature=0.0, model_name=\"gpt-4.1\",top_p=1,n = 1, openai_api_key=user_api_key\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in mathematical optimization and Python programming. Your task is to write Python code to solve the provided mathematical optimization model using the Gurobi library. The code should include the definition of the objective function, constraints, and decision variables. Please don't add additional explanations. Please don't include ```python and ```.Below is the provided mathematical optimization model:\n",
        "\n",
        "    Mathematical Optimization Model:\n",
        "    {output}\n",
        "    \"\"\"\n",
        "\n",
        "    if selected_problem == \"Network Revenue Management\" or selected_problem == \"NRM\" or selected_problem == \"Network Revenue Management Problem\":\n",
        "\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\max \\quad \\sum_i A_i \\cdot x_i$\n",
        "Constraints\n",
        "1. Inventory Constraints:\n",
        "$\\quad \\quad x_i \\leq I_i, \\quad \\forall i$\n",
        "2. Demand Constraints:\n",
        "$x_i \\leq d_i, \\quad \\forall i$\n",
        "3. Startup Constraint:\n",
        "$\\sum_i x_i \\geq s$\n",
        "Retrieved Information\n",
        "$\\small I = [7550, 6244]$\n",
        "$\\small A = [149, 389]$\n",
        "$\\small d = [15057, 12474]$\n",
        "$\\small s = 100$\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Product_Optimization\")\n",
        "\n",
        "# Decision variables for the number of units of each product\n",
        "x_1 = m.addVar(vtype=GRB.INTEGER, name=\"x_1\") # Number of units of product 1\n",
        "x_2 = m.addVar(vtype=GRB.INTEGER, name=\"x_2\") # Number of units of product 2\n",
        "\n",
        "# Objective function: Maximize 149 x_1 + 389 x_2\n",
        "m.setObjective(149 * x_1 + 389 * x_2, GRB.MAXIMIZE)\n",
        "\n",
        "# Constraints\n",
        "m.addConstr(x_1 <= 7550, name=\"inventory_constraint_1\")\n",
        "m.addConstr(x_2 <= 6244, name=\"inventory_constraint_2\")\n",
        "m.addConstr(x_1 <= 15057, name=\"demand_constraint_1\")\n",
        "m.addConstr(x_2 <= 12474, name=\"demand_constraint_2\")\n",
        "\n",
        "# Non-negativity constraints are implicitly handled by the integer constraints (x_1, x_2 >= 0)\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    elif selected_problem == \"Facility Location Problem\" or selected_problem == \"FLP\" or selected_problem == \"Facility Location\":\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\min \\quad \\sum_{i} \\sum_{j} A_{ij} \\cdot x_{ij} + \\sum_{i} c_i \\cdot y_i$\n",
        "\n",
        "Constraints\n",
        "1. Demand Constraint:\n",
        "$\\quad \\quad \\sum_i x_{ij} = d_j, \\quad \\forall j$\n",
        "2. Capacity Constraint:\n",
        "$\\quad \\quad \\sum_j x_{ij} \\leq M \\cdot y_i, \\quad \\forall i$\n",
        "3. Non-negativity:\n",
        "$\\quad \\quad x_{ij} \\geq 0, \\quad \\forall i,j$\n",
        "4. Binary Requirement:\n",
        "$\\quad \\quad y_i \\in \\{0,1\\}, \\quad \\forall i$\n",
        "\n",
        "Retrieved Information\n",
        "$\\small d = [1083, 776, 16214, 553, 17106, 594, 732]$\n",
        "$\\small c = [102.33, 94.92, 91.83, 98.71, 95.73, 99.96, 98.16]$\n",
        "$\\small A = \\begin{bmatrix}\n",
        "1506.22 & 70.90 & 8.44 & 260.27 & 197.47 & 71.71 & 61.19 \\\\  \n",
        "1732.65 & 1780.72 & 567.44 & 448.68 & 29.00 & 1484.91 & 963.92 \\\\  \n",
        "115.66 & 100.76 & 64.68 & 1324.53 & 64.99 & 134.88 & 2102.83 \\\\  \n",
        "1254.78 & 1115.63 & 52.31 & 1036.16 & 892.63 & 1464.04 & 1383.41 \\\\  \n",
        "42.90 & 891.01 & 1013.94 & 1128.72 & 58.91 & 42.89 & 1570.31 \\\\  \n",
        "0.70 & 139.46 & 70.03 & 79.15 & 1482.00 & 0.91 & 110.46 \\\\  \n",
        "1732.30 & 1780.44 & 486.50 & 523.74 & 522.08 & 82.48 & 826.41\n",
        "\\end{bmatrix}$\n",
        "$\\small M = \\sum_j d_j = 1083 + 776 + 16214 + 553 + 17106 + 594 + 732 = 38058 $\n",
        "\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "d = np.array([1083, 776, 16214, 553, 17106, 594, 732])\n",
        "c = np.array([102.33, 94.92, 91.83, 98.71, 95.73, 99.96, 98.16])\n",
        "A = np.array([[1506.22, 70.90, 8.44, 260.27, 197.47, 71.71, 61.19],  \n",
        "[1732.65, 1780.72, 567.44, 448.68, 29.00, 1484.91, 963.92],  \n",
        "[115.66, 100.76, 64.68, 1324.53, 64.99, 134.88, 2102.83],  \n",
        "[1254.78, 1115.63, 52.31, 1036.16, 892.63, 1464.04, 1383.41],  \n",
        "[42.90, 891.01, 1013.94, 1128.72, 58.91, 42.89, 1570.31],  \n",
        "[0.70, 139.46, 70.03, 79.15, 1482.00, 0.91, 110.46],  \n",
        "[1732.30, 1780.44, 486.50, 523.74, 522.08, 82.48, 826.41]])\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization_Model\")\n",
        "\n",
        "# Decision variables\n",
        "x = m.addVars(A.shape[0], A.shape[1], lb=0, name=\"x\")\n",
        "y = m.addVars(A.shape[0], vtype=GRB.BINARY, name=\"y\")\n",
        "\n",
        "# Objective function\n",
        "m.setObjective(gp.quicksum(A[i, j]*x[i, j] for i in range(A.shape[0]) for j in range(A.shape[1])) + gp.quicksum(c[i]*y[i] for i in range(A.shape[0])), GRB.MINIMIZE)\n",
        "\n",
        "# Constraints\n",
        "for j in range(A.shape[1]):\n",
        "    m.addConstr(gp.quicksum(x[i, j] for i in range(A.shape[0])) == d[j], name=f\"demand_constraint_{j}\")\n",
        "\n",
        "M = 1000000  # large number\n",
        "for i in range(A.shape[0]):\n",
        "    m.addConstr(-M*y[i] + gp.quicksum(x[i, j] for j in range(A.shape[1])) <= 0, name=f\"M_constraint_{i}\")\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "        \"\"\"\n",
        "\n",
        "    elif selected_problem == \"Assignment Problem\" or selected_problem == \"AP\" or selected_problem == \"Assignment\":\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\min \\quad \\sum_{i=1}^3 \\sum_{j=1}^3 c_{ij} \\cdot x_{ij}$\n",
        "\n",
        "Constraints\n",
        "1. Row Assignment Constraint:\n",
        "$\\quad \\quad \\sum_{j=1}^3 x_{ij} = 1, \\quad \\forall i \\in \\{1,2,3\\}$\n",
        "2. Column Assignment Constraint:\n",
        "$\\quad \\quad \\sum_{i=1}^3 x_{ij} = 1, \\quad \\forall j \\in \\{1,2,3\\}$\n",
        "3. Binary Constraint:\n",
        "$\\quad \\quad x_{ij} \\in \\{0,1\\}, \\quad \\forall i,j$\n",
        "\n",
        "Retrieved Information\n",
        "$\\small c = \\begin{bmatrix}\n",
        "3000 & 3200 & 3100 \\\\\n",
        "2800 & 3300 & 2900 \\\\\n",
        "2900 & 3100 & 3000 \n",
        "\\end{bmatrix}$\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "c = np.array([\n",
        "    [3000, 3200, 3100],\n",
        "    [2800, 3300, 2900],\n",
        "    [2900, 3100, 3000]\n",
        "])\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization_Model\")\n",
        "\n",
        "# Decision variables\n",
        "x = m.addVars(c.shape[0], c.shape[1], vtype=GRB.BINARY, name=\"x\")\n",
        "\n",
        "# Objective function\n",
        "m.setObjective(gp.quicksum(c[i, j]*x[i, j] for i in range(c.shape[0]) for j in range(c.shape[1])), GRB.MINIMIZE)\n",
        "\n",
        "# Constraints\n",
        "for i in range(c.shape[0]):\n",
        "    m.addConstr(gp.quicksum(x[i, j] for j in range(c.shape[1])) == 1, name=f\"row_constraint_{i}\")\n",
        "\n",
        "for j in range(c.shape[1]):\n",
        "    m.addConstr(gp.quicksum(x[i, j] for i in range(c.shape[0])) == 1, name=f\"col_constraint_{j}\")\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "\"\"\"\n",
        "\n",
        "    \n",
        "    elif selected_problem == \"Transportation Problem\" or selected_problem == \"TP\" or selected_problem == \"Transportation\":\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\min \\quad \\sum_i \\sum_j c_{ij} \\cdot x_{ij}$\n",
        "\n",
        "Constraints\n",
        "1. Demand Constraint:\n",
        "$\\quad \\quad \\sum_i x_{ij} \\geq d_j, \\quad \\forall j$\n",
        "2. Capacity Constraint:\n",
        "$\\quad \\quad \\sum_j x_{ij} \\leq s_i, \\quad \\forall i$\n",
        "\n",
        "Retrieved Information\n",
        "$\\small d = [94, 39, 65, 435]$\n",
        "$\\small s = [2531, 20, 210, 241]$\n",
        "$\\small c = \\begin{bmatrix}\n",
        "883.91 & 0.04 & 0.03 & 44.45 \\\\\n",
        "543.75 & 23.68 & 23.67 & 447.75 \\\\\n",
        "537.34 & 23.76 & 498.95 & 440.60 \\\\\n",
        "1791.49 & 68.21 & 1432.48 & 1527.76\n",
        "\\end{bmatrix}$\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization\")\n",
        "\n",
        "# Decision variables\n",
        "x_S1_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C1\")\n",
        "x_S1_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C2\")\n",
        "x_S1_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C3\")\n",
        "x_S1_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S1_C4\")\n",
        "x_S2_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C1\")\n",
        "x_S2_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C2\")\n",
        "x_S2_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C3\")\n",
        "x_S2_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S2_C4\")\n",
        "x_S3_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C1\")\n",
        "x_S3_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C2\")\n",
        "x_S3_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C3\")\n",
        "x_S3_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S3_C4\")\n",
        "x_S4_C1 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C1\")\n",
        "x_S4_C2 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C2\")\n",
        "x_S4_C3 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C3\")\n",
        "x_S4_C4 = m.addVar(vtype=GRB.INTEGER, name=\"x_S4_C4\")\n",
        "\n",
        "# Objective function\n",
        "m.setObjective(883.91 * x_S2_C1 + 0.04 * x_S2_C2 + 0.03 * x_S2_C3 + 44.45 * x_S2_C4 + 543.75 * x_S1_C1 + 23.68 * x_S1_C2 + 23.67 * x_S1_C3 + 447.75 * x_S1_C4 + 537.34 * x_S3_C1 + 23.76 * x_S3_C2 + 498.95 * x_S3_C3 + 440.60 * x_S3_C4 + 1791.49 * x_S4_C1 + 68.21 * x_S4_C2 + 1432.48 * x_S4_C3 + 1527.76 * x_S4_C4, GRB.MINIMIZE)\n",
        "\n",
        "# Constraints\n",
        "m.addConstr(x_S1_C1 + x_S2_C1 + x_S3_C1 + x_S4_C1 >= 94, name=\"demand_constraint1\")\n",
        "m.addConstr(x_S1_C2 + x_S2_C2 + x_S3_C2 + x_S4_C2 >= 39, name=\"demand_constraint2\")\n",
        "m.addConstr(x_S1_C3 + x_S2_C3 + x_S3_C3 + x_S4_C3 >= 65, name=\"demand_constraint3\")\n",
        "m.addConstr(x_S1_C4 + x_S2_C4 + x_S3_C4 + x_S4_C4 >= 435, name=\"demand_constraint4\")\n",
        "m.addConstr(x_S1_C1 + x_S1_C2 + x_S1_C3 + x_S1_C4 <= 2531, name=\"capacity_constraint1\")\n",
        "m.addConstr(x_S2_C1 + x_S2_C2 + x_S2_C3 + x_S2_C4 <= 20, name=\"capacity_constraint2\")\n",
        "m.addConstr(x_S3_C1 + x_S3_C2 + x_S3_C3 + x_S3_C4 <= 210, name=\"capacity_constraint3\")\n",
        "m.addConstr(x_S4_C1 + x_S4_C2 + x_S4_C3 + x_S4_C4 <= 241, name=\"capacity_constraint4\")\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "        \"\"\"\n",
        "    \n",
        "    elif selected_problem == \"Resource Allocation\" or selected_problem == \"RA\" or selected_problem == \"Resource Allocation Problem\":\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Always remember: If not specified. All the variables are non-negative interger.\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\max \\quad \\sum_i \\sum_j p_i \\cdot x_{ij}$\n",
        "\n",
        "Constraints\n",
        "1. Capacity Constraint:\n",
        "$\\quad \\quad \\sum_i a_i \\cdot x_{ij} \\leq c_j, \\quad \\forall j$\n",
        "2. Non-negativity Constraint:\n",
        "$\\quad \\quad x_{ij} \\geq 0, \\quad \\forall i,j$\n",
        "\n",
        "Retrieved Information\n",
        "$\\small p = [321, 309, 767, 300, 763, 318, 871, 522, 300, 275, 858, 593, 126, 460, 685, 443, 700, 522, 940, 598]$\n",
        "$\\small a = [495, 123, 165, 483, 472, 258, 425, 368, 105, 305, 482, 387, 469, 341, 318, 104, 377, 213, 56, 131]$\n",
        "$\\small c = [4466]$\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization_Model\")\n",
        "\n",
        "# Decision variables\n",
        "x = m.addVars(20, vtype=GRB.INTEGER, name=\"x\")\n",
        "\n",
        "# Objective function\n",
        "m.setObjective(sum(x[i]*c[i] for i in range(20)), GRB.MAXIMIZE)\n",
        "\n",
        "# Constraints\n",
        "m.addConstr(sum(x[i]*w[i] for i in range(20)) <= 4466, name=\"capacity_constraint\")\n",
        "\n",
        "# Coefficients for the objective function\n",
        "c = [321, 309, 767, 300, 763, 318, 871, 522, 300, 275, 858, 593, 126, 460, 685, 443, 700, 522, 940, 598]\n",
        "\n",
        "# Coefficients for the capacity constraint\n",
        "w = [495, 123, 165, 483, 472, 258, 425, 368, 105, 305, 482, 387, 469, 341, 318, 104, 377, 213, 56, 131]\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "```\n",
        "\n",
        "-----\n",
        "Here is another simple instance for reference:\n",
        "\n",
        "Objective Function:\n",
        "$\\quad \\quad \\max \\quad \\sum_i p_i \\cdot x_i$\n",
        "\n",
        "Constraints\n",
        "1. Capacity Constraint:\n",
        "$\\quad \\quad \\sum_i a_i \\cdot x_i \\leq 180$\n",
        "2. Dependency Constraint:\n",
        "$\\quad \\quad x_1 \\leq x_3$\n",
        "3. Non-negativity Constraint:\n",
        "$\\quad \\quad x_i \\geq 0, \\quad \\forall i$\n",
        "\n",
        "Retrieved Information\n",
        "$\\small p = [888, 134, 129, 370, 921, 765, 154, 837, 584, 365]$\n",
        "$\\small a = [4, 2, 4, 3, 2, 1, 2, 1, 3, 3]$\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization_Model\")\n",
        "\n",
        "# Decision variables\n",
        "x = m.addVars(10, vtype=GRB.INTEGER, name=\"x\")\n",
        "\n",
        "# Objective function\n",
        "p = [888, 134, 129, 370, 921, 765, 154, 837, 584, 365]\n",
        "m.setObjective(sum(x[i]*p[i] for i in range(10)), GRB.MAXIMIZE)\n",
        "\n",
        "# Constraints\n",
        "a = [4, 2, 4, 3, 2, 1, 2, 1, 3, 3]\n",
        "m.addConstr(sum(x[i]*a[i] for i in range(10)) <= 180, name=\"capacity_constraint\")\n",
        "m.addConstr(x[0] <= x[2], name=\"dependency_constraint\")\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    else:\n",
        "        prompt += \"\"\"\n",
        "For example, here is a simple instance for reference:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "Maximize 5x_S + 8x_F\n",
        "Subject to\n",
        "    2x_S + 5x_F <= 200\n",
        "    x_S <= 0.3(x_S + x_F)\n",
        "    x_F >= 10\n",
        "    x_S, x_F _ Z+\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Worker_Optimization\")\n",
        "\n",
        "# Decision variables for the number of seasonal (x_S) and full-time (x_F) workers\n",
        "x_S = m.addVar(vtype=GRB.INTEGER, lb=0, name=\"x_S\")  # Number of seasonal workers\n",
        "x_F = m.addVar(vtype=GRB.INTEGER, lb=0, name=\"x_F\")  # Number of full-time workers\n",
        "\n",
        "# Objective function: Maximize Z = 5x_S + 8x_F\n",
        "m.setObjective(5 * x_S + 8 * x_F, GRB.MAXIMIZE)\n",
        "\n",
        "# Constraints\n",
        "m.addConstr(2 * x_S + 5 * x_F <= 200, name=\"resource_constraint\")\n",
        "m.addConstr(x_S <= 0.3 * (x_S + x_F), name=\"seasonal_ratio_constraint\")\n",
        "m.addConstr(x_F >= 10, name=\"full_time_minimum_constraint\")\n",
        "\n",
        "# Non-negativity constraints are implicitly handled by the integer constraints (x_S, x_F >= 0)\n",
        "\n",
        "# Solve the model\n",
        "m.optimize()\n",
        "```\n",
        "The another example is:\n",
        "\n",
        "Mathematical Optimization Model:\n",
        "Minimize 919x_11 + 556x_12 + 951x_13 + 21x_21 + 640x_22 + 409x_23 + 59x_31 + 786x_32 + 304x_33\n",
        "Subject to\n",
        "    x_11 + x_12 + x_13 = 1\n",
        "    x_21 + x_22 + x_23 = 1\n",
        "    x_31 + x_32 + x_33 = 1\n",
        "    x_11 + x_21 + x_31 = 1\n",
        "    x_12 + x_22 + x_32 = 1\n",
        "    x_13 + x_23 + x_33 = 1\n",
        "    x_11, x_12, x_13, x_21, x_22, x_23, x_31, x_32, x_33 ∈ {{0,1}}\n",
        "\n",
        "\n",
        "The corresponding Python code for this instance is as follows:\n",
        "\n",
        "```python\n",
        "\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "c = np.array([\n",
        "    [919, 556, 951],\n",
        "    [21, 640, 409],\n",
        "    [59, 786, 304]\n",
        "])\n",
        "\n",
        "# Create the model\n",
        "m = gp.Model(\"Optimization_Model\")\n",
        "\n",
        "# Decision variables\n",
        "x = m.addVars(c.shape[0], c.shape[1], vtype=GRB.BINARY, name=\"x\")\n",
        "\n",
        "# Objective function\n",
        "m.setObjective(gp.quicksum(c[i, j]*x[i, j] for i in range(c.shape[0]) for j in range(c.shape[1])), GRB.MINIMIZE)\n",
        "\n",
        "# Constraints\n",
        "for i in range(c.shape[0]):\n",
        "    m.addConstr(gp.quicksum(x[i, j] for j in range(c.shape[1])) == 1, name=f\"row_constraint_{i}\")\n",
        "\n",
        "for j in range(c.shape[1]):\n",
        "    m.addConstr(gp.quicksum(x[i, j] for i in range(c.shape[0])) == 1, name=f\"col_constraint_{j}\")\n",
        "\n",
        "# Solve the model\n",
        "m.optimize() \n",
        "```\n",
        "\"\"\"\n",
        "    messages = [\n",
        "        HumanMessage(content=prompt) \n",
        "    ]\n",
        "\n",
        "    response = llm_code(messages)\n",
        "\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_test(test, agent):\n",
        "    output_model = []\n",
        "    output_code = []\n",
        "    for index, row in test.iterrows():\n",
        "        try:\n",
        "            query = row['Query']\n",
        "            response = agent.invoke(f\"What is the problem type of the text? text:{query}\")\n",
        "            \n",
        "            def extract_problem_type(output_text):\n",
        "                pattern = r'(Network Revenue Management|Network Revenue Management Problem|Resource Allocation|Resource Allocation Problem|Transportation|Transportation Problem|Facility Location Problem|Assignment Problem|AP|Uncapacited Facility Location Problem|NRM|RA|TP|FLP|UFLP|Others without CSV|Sales-Based Linear Programming|SBLP|Others with CSV)'\n",
        "                match = re.search(pattern, output_text, re.IGNORECASE)\n",
        "                return match.group(0) if match else None\n",
        "            \n",
        "            def csv_detect(query):\n",
        "                if re.search(r'multicolumn|Each column in the table represents|Table.csv', query, re.IGNORECASE):\n",
        "                    return 0  \n",
        "                elif re.search(r'column|csv', query, re.IGNORECASE):\n",
        "                    return 1  \n",
        "                else:\n",
        "                    return 0\n",
        "    \n",
        "            selected_problem = extract_problem_type(response['output'])\n",
        "            \n",
        "            if csv_detect(query):\n",
        "                dataset_address = row['Dataset_address']\n",
        "                if selected_problem == \"Network Revenue Management\" or selected_problem == \"NRM\" or selected_problem == \"Network Revenue Management Problem\":\n",
        "                    print(\"----------Network Revenue Management-----------\")\n",
        "                    output = get_NRM_response(query,dataset_address)\n",
        "                    output_model.append(output)\n",
        "                    code_response = get_code(output,selected_problem)\n",
        "                    output_code.append(code_response)\n",
        "    \n",
        "                elif selected_problem == \"Resource Allocation\" or selected_problem == \"RA\" or selected_problem == \"Resource Allocation Problem\":\n",
        "                    print(\"----------Resource Allocation-----------\")\n",
        "                    output = get_RA_response(query,dataset_address)\n",
        "                    output_model.append(output)\n",
        "                    code_response = get_code(output,selected_problem)\n",
        "                    output_code.append(code_response)\n",
        "    \n",
        "                elif selected_problem == \"Transportation\" or selected_problem == \"TP\" or selected_problem == \"Transportation Problem\":\n",
        "                    print(\"----------Transportation-----------\")\n",
        "                    output = get_TP_response(query,dataset_address)\n",
        "                    output_model.append(output)\n",
        "                    code_response = get_code(output,selected_problem)\n",
        "                    output_code.append(code_response)    \n",
        "    \n",
        "                elif selected_problem == \"Facility Location Problem\" or selected_problem == \"FLP\" or selected_problem == \"Uncapacited Facility Location\" or selected_problem == \"UFLP\":\n",
        "                    print(\"----------Facility Location Problem-----------\")\n",
        "                    output = get_FLP_response(query,dataset_address)\n",
        "                    output_model.append(output)\n",
        "                    code_response = get_code(output,selected_problem)\n",
        "                    output_code.append(code_response)\n",
        "                \n",
        "                elif selected_problem == \"Assignment Problem\" or selected_problem == \"AP\":\n",
        "                    print(\"----------Assignment Problem-----------\")\n",
        "                    output = get_AP_response(query,dataset_address)\n",
        "                    output_model.append(output)\n",
        "                    code_response = get_code(output,selected_problem)\n",
        "                    output_code.append(code_response)\n",
        "    \n",
        "            else:\n",
        "                print(\"----------Others without CSV-----------\")\n",
        "                output = get_others_without_CSV_response(query)\n",
        "                output_model.append(output)\n",
        "                code_response = get_code(output,selected_problem)\n",
        "                output_code.append(code_response)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Connection error: {e}\")\n",
        "            continue\n",
        "        time.sleep(15)\n",
        "    return output_model, output_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_and_combine_csvs(file_order):\n",
        "    dfs = []\n",
        "    for fname in file_order:\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            dfs.append(df)\n",
        "            print(f\"Read file: {fname} (Row length: {len(df)})\")\n",
        "        else:\n",
        "            print(f\"File doesn't exist: {fname}, already skipped\")\n",
        "    \n",
        "    if not dfs:\n",
        "        raise ValueError(\"No effective files\")\n",
        "    \n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def run_gurobi_code(code_str):\n",
        " \n",
        "    try:\n",
        "      \n",
        "        with StringIO() as buf, contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
        "            env = {\n",
        "                '__builtins__': __builtins__,\n",
        "                'gp': gp,\n",
        "                'GRB': GRB\n",
        "            }\n",
        "            \n",
        "           \n",
        "            code_str += \"\\n\\n# Added by executor\\n\"\n",
        "            code_str += \"if hasattr(m, 'status') and m.status == GRB.OPTIMAL:\\n\"\n",
        "            code_str += \"    __result__ = m.ObjVal\\n\"\n",
        "            code_str += \"else:\\n\"\n",
        "            code_str += \"    __result__ = None\\n\"\n",
        "            \n",
        "            \n",
        "            exec(code_str, env)\n",
        "            result = env.get('__result__', None)\n",
        "            \n",
        "     \n",
        "            if 'm' in env:\n",
        "                env['m'].dispose()\n",
        "                del env['m']\n",
        "            \n",
        "            return result\n",
        "    except Exception as e:\n",
        "        print(f\"Execution error: {str(e)}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Large Scale OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test NRM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "testnrm = pd.read_csv('Test_Dataset/Large-scale-or/final_large_scale_OR_New.csv')\n",
        "testnrm_1 = testnrm[9:18]\n",
        "testnrm_2 = testnrm[18:26]\n",
        "testnrm_3 = testnrm[26:34]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output_model_nrm1, output_code_nrm1 = run_test(testnrm_1,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testnrm_1['Query'], 'model_output':output_model_nrm1, 'code_output':output_code_nrm1})\n",
        "output_df.to_csv(\"nrm1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "output_model_nrm2, output_code_nrm2 = run_test(testnrm_2,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testnrm_2['Query'], 'model_output':output_model_nrm2, 'code_output':output_code_nrm2})\n",
        "output_df.to_csv(\"nrm2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "output_model_nrm3, output_code_nrm3 = run_test(testnrm_3,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testnrm_3['Query'], 'model_output':output_model_nrm3, 'code_output':output_code_nrm3})\n",
        "output_df.to_csv(\"nrm3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"nrm1.csv\",\n",
        "    \"nrm2.csv\",\n",
        "    \"nrm3.csv\"\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"nrm_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test RA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "testra = pd.read_csv('Test_Dataset/Large-scale-or/final_large_scale_OR_New.csv')\n",
        "testra_1 = testra[34:42]\n",
        "testra_2 = testra[42:50]\n",
        "testra_3 = testra[50:57]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_ra1, output_code_ra1 = run_test(testra_1,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testra_1['Query'], 'model_output':output_model_ra1, 'code_output':output_code_ra1})\n",
        "output_df.to_csv(\"ra1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output_model_ra2, output_code_ra2 = run_test(testra_2,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testra_2['Query'], 'model_output':output_model_ra2, 'code_output':output_code_ra2})\n",
        "output_df.to_csv(\"ra2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output_model_ra3, output_code_ra3 = run_test(testra_3,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testra_3['Query'], 'model_output':output_model_ra3, 'code_output':output_code_ra3})\n",
        "output_df.to_csv(\"ra3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"ra1.csv\",\n",
        "    \"ra2.csv\",\n",
        "    \"ra3.csv\"\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"ra_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test TP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "testtp = pd.read_csv('Test_Dataset/Large-scale-or/final_large_scale_OR_New.csv')\n",
        "testtp = testtp[:9]\n",
        "testtp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_tp, output_code_tp = run_test(testtp,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testtp['Query'], 'model_output':output_model_tp, 'code_output':output_code_tp})\n",
        "output_df.to_csv(\"tp.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"tp.csv\",\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"tp_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test AP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testap = pd.read_csv('Test_Dataset/Large-scale-or/final_large_scale_OR_New.csv')\n",
        "testap = testap[66:]\n",
        "testap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output_model_ap, output_code_ap = run_test(testap,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testap['Query'], 'model_output':output_model_ap, 'code_output':output_code_ap})\n",
        "output_df.to_csv(\"ap.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"ap.csv\",\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"ap_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test FLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testflp = pd.read_csv('Test_Dataset/Large-scale-or/final_large_scale_OR_New.csv')\n",
        "testflp = testflp[57:66]\n",
        "testflp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output_model_flp, output_code_flp = run_test(testflp,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': testflp['Query'], 'model_output':output_model_flp, 'code_output':output_code_flp})\n",
        "output_df.to_csv(\"flp.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"flp.csv\",\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"flp_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Small-Scale Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test NL4OPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test_nl4opt = pd.read_csv('Test_Dataset/Small-scale/NL4OPT.csv')\n",
        "test_nl4opt1=test_nl4opt[:30]\n",
        "test_nl4opt2=test_nl4opt[30:60]\n",
        "test_nl4opt3=test_nl4opt[60:90]\n",
        "test_nl4opt4=test_nl4opt[90:120]\n",
        "test_nl4opt5=test_nl4opt[120:150]\n",
        "test_nl4opt6=test_nl4opt[150:180]\n",
        "test_nl4opt7=test_nl4opt[180:210]\n",
        "test_nl4opt8=test_nl4opt[210:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt1, output_code_nl4opt1 = run_test(test_nl4opt1,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt1['Query'], 'model_output':output_model_nl4opt1, 'code_output':output_code_nl4opt1})\n",
        "output_df.to_csv(\"NL4OPT_1-30.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "output_model_nl4opt2, output_code_nl4opt2 = run_test(test_nl4opt2,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt2['Query'], 'model_output':output_model_nl4opt2, 'code_output':output_code_nl4opt2})\n",
        "output_df.to_csv(\"NL4OPT_31-60.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt3, output_code_nl4opt3 = run_test(test_nl4opt3,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt3['Query'], 'model_output':output_model_nl4opt3, 'code_output':output_code_nl4opt3})\n",
        "output_df.to_csv(\"NL4OPT_61-90.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt4, output_code_nl4opt4 = run_test(test_nl4opt4,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt4['Query'], 'model_output':output_model_nl4opt4, 'code_output':output_code_nl4opt4})\n",
        "output_df.to_csv(\"NL4OPT_91-120.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt5, output_code_nl4opt5 = run_test(test_nl4opt5,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt5['Query'], 'model_output':output_model_nl4opt5, 'code_output':output_code_nl4opt5})\n",
        "output_df.to_csv(\"NL4OPT_121-150.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt6, output_code_nl4opt6 = run_test(test_nl4opt6,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt6['Query'], 'model_output':output_model_nl4opt6, 'code_output':output_code_nl4opt6})\n",
        "output_df.to_csv(\"NL4OPT_151-180.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt7, output_code_nl4opt7 = run_test(test_nl4opt7,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt7['Query'], 'model_output':output_model_nl4opt7, 'code_output':output_code_nl4opt7})\n",
        "output_df.to_csv(\"NL4OPT_181-210.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_nl4opt8, output_code_nl4opt8 = run_test(test_nl4opt8,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_nl4opt8['Query'], 'model_output':output_model_nl4opt8, 'code_output':output_code_nl4opt8})\n",
        "output_df.to_csv(\"NL4OPT_211-245.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"NL4OPT_1-30.csv\",\n",
        "    \"NL4OPT_31-60.csv\",\n",
        "    \"NL4OPT_61-90.csv\",\n",
        "    \"NL4OPT_91-120.csv\",\n",
        "    \"NL4OPT_121-150.csv\",\n",
        "    \"NL4OPT_151-180.csv\",\n",
        "    \"NL4OPT_181-210.csv\",\n",
        "    \"NL4OPT_211-245.csv\",\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "# Add results column\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "# Step 3: Save results\n",
        "output_file = \"NL4OPT_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test IndustryOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_industryOR = pd.read_csv('Test_Dataset/Small-scale/industryOR.csv', encoding='gbk')\n",
        "test_industryOR1=test_industryOR[:25]\n",
        "test_industryOR2=test_industryOR[25:50]\n",
        "test_industryOR3=test_industryOR[50:75]\n",
        "test_industryOR4=test_industryOR[75:]\n",
        "test_industryOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_industryOR1, output_code_industryOR1 = run_test(test_industryOR1,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_industryOR1['Query'], 'model_output':output_model_industryOR1, 'code_output':output_code_industryOR1})\n",
        "output_df.to_csv(\"IndustryOR_1-25.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_industryOR2, output_code_industryOR2 = run_test(test_industryOR2,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_industryOR2['Query'], 'model_output':output_model_industryOR2, 'code_output':output_code_industryOR2})\n",
        "output_df.to_csv(\"IndustryOR_26-50.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_industryOR3, output_code_industryOR3 = run_test(test_industryOR3,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_industryOR3['Query'], 'model_output':output_model_industryOR3, 'code_output':output_code_industryOR3})\n",
        "output_df.to_csv(\"IndustryOR_51-75.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_industryOR4, output_code_industryOR4 = run_test(test_industryOR4,classification_agent)\n",
        "output_df = pd.DataFrame({'Query': test_industryOR4['Query'], 'model_output':output_model_industryOR4, 'code_output':output_code_industryOR4})\n",
        "output_df.to_csv(\"IndustryOR_76-100.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_order=[\n",
        "    \"IndustryOR_1-25.csv\",\n",
        "    \"IndustryOR_26-50.csv\",\n",
        "    \"IndustryOR_51-75.csv\",\n",
        "    \"IndustryOR_76-100.csv\",\n",
        "]\n",
        "try:\n",
        "    combined_df = read_and_combine_csvs(file_order)\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "except Exception as e:\n",
        "    print(f\"File processing failed: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "    \n",
        "print(\"\\n Running Gurobi Code...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, row in combined_df.iterrows():\n",
        "    code = row['code_output']\n",
        "    code = re.sub(r'^```python|```$', '', code, flags=re.IGNORECASE).strip()\n",
        "    \n",
        "    print(f\"Processing row {i+1}/{len(combined_df)}...\", end='\\r')\n",
        "    result = run_gurobi_code(code)\n",
        "    results.append(result)\n",
        "\n",
        "combined_df['objective_value'] = results\n",
        "print(f\"\\nCode execution completed! Time used: {time.time()-start_time:.2f} seconds\")\n",
        "print(f\"Success: {len([x for x in results if x is not None])}/{len(results)}\")\n",
        "\n",
        "output_file = \"IndustryOR_result.csv\"\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Output saved to: {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
