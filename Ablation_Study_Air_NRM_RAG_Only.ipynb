{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1lPfpKyINT7"
   },
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPITQXTZKCHU"
   },
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 37897,
     "status": "ok",
     "timestamp": 1749393236794,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "Bd-O0P3VJ97X",
    "outputId": "4a9dc141-ddec-4ccf-cbab-0d85ce12045d"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import tempfile\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime, time\n",
    "import ast\n",
    "import os\n",
    "from datetime import time as time_class\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWc5zr1lKJv1"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUV5FZV1KMpc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "user_api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your OpenAI API key\n",
    "\n",
    "def LoadFiles():\n",
    "  v1 = pd.read_csv('Test_Dataset/Air_NRM/v1.csv')\n",
    "  v2 = pd.read_csv('Test_Dataset/Air_NRM/v2.csv')\n",
    "  demand = pd.read_csv('Test_Dataset/Air_NRM/od_demand.csv')\n",
    "  flight = pd.read_csv('Test_Dataset/Air_NRM/flight.csv')\n",
    "  return v1,v2,demand,flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification_Agent(file_path=\"Large_Scale_Or_Files/RefData.csv\"):\n",
    "    # Initialize the LLM\n",
    "    llm1 = ChatOpenAI(\n",
    "        temperature=0.0, model_name=\"gpt-4.1\", openai_api_key=user_api_key\n",
    "    )\n",
    "\n",
    "    # Load and process the data\n",
    "    loader1 = CSVLoader(file_path=\"Large_Scale_Or_Files/RefData.csv\", encoding=\"utf-8\")\n",
    "    refdata = loader1.load()\n",
    "\n",
    "    # Each line is a document\n",
    "    refdocuments = refdata\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "    vectors1 = FAISS.from_documents(refdocuments, embeddings)\n",
    "\n",
    "    # Create a retriever\n",
    "    retriever1 = vectors1.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm1,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever1,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create a tool using the RetrievalQA chain\n",
    "    qa_tool1 = Tool(\n",
    "        name=\"FileQA\",\n",
    "        func=qa_chain.invoke,\n",
    "        description=(\n",
    "            \"Use this tool to answer questions about the problem type of the text. \"\n",
    "            \"Provide the question as input, and the tool will retrieve the relevant information from the file and use it to answer the question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Define few-shot examples as a string\n",
    "    few_shot_examples = \"\"\"\n",
    "\n",
    "    Question: What is the problem type in operation of the text? Please give the answer directly. Text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1,w_2,w_3.There is an independent demand stream for each of the products. The objective of the company is to decide which demands to be fufilled over a ﬁnite sales horizon [0,10] to maximize the total expected revenue from ﬁxed initial inventories. The on-hand inventories for the three items are c_1,c_2,c_3 respectively. During the sales horizon, replenishment is not allowed and there is no any in-transit inventories. Customers who want to purchase P1,P2,P3 arrive at each period accoring to a Poisson process with a_1,a_2,a_3 the arrival rates respectively. Decision variables y_1,y_2,y_3 correspond to the number of requests that the firm plans to fulfill for product 1,2,3. These variables are all positive integers.\n",
    "\n",
    "    Thought: I need to determine the problem type of the text. I'll use the FileQA tool to retrieve the relevant information.\n",
    "\n",
    "    Action: FileQA\n",
    "\n",
    "    Action Input: \"What is the problem type in operation of the text? text:There are three best-selling items (P1, P2, P3) on Amazon with the profit w_1, w_2, w_3. ...\"\n",
    "\n",
    "    Observation: The problem type of the text is Network Revenue Management.\n",
    "\n",
    "    Final Answer: Network Revenue Management.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the prefix and suffix for the agent's prompt\n",
    "    prefix = f\"\"\"You are a helpful assistant that can answer questions about operation problems.\n",
    "\n",
    "    Use the following examples as a guide. Always use the FileQA tool when you need to retrieve information from the file:\n",
    "\n",
    "\n",
    "    {few_shot_examples}\n",
    "\n",
    "    When you need to find information from the file, use the provided tools. And answer the question by given the answer directly. For example,\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    suffix = \"\"\"\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Question: {input}\n",
    "    {agent_scratchpad}\"\"\"\n",
    "\n",
    "    agent_pc = initialize_agent(\n",
    "        tools=[qa_tool1],\n",
    "        llm=llm1,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix.format(few_shot_examples=few_shot_examples),\n",
    "            \"suffix\": suffix,\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,  # Enable error handling\n",
    "    )\n",
    "\n",
    "    openai.api_request_timeout = 60 \n",
    "    return agent_pc\n",
    "\n",
    "def Problemtype(query):\n",
    "    agent_pc = Classification_Agent(file_path=\"Large_Scale_Or_Files/RefData.csv\")\n",
    "    category_original=agent_pc.invoke(f\"What is the problem type in operation of the text? text:{query}\")\n",
    "    type_output = category_original['output']\n",
    "    return type_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73aGXyz7IS2e"
   },
   "source": [
    "## Our Framework without One-shot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Im3k7d-YncG"
   },
   "outputs": [],
   "source": [
    "def New_Vectors_Flight(query):\n",
    "  v1,v2,demand,df = LoadFiles()\n",
    "  new_docs = []\n",
    "  for _, row in df.iterrows():\n",
    "      new_docs.append(Document(\n",
    "          page_content=f\"OD={row['Oneway_OD']},Departure_Time_Flight1={row['Departure Time']},Oneway_Product={row['Oneway_Product']},avg_price={row['Avg Price']}\",\n",
    "          metadata={\n",
    "              'OD': row['Oneway_OD'],\n",
    "              'time': row['Departure Time'],\n",
    "              'product': row['Oneway_Product'],\n",
    "              'avg_price': row['Avg Price']\n",
    "          }\n",
    "      ))\n",
    "\n",
    "  embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "  new_vectors = FAISS.from_documents(new_docs, embeddings)\n",
    "  return new_vectors\n",
    "\n",
    "def New_Vectors_Demand(query):\n",
    "  v1,v2,demand,df = LoadFiles()\n",
    "  new_docs = []\n",
    "  for _, row in demand.iterrows():\n",
    "      new_docs.append(Document(\n",
    "          page_content=f\"OD={row['Oneway_OD']}, avg_pax={row['Avg Pax']}\",\n",
    "          metadata={\n",
    "              'OD': row['Oneway_OD'],\n",
    "              'avg_pax': row['Avg Pax']\n",
    "          }\n",
    "      ))\n",
    "\n",
    "  embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "  new_vectors = FAISS.from_documents(new_docs, embeddings)\n",
    "  return new_vectors\n",
    "\n",
    "\n",
    "def retrieve_key_information(query):\n",
    "    flight_pattern = r\"\\(OD\\s*=\\s*\\(\\s*'(\\w+)'\\s*,\\s*'(\\w+)'\\s*\\)\\s+AND\\s+Departure\\s+Time='(\\d{1,2}:\\d{2})'\\)\"\n",
    "    matches = re.findall(flight_pattern, query)\n",
    "\n",
    "    flight_strings = [f\"(OD = ('{origin}', '{destination}') AND Departure Time='{departure_time}')\" for origin, destination, departure_time in matches]\n",
    "\n",
    "    new_query = \"Retrieve avg_price, avg_pax, value_list, ratio_list, value_0_list, ratio_0_list, and flight_capacity for the following flights:\\n\" + \", \".join(flight_strings) + \".\"\n",
    "\n",
    "    return new_query\n",
    "def retrieve_time_period(departure_time):\n",
    "    intervals = {\n",
    "        '12pm~6pm': (time_class(12, 0), time_class(18, 0)),\n",
    "        '6pm~10pm': (time_class(18, 0), time_class(22, 0)),\n",
    "        '10pm~8am': (time_class(22, 0), time_class(8, 0)),\n",
    "        '8am~12pm': (time_class(8, 0), time_class(12, 0))\n",
    "    }\n",
    "\n",
    "    if isinstance(departure_time, str):\n",
    "        try:\n",
    "            hours, minutes = map(int, departure_time.split(':'))\n",
    "            departure_time = time_class(hours, minutes)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Time format should be: 'HH:MM'\")\n",
    "\n",
    "    for interval_name, (start, end) in intervals.items():\n",
    "        if start < end:\n",
    "            if start <= departure_time < end:\n",
    "                return interval_name\n",
    "        else:\n",
    "            if departure_time >= start or departure_time < end:\n",
    "                return interval_name\n",
    "\n",
    "    return \"Unknown\"\n",
    "def retrieve_parameter(O,time_interval,product):\n",
    "    v1,v2,df,demand = LoadFiles()\n",
    "    time_interval = f'({time_interval})'\n",
    "    key = product + '*' + time_interval\n",
    "    _value_ = 0\n",
    "    _ratio_ = 0\n",
    "    no_purchase_value = 0\n",
    "    no_purchase_value_ratio = 0\n",
    "    subset = v1[v1['OD Pairs'] == O]\n",
    "    if key in subset.columns and not subset.empty:\n",
    "        _value_ = subset[key].values[0]\n",
    "\n",
    "    subset2 = v2[v2['OD Pairs'] == O]\n",
    "    if key in subset2.columns and not subset2.empty:\n",
    "        _ratio_ = subset2[key].values[0]\n",
    "\n",
    "    if 'no_purchase' in subset.columns and not subset.empty:\n",
    "        no_purchase_value = subset['no_purchase'].values[0]\n",
    "\n",
    "    if 'no_purchase' in subset2.columns and not subset2.empty:\n",
    "        no_purchase_value_ratio = subset2['no_purchase'].values[0]\n",
    "    return _value_,_ratio_,no_purchase_value,no_purchase_value_ratio\n",
    "\n",
    "def generate_coefficients(OD,time):\n",
    "    value_f_list, ratio_f_list, value_l_list, ratio_l_list = [], [], [], []\n",
    "    value_0_list, ratio_0_list = [], []\n",
    "\n",
    "    departure_time = datetime.strptime(time, '%H:%M').time()\n",
    "    time_interval = retrieve_time_period(departure_time)\n",
    "    value_1,ratio_1,value_0,ratio_0 = retrieve_parameter(OD,time_interval,'Eco_flexi')\n",
    "\n",
    "    value_2,ratio_2,value_0,ratio_0 = retrieve_parameter(OD,time_interval,'Eco_lite')\n",
    "\n",
    "    return value_1,ratio_1,value_2,ratio_2,value_0,ratio_0\n",
    "\n",
    "\n",
    "def clean_text_preserve_newlines(text):\n",
    "    cleaned = re.sub(r'\\x1b\\[[0-9;]*[mK]', '', text)\n",
    "    cleaned = re.sub(r'[^\\x20-\\x7E\\n]', '', cleaned)\n",
    "    cleaned = re.sub(r'(\\n\\s+)(\\w+\\s*=)', r'\\n\\2', cleaned)\n",
    "    cleaned = re.sub(r'\\[\\s+', '[', cleaned)\n",
    "    cleaned = re.sub(r'\\s+\\]', ']', cleaned)\n",
    "    cleaned = re.sub(r',\\s+', ', ', cleaned)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749393315733,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "iWJr_yg3ilVJ"
   },
   "outputs": [],
   "source": [
    "def csv_qa_tool_flow(query: str):\n",
    "    new_vectors = New_Vectors_Flight(query)\n",
    "    matches = re.findall(r\"\\(OD\\s*=\\s*(\\(\\s*'[^']+'\\s*,\\s*'[^']+'\\s*\\))\\s+AND\\s+Departure\\s*Time\\s*=\\s*'(\\d{1,2}:\\d{2})'\\)\", query)\n",
    "    num_match = re.search(r\"optimal (\\d+) flights\", query)\n",
    "    num_flights = int(num_match.group(1)) if num_match else None  # 3\n",
    "    capacity_match = re.search(r\"Eco_flex ticket consumes (\\d+\\.?\\d*)\\s*units\", query)\n",
    "    if matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\),\\s*'(\\d{1,2}:\\d{2})'\\)\"\n",
    "        matches_2 = re.findall(pattern, query)\n",
    "\n",
    "    if capacity_match:\n",
    "        eco_flex_capacity = capacity_match.group(1)\n",
    "    else:\n",
    "        eco_flex_capacity = 1.2\n",
    "\n",
    "    sigma_inflow_A = []\n",
    "    sigma_outflow_A = []\n",
    "    sigma_inflow_B = []\n",
    "    sigma_outflow_B = []\n",
    "    sigma_inflow_C = []\n",
    "    sigma_outflow_C = []\n",
    "\n",
    "    if matches == []:\n",
    "        matches = matches_2\n",
    "        for origin,destination,time in matches:\n",
    "            if origin == 'A':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_A.append(flight_name)\n",
    "            elif origin == 'B':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_B.append(flight_name)\n",
    "            elif origin == 'C':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_inflow_C.append(flight_name)\n",
    "            elif destination == 'A':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_A.append(flight_name)\n",
    "            elif destination == 'B':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_B.append(flight_name)\n",
    "            elif destination == 'C':\n",
    "                flight_name = f\"({origin}{destination},{time})\"\n",
    "                sigma_outflow_C.append(flight_name)\n",
    "    \n",
    "    else:\n",
    "        a_origin_flights_A_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'A'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_B_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'B'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_C_out = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[2] == 'C'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_A = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'A'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_B = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'B'\n",
    "        ]\n",
    "\n",
    "        a_origin_flights_C = [\n",
    "            (od, time)\n",
    "            for (od, time) in matches\n",
    "            if od[7] == 'C'\n",
    "        ]\n",
    "\n",
    "        for od, time in a_origin_flights_A:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_A.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_B:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_B.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_C:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_inflow_C.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_A_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_A.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_B_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_B.append(flight_name)\n",
    "\n",
    "        for od, time in a_origin_flights_C_out:\n",
    "            origin = od[2]\n",
    "            destination = od[7]\n",
    "            flight_name = f\"({origin}{destination},{time})\"\n",
    "            sigma_outflow_C.append(flight_name)\n",
    "\n",
    "    avg_price, x, x_o, ratio_0_list, ratio_list, value_list, value_0_list, avg_pax = {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    y = {}\n",
    "    N_l = {\"AB\":[],\"AC\":[],\"BA\":[],\"CA\":[]}\n",
    "\n",
    "    if matches == []:\n",
    "        matches = matches_2\n",
    "        for origin,destination,time in matches:\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            code_y = f\"({origin}{destination},{time})\"\n",
    "            y[code_y] = f\"y_{origin}{destination}_{time}\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "    else:\n",
    "        for match in matches:\n",
    "            origin = match[0][2]\n",
    "            destination = match[0][7]\n",
    "            time = match[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            code_y = f\"({origin}{destination},{time})\"\n",
    "            y[code_y] = f\"y_{origin}{destination}_{time}\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "\n",
    "\n",
    "    od_matches = re.findall(\n",
    "    r\"OD\\s*=\\s*\\(\\s*'([^']+)'\\s*,\\s*'([^']+)'\\s*\\)\", \n",
    "    query\n",
    "    )\n",
    "    if od_matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\)\"\n",
    "        matches = re.findall(pattern, query)\n",
    "        od_matches = matches\n",
    "    \n",
    "    od_matches = list(set(od_matches))\n",
    "\n",
    "    new_vectors_demand = New_Vectors_Demand(query)\n",
    "    for origin, dest in od_matches:\n",
    "        od = str((origin, dest))\n",
    "        code_o = f\"{origin}{dest}\"\n",
    "        x_o[code_o] = f\"x_{origin}{dest}_o\"\n",
    "        retriever = new_vectors_demand.as_retriever(search_kwargs={'k': 1})\n",
    "    \n",
    "        doc_1= retriever.get_relevant_documents(f\"OD={od}, avg_pax=\")\n",
    "        content = doc_1[0].page_content\n",
    "\n",
    "        pattern = r',\\s*(?=\\w+=)'\n",
    "        parts = re.split(pattern, content)\n",
    "        pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')\n",
    "            if key == 'avg_pax':\n",
    "                avg_pax[code_o] = value\n",
    "            \n",
    "    doc = f\"y = {y}\\n\"\n",
    "    doc = f\"avg_price={avg_price} \\n value_list ={value_list}\\n ratio_list={ratio_list}\\n\"\n",
    "    doc += f\"value_0_list={value_0_list}\\n ratio_0_list={ratio_0_list}\\n\"\n",
    "    doc += f\"avg_pax={avg_pax}\\n\"\n",
    "    doc += f\"capacity_consum={eco_flex_capacity}\\n\"\n",
    "    doc += f\"option_num={num_flights}\\n\"\n",
    "    doc += f\"sigma_inflow_A={sigma_inflow_A}\\n\"\n",
    "    doc += f\"sigma_outflow_A={sigma_outflow_A}\\n\"\n",
    "    doc += f\"sigma_inflow_B={sigma_inflow_B}\\n\"\n",
    "    doc += f\"sigma_outflow_B={sigma_outflow_B}\\n\"\n",
    "    doc += f\"sigma_inflow_C={sigma_inflow_C}\\n\"\n",
    "    doc += f\"sigma_outflow_C={sigma_outflow_C}\\n\"\n",
    "    doc += f\"M = 10000000\\n\"\n",
    "    doc += f\"flight_capacity=187\\n\"\n",
    "\n",
    "    return doc\n",
    "\n",
    "def retrieve_similar_docs(query,retriever):\n",
    "    \n",
    "    similar_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    results = []\n",
    "    for doc in similar_docs:\n",
    "        results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def FlowAgent(query):\n",
    "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_SBLP_Flow.csv\", encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    documents = data\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "    vectors = FAISS.from_documents(documents, embeddings)\n",
    "    retriever = vectors.as_retriever(search_kwargs={'k': 1})\n",
    "    similar_results = retrieve_similar_docs(query,retriever)\n",
    "    problem_description = similar_results[0]['content'].replace(\"prompt:\", \"\").strip()  \n",
    "    example_matches = retrieve_key_information(problem_description)\n",
    "    example_data_description = csv_qa_tool_flow(example_matches)\n",
    "    example_data_description = example_data_description.replace('{', '{{')\n",
    "    example_data_description = example_data_description.replace('}', '}}')   \n",
    "\n",
    "    tools = [Tool(name=\"CSVQA\", func=csv_qa_tool_flow, description=\"Retrieve flight data.\"),Tool(name=\"CName\", func=retrieve_key_information, description=\"Retrieve flight information.\")]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0, openai_api_key=user_api_key)\n",
    "    prefix = f\"\"\"You are an assistant that generates a SBLP mathematical model and the corresponding Gurobi Python code based on the user's description and provided CSV data.  When you need to retrieve information from the CSV file, please use the CSVQA tool.\n",
    "\n",
    "    Note: Please retrieve all neccessary information from the CSV file to generate the answer. When you generate the answer, please output required parameters in a whole text, including all vectors and matrices.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    suffix = \"\"\"\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    User Description: {input}\n",
    "    {agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "    agent2 = initialize_agent(\n",
    "        tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": prefix,\n",
    "            \"suffix\": suffix,\n",
    "        },\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    return agent2\n",
    "\n",
    "def policy_sblp_flow_model_code(query):\n",
    "    agent2 = FlowAgent(query)\n",
    "    result = agent2.invoke({\"input\": query})\n",
    "    output_model = result['output']\n",
    "\n",
    "    return output_model\n",
    "\n",
    "def ProcessPolicyFlow(query):\n",
    "    output_model = policy_sblp_flow_model_code(query)\n",
    "\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_qa_tool_CA(query: str):\n",
    "    new_vectors = New_Vectors_Flight(query)\n",
    "    matches = re.findall(r\"\\(OD\\s*=\\s*(\\(\\s*'[^']+'\\s*,\\s*'[^']+'\\s*\\))\\s+AND\\s+Departure\\s*Time\\s*=\\s*'(\\d{1,2}:\\d{2})'\\)\", query)\n",
    "    capacity_match = re.search(r\"Eco_flex ticket consumes (\\d+\\.?\\d*)\\s*units\", query)\n",
    "\n",
    "    if capacity_match:\n",
    "        eco_flex_capacity = capacity_match.group(1)\n",
    "    else:\n",
    "        eco_flex_capacity = 1.2\n",
    "\n",
    "    avg_price, x, x_o, ratio_0_list, ratio_list, value_list, value_0_list, avg_pax = {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    if matches == []:\n",
    "        pattern = r\"\\('(\\w+)'\\s*,\\s*'(\\d{1,2}:\\d{2})'\\)\"\n",
    "        matches_2 = re.findall(pattern, query)\n",
    "        matches = matches_2\n",
    "        for od,time in matches:\n",
    "            origin = od[0]\n",
    "            destination = od[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "\n",
    "            doc_1= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "    else:\n",
    "        for match in matches:\n",
    "            origin = match[0][2]\n",
    "            destination = match[0][7]\n",
    "            time = match[1]\n",
    "            od = str((origin, destination))\n",
    "            code_f = f\"({origin}{destination},{time},f)\"\n",
    "            code_l = f\"({origin}{destination},{time},l)\"\n",
    "            code_o = f\"{origin}{destination}\"\n",
    "            x[code_f] = f\"x_{origin}{destination}_{time}_f\"\n",
    "            x[code_l] = f\"x_{origin}{destination}_{time}_l\"\n",
    "\n",
    "            retriever = new_vectors.as_retriever(search_kwargs={'k': 1,\"filter\": {\"OD\": od, \"time\": time}})\n",
    "            doc_1= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_flexi, avg_price=\")\n",
    "            for doc in doc_1:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_f] = value\n",
    "\n",
    "\n",
    "            doc_2= retriever.get_relevant_documents(f\"OD={od}, Departure Time={time}, Oneway_Product=Eco_lite, avg_price=\")\n",
    "            for doc in doc_2:\n",
    "                content = doc.page_content\n",
    "                pattern = r',\\s*(?=\\w+=)'\n",
    "                parts = re.split(pattern, content)\n",
    "\n",
    "                pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "                for pair in pairs:\n",
    "                    key, value = pair.split('=')\n",
    "                    if key == 'avg_price':\n",
    "                        avg_price[code_l] = value\n",
    "\n",
    "            value_1,ratio_1,value_2,ratio_2,value_0,ratio_0 = generate_coefficients(od,time)\n",
    "\n",
    "            ratio_list[code_f] = ratio_1\n",
    "            ratio_list[code_l] = ratio_2\n",
    "            value_list[code_f] = value_1\n",
    "            value_list[code_l] = value_2\n",
    "            ratio_0_list[code_o] = ratio_0\n",
    "            value_0_list[code_o] = value_0\n",
    "\n",
    "    od_matches = re.findall(\n",
    "    r\"OD\\s*=\\s*\\(\\s*'([^']+)'\\s*,\\s*'([^']+)'\\s*\\)\", \n",
    "    query\n",
    "    )\n",
    "\n",
    "    if od_matches == []:\n",
    "        pattern = r\"\\(\\('(\\w+)','(\\w+)'\\)\"\n",
    "        matches = re.findall(pattern, query)\n",
    "        od_matches = matches\n",
    "    \n",
    "    od_matches = list(set(od_matches))\n",
    "\n",
    "    new_vectors_demand = New_Vectors_Demand(query)\n",
    "    for origin, dest in od_matches:\n",
    "        od = str((origin, dest))\n",
    "        code_o = f\"{origin}{dest}\"\n",
    "        x_o[code_o] = f\"x_{origin}{dest}_o\"\n",
    "        retriever = new_vectors_demand.as_retriever(search_kwargs={'k': 1})\n",
    "       \n",
    "        doc_1= retriever.get_relevant_documents(f\"OD={od}, avg_pax=\")\n",
    "        content = doc_1[0].page_content\n",
    "\n",
    "        pattern = r',\\s*(?=\\w+=)'\n",
    "        parts = re.split(pattern, content)\n",
    "        pairs = [p.strip().replace('\"', \"'\") for p in parts]\n",
    "\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')\n",
    "            if key == 'avg_pax':\n",
    "                avg_pax[code_o] = value\n",
    "        \n",
    "    doc = f\"avg_price={avg_price} \\n value_list ={value_list}\\n ratio_list={ratio_list}\\n\"\n",
    "    doc += f\"value_0_list={value_0_list}\\n ratio_0_list={ratio_0_list}\\n\"\n",
    "    doc += f\"avg_pax={avg_pax}\\n\"\n",
    "    doc += f\"capacity_consum = {eco_flex_capacity}\\n\"\n",
    "    doc += f\"flight_capacity = 187 \\n\"\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "def CA_Agent(query):\n",
    "\n",
    "    loader = CSVLoader(file_path=\"Large_Scale_Or_Files/RAG_Example_SBLP_CA.csv\", encoding=\"utf-8\")\n",
    "    data = loader.load()\n",
    "    documents = data\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "    vectors = FAISS.from_documents(documents, embeddings)\n",
    "    retriever = vectors.as_retriever(search_kwargs={'k': 1})\n",
    "    similar_results = retrieve_similar_docs(query,retriever)\n",
    "    problem_description = similar_results[0]['content'].replace(\"prompt:\", \"\").strip()  \n",
    "    example_matches = retrieve_key_information(problem_description)\n",
    "    example_data_description = csv_qa_tool_CA(example_matches)\n",
    "    example_data_description = example_data_description.replace('{', '{{')\n",
    "    example_data_description = example_data_description.replace('}', '}}')    \n",
    "    \n",
    "    tools = [Tool(name=\"CSVQA\", func=csv_qa_tool_CA, description=\"Retrieve flight data.\"),Tool(name=\"CName\", func=retrieve_key_information, description=\"Retrieve flight information.\")]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0, openai_api_key=user_api_key)\n",
    "    prefix = f\"\"\"You are an assistant that generates a SBLP mathematical model and the corresponding Gurobi Python code based on the user's description and provided CSV data.  When you need to retrieve information from the CSV file, please use the CSVQA tool. When you need to retrieve flight information, please use the CName tool.\n",
    "\n",
    "    Note: Please retrieve all neccessary information from the CSV file to generate the answer. When you generate the answer, please output required parameters in a whole text, including all vectors and matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    suffix = \"\"\"\n",
    "\n",
    "        Begin!\n",
    "\n",
    "        User Description: {input}\n",
    "        {agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "    agent2 = initialize_agent(\n",
    "    tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix,\n",
    "    },\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    return agent2\n",
    "\n",
    "def conversational_chat(query):\n",
    "    agent2 = CA_Agent(query)\n",
    "    result = agent2.invoke({\"input\": query})\n",
    "    output = result['output']\n",
    "    return output\n",
    "\n",
    "def get_answer(query):\n",
    "    agent2 = CA_Agent(query)\n",
    "    result = agent2.invoke({\"input\": query})\n",
    "    output_model = result['output']\n",
    "    return output_model\n",
    "def ProcessCA(query):\n",
    "    CA_model = get_answer(query)\n",
    "    return CA_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Input(query):\n",
    "  category_original = Problemtype(query)\n",
    "  print(f\"Problem type classification finished, it belongs to {category_original}.\")\n",
    "  if \"Sales-Based Linear Programming\" in    category_original:\n",
    "    print(\"Processing AirNRM queries\")\n",
    "    if \"flow conservation constraints\" in query or \"flow conservation constraint\" in query:\n",
    "        print('----------Flow Constraints----------')\n",
    "        print(\"Recommend Optimal Flights With Flow Conervation Constraints\")\n",
    "        output_model= ProcessPolicyFlow(query)\n",
    "        Type = \"Policy_Flow\"\n",
    "\n",
    "    else:\n",
    "        print('----------CA----------')\n",
    "        print(\"Only Develop Mathematic Formulations. No Recommendation for Flights.\")\n",
    "        output_model = ProcessCA(query)\n",
    "        Type = \"CA\"\n",
    "  return Type,output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def Batch_Process_Queries(df, query_column='Query'):\n",
    "    \"\"\"Process in Batches\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for query in tqdm(df['Query'], desc=\"Processing Queries\"):\n",
    "        category, output_model = Process_Input(query)\n",
    "        record = {\n",
    "            \"Category\": category,\n",
    "            \"Original_Query\": query,\n",
    "            \"Output\": output_model,\n",
    "        }\n",
    "        results.append(record)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrjSakOq7QNS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_objective(log_text):\n",
    "    \n",
    "\n",
    "    if \"Optimal objective\" in log_text:\n",
    "        pattern = r\"Optimal objective value: ([-+]?\\d*\\.?\\d+)\"\n",
    "        match = re.search(pattern, log_text)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "  \n",
    "    elif \"Best objective\" in log_text:\n",
    "        pattern = r\"Best objective\\s+([-+]?\\d*\\.?\\d+e[-+]?\\d+)\"\n",
    "        match = re.search(pattern, log_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def gain_obj(df_out):\n",
    "    output_code = df_out['Code'].tolist()\n",
    "    obj = []\n",
    "    i = 1\n",
    "\n",
    "    for code in output_code:\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "        namespace = {'gp': gp, 'GRB': GRB}\n",
    "\n",
    "        log_output = []\n",
    "\n",
    "        class LogCapture:\n",
    "            def write(self, message):\n",
    "                log_output.append(message)\n",
    "\n",
    "            def flush(self):  \n",
    "                pass\n",
    "\n",
    "        import sys\n",
    "        log_capture = LogCapture()\n",
    "        original_stdout = sys.stdout  \n",
    "        sys.stdout = log_capture  \n",
    "\n",
    "        try:\n",
    "            exec(code, namespace)\n",
    "        except Exception as e:\n",
    "            log_output.append(f\"Error: {e}\\n\")\n",
    "            print(f\"Error executing code block {i}: {e}\")\n",
    "        finally:\n",
    "            sys.stdout = original_stdout  \n",
    "\n",
    "      \n",
    "        log_text = ''.join(log_output)\n",
    "\n",
    "        optimal_value = extract_objective(log_text)\n",
    "        if optimal_value is not None:\n",
    "            obj.append(optimal_value)\n",
    "            print(f\"Optimal Value: {optimal_value}\")\n",
    "        else:\n",
    "\n",
    "            obj.append(\"No optimal value found.\")\n",
    "            print(\"No optimal value found.\")\n",
    "        i += 1\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34876,
     "status": "ok",
     "timestamp": 1749393358559,
     "user": {
      "displayName": "曾聪聪",
      "userId": "09963294123882050219"
     },
     "user_tz": -480
    },
    "id": "0Unovj497CFc",
    "outputId": "86a32b13-380c-42f3-9d5a-2c3e088ff6bc"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_Dataset/Air_NRM/query_CA.csv')\n",
    "result_df = Batch_Process_Queries(df)\n",
    "result_df.to_csv(\"RAGOnly_CA_GPT4.1_bench_New.csv\", index=False)\n",
    "\n",
    "Code = []\n",
    "for i in range(len(result_df['Output'])):\n",
    "    text = result_df['Output'][i]\n",
    "    pattern = r'```python(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    code = matches[0]\n",
    "    if 'gurobipy' in code:\n",
    "        Code.append(code)\n",
    "    else:\n",
    "        if len(matches) > 0:\n",
    "            Code.append(matches[1])\n",
    "        else:\n",
    "            Code.append(\"No code found\")\n",
    "\n",
    "code_df = pd.DataFrame(Code, columns=['Code'])\n",
    "code_df.to_csv(\"RAGOnly_CA_GPT4.1_New.csv\", index=False)\n",
    "\n",
    "obj = gain_obj(code_df)\n",
    "obj_df = pd.DataFrame({'Optimal Value': obj})\n",
    "obj_df.to_csv('OBJ_CA_RAG_4.1.csv', index=False)\n",
    "\n",
    "combined_df = pd.concat([result_df, code_df,obj_df], axis=1)\n",
    "combined_df.to_csv(\"final_ragonly_CA_bench_GPT4.1_New.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_Dataset/Air_NRM/query_NP_Flow.csv')\n",
    "result_df = Batch_Process_Queries(df)\n",
    "result_df.to_csv(\"RAGOnly_NP_Flow_GPT4.1_bench_New.csv\", index=False)\n",
    "\n",
    "Code = []\n",
    "for i in range(len(result_df['Output'])):\n",
    "    text = result_df['Output'][i]\n",
    "    pattern = r'```python(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    code = matches[0]\n",
    "    if 'gurobipy' in code:\n",
    "        Code.append(code)\n",
    "    else:\n",
    "        if len(matches) > 0:\n",
    "            Code.append(matches[1])\n",
    "        else:\n",
    "            Code.append(\"No code found\")\n",
    "\n",
    "code_df = pd.DataFrame(Code, columns=['Code'])\n",
    "code_df.to_csv(\"RAGOnly_NP_Flow_GPT4.1_New.csv\", index=False)\n",
    "\n",
    "obj = gain_obj(code_df)\n",
    "obj_df = pd.DataFrame({'Optimal Value': obj})\n",
    "obj_df.to_csv('OBJ_NP_Flow_RAG_4.1.csv', index=False)\n",
    "\n",
    "combined_df = pd.concat([result_df, code_df,obj_df], axis=1)\n",
    "combined_df.to_csv(\"final_ragonly_NP_Flow_bench_GPT4.1_New.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "test_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
